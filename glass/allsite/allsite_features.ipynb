{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisherscore(data, labels, num):\n",
    "\n",
    "    high = len(data)  # 向量个数\n",
    "    weight = len(data[0])  # 向量长度\n",
    "    P_num = np.sum(labels == 0)  # 正样本\n",
    "    N_num = np.sum(labels == 1)  # 负样本\n",
    "\n",
    "    # 计算Fisher score\n",
    "\n",
    "    fisherscore = []\n",
    "    for i in range(weight):\n",
    "        p = []\n",
    "        n = []\n",
    "        p_var = []\n",
    "        n_var = []\n",
    "        for j in range(high):\n",
    "            if labels[j] == 0:\n",
    "                p.append(data[j, i])\n",
    "            if labels[j] == 1:\n",
    "                n.append(data[j, i])\n",
    "\n",
    "        p_average = np.sum(p) / len(p)\n",
    "        n_average = np.sum(n) / len(n)\n",
    "        average = (np.sum(p) + np.sum(n)) / (len(p) + len(n))\n",
    "\n",
    "        for j in range(high):\n",
    "            if labels[j] == 0:\n",
    "                p_var.append((data[j, i] - p_average) ** 2)\n",
    "            if labels[j] == 1:\n",
    "                n_var.append((data[j, i] - n_average) ** 2)\n",
    "\n",
    "        score = ((p_average - average) ** 2 + (n_average - average) ** 2) / (\n",
    "                    np.sum(p_var) / len(p) + np.sum(n_var) / len(n))\n",
    "\n",
    "        fisherscore.append(score)\n",
    "\n",
    "    index = np.argsort(-np.array(fisherscore))  # 返回索引\n",
    "    new_data = []\n",
    "    for i in range(num):\n",
    "        new_data.append(data[:, index[i]])\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_data = new_data.transpose(1, 0)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caltech: (30, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caltech: (7, 2000)\n",
      "leuven: (52, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leuven: (11, 2000)\n",
      "nyu: (139, 2000)\n",
      "nyu: (35, 2000)\n",
      "ohsu: (22, 2000)\n",
      "ohsu: (4, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olin: (28, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olin: (6, 2000)\n",
      "pitt: (46, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitt: (10, 2000)\n",
      "sbl: (24, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbl: (5, 2000)\n",
      "sdsu: (30, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdsu: (6, 2000)\n",
      "um: (86, 2000)\n",
      "um: (20, 2000)\n",
      "usm: (57, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usm: (14, 2000)\n",
      "train_data: (514, 2000)\n",
      "train_label: (514,)\n",
      "test_data: (118, 2000)\n",
      "test_label: (118,)\n",
      "data (632, 2000)\n",
      "label (632,)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "from nilearn.connectome import sym_matrix_to_vec\n",
    "\n",
    "\n",
    "'''\n",
    "加载提取出来的特征数据集\n",
    "'''\n",
    "\n",
    "'''\n",
    "加载caltech数据\n",
    "'''\n",
    "caltech_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\caltech\\\\caltech_train_features.npy')\n",
    "caltech_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\caltech\\\\caltech_train_labels.npy')\n",
    "caltech_train_features = np.squeeze(caltech_train_features)\n",
    "#print(\"caltech_train_features.shape:\", caltech_train_features.shape)\n",
    "#print(\"caltech_train_labels.shape\", caltech_train_labels.shape)\n",
    "\n",
    "caltech_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\caltech\\\\caltech_test_features.npy')\n",
    "caltech_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\caltech\\\\caltech_test_labels.npy')\n",
    "caltech_test_features = np.squeeze(caltech_test_features)\n",
    "#print(\"caltech_test_features.shape:\", caltech_test_features.shape)\n",
    "#print(\"caltech_test_labels.shape\", caltech_test_labels.shape)\n",
    "\n",
    "\n",
    "#caltech构建皮尔逊矩阵\n",
    "caltech_addfeatures_train = []\n",
    "caltech_addfeatures_test = []\n",
    "\n",
    "for i in range(30):\n",
    "    min_train = caltech_train_features[i * 146:(i + 1) * 146]\n",
    "    caltech_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(7):\n",
    "    min_test = caltech_test_features[i * 146:(i + 1) * 146]\n",
    "    caltech_addfeatures_test.append(min_test)\n",
    "\n",
    "caltech_addfeatures_train = np.squeeze(np.array(caltech_addfeatures_train))\n",
    "caltech_addfeatures_train = caltech_addfeatures_train.transpose(0,2,1)\n",
    "caltech_addfeatures_test = np.squeeze(np.array(caltech_addfeatures_test))\n",
    "caltech_addfeatures_test = caltech_addfeatures_test.transpose(0,2,1)\n",
    "\n",
    "caltech_addtrain_labels = np.array([0] * 15 + [1] * 15)\n",
    "caltech_addtest_labels = np.array([0] * 4 + [1] * 3)\n",
    "\n",
    "\n",
    "conn_est = ConnectivityMeasure(kind='partial correlation')\n",
    "caltech_addfeatures_train = conn_est.fit_transform(caltech_addfeatures_train)\n",
    "caltech_addfeatures_train = sym_matrix_to_vec(caltech_addfeatures_train)\n",
    "caltech_addfeatures_train = fisherscore(caltech_addfeatures_train, caltech_addtrain_labels, 2000)\n",
    "print('caltech:',caltech_addfeatures_train.shape)\n",
    "\n",
    "caltech_addfeatures_test = conn_est.fit_transform(caltech_addfeatures_test)\n",
    "caltech_addfeatures_test = sym_matrix_to_vec(caltech_addfeatures_test)\n",
    "caltech_addfeatures_test = fisherscore(caltech_addfeatures_test, caltech_addtest_labels, 2000)\n",
    "print('caltech:',caltech_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载leuven数据\n",
    "'''\n",
    "leuven_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\leuven\\\\leuven_train_features.npy')\n",
    "leuven_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\leuven\\\\leuven_train_labels.npy')\n",
    "leuven_train_features = np.squeeze(leuven_train_features)\n",
    "#print(\"leuven_train_features.shape:\", leuven_train_features.shape)\n",
    "#print(\"leuven_train_labels.shape\", leuven_train_labels.shape)\n",
    "\n",
    "leuven_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\leuven\\\\leuven_test_features.npy')\n",
    "leuven_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\leuven\\\\leuven_test_labels.npy')\n",
    "leuven_test_features = np.squeeze(leuven_test_features)\n",
    "#print(\"leuven_test_features.shape:\", leuven_test_features.shape)\n",
    "#print(\"leuven_test_labels.shape\", leuven_test_labels.shape)\n",
    "\n",
    "#leuven构建皮尔逊矩阵\n",
    "leuven_addfeatures_train = []\n",
    "leuven_addfeatures_test = []\n",
    "\n",
    "for i in range(52):\n",
    "    min_train = leuven_train_features[i * 246:(i + 1) * 246]\n",
    "    leuven_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(11):\n",
    "    min_test = leuven_test_features[i * 246:(i + 1) * 246]\n",
    "    leuven_addfeatures_test.append(min_test)\n",
    "\n",
    "leuven_addfeatures_train = np.squeeze(np.array(leuven_addfeatures_train))\n",
    "leuven_addfeatures_train = leuven_addfeatures_train.transpose(0,2,1)\n",
    "leuven_addfeatures_test = np.squeeze(np.array(leuven_addfeatures_test))\n",
    "leuven_addfeatures_test = leuven_addfeatures_test.transpose(0,2,1)\n",
    "leuven_addtrain_labels = np.array([0] * 24 + [1] * 28)\n",
    "leuven_addtest_labels = np.array([0] * 5 + [1] * 6)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "leuven_addfeatures_train = conn_est.fit_transform(leuven_addfeatures_train)\n",
    "leuven_addfeatures_train = sym_matrix_to_vec(leuven_addfeatures_train)\n",
    "leuven_addfeatures_train = fisherscore(leuven_addfeatures_train, leuven_addtrain_labels, 2000)\n",
    "print('leuven:',leuven_addfeatures_train.shape)\n",
    "\n",
    "leuven_addfeatures_test = conn_est.fit_transform(leuven_addfeatures_test)\n",
    "leuven_addfeatures_test = sym_matrix_to_vec(leuven_addfeatures_test)\n",
    "leuven_addfeatures_test = fisherscore(leuven_addfeatures_test, leuven_addtest_labels, 2000)\n",
    "print('leuven:',leuven_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载nyu数据\n",
    "'''\n",
    "nyu_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\nyu\\\\nyu_train_features.npy')\n",
    "nyu_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\nyu\\\\nyu_train_labels.npy')\n",
    "nyu_train_features = np.squeeze(nyu_train_features)\n",
    "# print(\"ohsu_train_features.shape:\", nyu_train_features.shape)\n",
    "# print(\"ohsu_train_labels.shape\", nyu_train_labels.shape)\n",
    "\n",
    "nyu_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\nyu\\\\nyu_test_features.npy')\n",
    "nyu_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\nyu\\\\nyu_test_labels.npy')\n",
    "nyu_test_features = np.squeeze(nyu_test_features)\n",
    "# print(\"ohsu_test_features.shape:\", nyu_test_features.shape)\n",
    "# print(\"ohsu_test_labels.shape\", nyu_test_labels.shape)\n",
    "\n",
    "#nyu构建皮尔逊矩阵\n",
    "nyu_addfeatures_train = []\n",
    "nyu_addfeatures_test = []\n",
    "\n",
    "for i in range(139):\n",
    "    min_train = nyu_train_features[i * 176:(i + 1) * 176]\n",
    "    nyu_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(35):\n",
    "    min_test = nyu_test_features[i * 176:(i + 1) * 176]\n",
    "    nyu_addfeatures_test.append(min_test)\n",
    "\n",
    "nyu_addfeatures_train = np.squeeze(np.array(nyu_addfeatures_train))\n",
    "nyu_addfeatures_train = nyu_addfeatures_train.transpose(0,2,1)\n",
    "nyu_addfeatures_test = np.squeeze(np.array(nyu_addfeatures_test))\n",
    "nyu_addfeatures_test = nyu_addfeatures_test.transpose(0,2,1)\n",
    "nyu_addtrain_labels = np.array([0] * 60 + [1] * 79)\n",
    "nyu_addtest_labels = np.array([0] * 15 + [1] * 20)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "nyu_addfeatures_train = conn_est.fit_transform(nyu_addfeatures_train)\n",
    "nyu_addfeatures_train = sym_matrix_to_vec(nyu_addfeatures_train)\n",
    "nyu_addfeatures_train = fisherscore(nyu_addfeatures_train, nyu_addtrain_labels, 2000)\n",
    "print('nyu:',nyu_addfeatures_train.shape)\n",
    "\n",
    "nyu_addfeatures_test = conn_est.fit_transform(nyu_addfeatures_test)\n",
    "nyu_addfeatures_test = sym_matrix_to_vec(nyu_addfeatures_test)\n",
    "nyu_addfeatures_test = fisherscore(nyu_addfeatures_test, nyu_addtest_labels, 2000)\n",
    "print('nyu:',nyu_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载ohsu数据\n",
    "'''\n",
    "ohsu_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\ohsu\\\\ohsu_train_features.npy')\n",
    "ohsu_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\ohsu\\\\ohsu_train_labels.npy')\n",
    "ohsu_train_features = np.squeeze(ohsu_train_features)\n",
    "# print(\"ohsu_train_features.shape:\", ohsu_train_features.shape)\n",
    "# print(\"ohsu_train_labels.shape\", ohsu_train_labels.shape)\n",
    "\n",
    "ohsu_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\ohsu\\\\ohsu_test_features.npy')\n",
    "ohsu_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\ohsu\\\\ohsu_test_labels.npy')\n",
    "ohsu_test_features = np.squeeze(ohsu_test_features)\n",
    "# print(\"ohsu_test_features.shape:\", ohsu_test_features.shape)\n",
    "# print(\"ohsu_test_labels.shape\", ohsu_test_labels.shape)\n",
    "\n",
    "#ohsu构建皮尔逊矩阵\n",
    "ohsu_addfeatures_train = []\n",
    "ohsu_addfeatures_test = []\n",
    "\n",
    "for i in range(22):\n",
    "    min_train = ohsu_train_features[i * 78:(i + 1) * 78]\n",
    "    ohsu_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(4):\n",
    "    min_test = ohsu_test_features[i * 78:(i + 1) * 78]\n",
    "    ohsu_addfeatures_test.append(min_test)\n",
    "\n",
    "ohsu_addfeatures_train = np.squeeze(np.array(ohsu_addfeatures_train))\n",
    "ohsu_addfeatures_train = ohsu_addfeatures_train.transpose(0,2,1)\n",
    "ohsu_addfeatures_test = np.squeeze(np.array(ohsu_addfeatures_test))\n",
    "ohsu_addfeatures_test = ohsu_addfeatures_test.transpose(0,2,1)\n",
    "ohsu_addtrain_labels = np.array([0] * 10 + [1] * 12)\n",
    "ohsu_addtest_labels = np.array([0] * 2 + [1] * 2)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "ohsu_addfeatures_train = conn_est.fit_transform(ohsu_addfeatures_train)\n",
    "ohsu_addfeatures_train = sym_matrix_to_vec(ohsu_addfeatures_train)\n",
    "ohsu_addfeatures_train = fisherscore(ohsu_addfeatures_train, ohsu_addtrain_labels, 2000)\n",
    "print('ohsu:',ohsu_addfeatures_train.shape)\n",
    "\n",
    "ohsu_addfeatures_test = conn_est.fit_transform(ohsu_addfeatures_test)\n",
    "ohsu_addfeatures_test = sym_matrix_to_vec(ohsu_addfeatures_test)\n",
    "ohsu_addfeatures_test = fisherscore(ohsu_addfeatures_test, ohsu_addtest_labels, 2000)\n",
    "print('ohsu:',ohsu_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载olin数据\n",
    "'''\n",
    "olin_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\olin\\\\olin_train_features.npy')\n",
    "olin_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\olin\\\\olin_train_labels.npy')\n",
    "olin_train_features = np.squeeze(olin_train_features)\n",
    "# print(\"olin_train_features.shape:\", olin_train_features.shape)\n",
    "# print(\"olin_train_labels.shape\", olin_train_labels.shape)\n",
    "\n",
    "olin_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\olin\\\\olin_test_features.npy')\n",
    "olin_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\olin\\\\olin_test_labels.npy')\n",
    "olin_test_features = np.squeeze(olin_test_features)\n",
    "# print(\"olin_test_features.shape:\", olin_test_features.shape)\n",
    "# print(\"olin_test_labels.shape\", olin_test_labels.shape)\n",
    "\n",
    "\n",
    "#olin构建皮尔逊矩阵\n",
    "olin_addfeatures_train = []\n",
    "olin_addfeatures_test = []\n",
    "\n",
    "for i in range(28):\n",
    "    min_train = olin_train_features[i * 206:(i + 1) * 206]\n",
    "    olin_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(6):\n",
    "    min_test = olin_test_features[i * 206:(i + 1) * 206]\n",
    "    olin_addfeatures_test.append(min_test)\n",
    "\n",
    "olin_addfeatures_train = np.squeeze(np.array(olin_addfeatures_train))\n",
    "olin_addfeatures_train = olin_addfeatures_train.transpose(0,2,1)\n",
    "olin_addfeatures_test = np.squeeze(np.array(olin_addfeatures_test))\n",
    "olin_addfeatures_test = olin_addfeatures_test.transpose(0,2,1)\n",
    "olin_addtrain_labels = np.array([0] * 16 + [1] * 12)\n",
    "olin_addtest_labels = np.array([0] * 3 + [1] * 3)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "olin_addfeatures_train = conn_est.fit_transform(olin_addfeatures_train)\n",
    "olin_addfeatures_train = sym_matrix_to_vec(olin_addfeatures_train)\n",
    "olin_addfeatures_train = fisherscore(olin_addfeatures_train, olin_addtrain_labels, 2000)\n",
    "print('olin:',olin_addfeatures_train.shape)\n",
    "\n",
    "olin_addfeatures_test = conn_est.fit_transform(olin_addfeatures_test)\n",
    "olin_addfeatures_test = sym_matrix_to_vec(olin_addfeatures_test)\n",
    "olin_addfeatures_test = fisherscore(olin_addfeatures_test, olin_addtest_labels, 2000)\n",
    "print('olin:',olin_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载pitt数据\n",
    "'''\n",
    "pitt_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\pitt\\\\pitt_train_features.npy')\n",
    "pitt_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\pitt\\\\pitt_train_labels.npy')\n",
    "pitt_train_features = np.squeeze(pitt_train_features)\n",
    "# print(\"pitt_train_features.shape:\", pitt_train_features.shape)\n",
    "# print(\"pitt_train_labels.shape\", pitt_train_labels.shape)\n",
    "\n",
    "pitt_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\pitt\\\\pitt_test_features.npy')\n",
    "pitt_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\pitt\\\\pitt_test_labels.npy')\n",
    "pitt_test_features = np.squeeze(pitt_test_features)\n",
    "# print(\"pitt_test_features.shape:\", pitt_test_features.shape)\n",
    "# print(\"pitt_test_labels.shape\", pitt_test_labels.shape)\n",
    "\n",
    "#pitt构建皮尔逊矩阵\n",
    "pitt_addfeatures_train = []\n",
    "pitt_addfeatures_test = []\n",
    "\n",
    "for i in range(46):\n",
    "    min_train = pitt_train_features[i * 196:(i + 1) * 196]\n",
    "    pitt_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(10):\n",
    "    min_test = pitt_test_features[i * 196:(i + 1) * 196]\n",
    "    pitt_addfeatures_test.append(min_test)\n",
    "\n",
    "pitt_addfeatures_train = np.squeeze(np.array(pitt_addfeatures_train))\n",
    "pitt_addfeatures_train = pitt_addfeatures_train.transpose(0,2,1)\n",
    "pitt_addfeatures_test = np.squeeze(np.array(pitt_addfeatures_test))\n",
    "pitt_addfeatures_test = pitt_addfeatures_test.transpose(0,2,1)\n",
    "pitt_addtrain_labels = np.array([0] * 24 + [1] * 22)\n",
    "pitt_addtest_labels = np.array([0] * 5 + [1] * 5)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "pitt_addfeatures_train = conn_est.fit_transform(pitt_addfeatures_train)\n",
    "pitt_addfeatures_train = sym_matrix_to_vec(pitt_addfeatures_train)\n",
    "pitt_addfeatures_train = fisherscore(pitt_addfeatures_train, pitt_addtrain_labels, 2000)\n",
    "print('pitt:',pitt_addfeatures_train.shape)\n",
    "\n",
    "pitt_addfeatures_test = conn_est.fit_transform(pitt_addfeatures_test)\n",
    "pitt_addfeatures_test = sym_matrix_to_vec(pitt_addfeatures_test)\n",
    "pitt_addfeatures_test = fisherscore(pitt_addfeatures_test, pitt_addtest_labels, 2000)\n",
    "print('pitt:',pitt_addfeatures_test.shape)\n",
    "\n",
    "\n",
    "'''\n",
    "加载sbl数据\n",
    "'''\n",
    "sbl_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sbl\\\\sbl_train_features.npy')\n",
    "sbl_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sbl\\\\sbl_train_labels.npy')\n",
    "sbl_train_features = np.squeeze(sbl_train_features)\n",
    "# print(\"sbl_train_features.shape:\", sbl_train_features.shape)\n",
    "# print(\"sbl_train_labels.shape\", sbl_train_labels.shape)\n",
    "\n",
    "sbl_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sbl\\\\sbl_test_features.npy')\n",
    "sbl_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sbl\\\\sbl_test_labels.npy')\n",
    "sbl_test_features = np.squeeze(sbl_test_features)\n",
    "# print(\"sbl_test_features.shape:\", sbl_test_features.shape)\n",
    "# print(\"sbl_test_labels.shape\", sbl_test_labels.shape)\n",
    "\n",
    "#sbl构建皮尔逊矩阵\n",
    "sbl_addfeatures_train = []\n",
    "sbl_addfeatures_test = []\n",
    "\n",
    "for i in range(24):\n",
    "    min_train = sbl_train_features[i * 196:(i + 1) * 196]\n",
    "    sbl_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(5):\n",
    "    min_test = sbl_test_features[i * 196:(i + 1) * 196]\n",
    "    sbl_addfeatures_test.append(min_test)\n",
    "\n",
    "sbl_addfeatures_train = np.squeeze(np.array(sbl_addfeatures_train))\n",
    "sbl_addfeatures_train = sbl_addfeatures_train.transpose(0,2,1)\n",
    "sbl_addfeatures_test = np.squeeze(np.array(sbl_addfeatures_test))\n",
    "sbl_addfeatures_test = sbl_addfeatures_test.transpose(0,2,1)\n",
    "sbl_addtrain_labels = np.array([0] * 12 + [1] * 12)\n",
    "sbl_addtest_labels = np.array([0] * 2 + [1] * 3)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "sbl_addfeatures_train = conn_est.fit_transform(sbl_addfeatures_train)\n",
    "sbl_addfeatures_train = sym_matrix_to_vec(sbl_addfeatures_train)\n",
    "sbl_addfeatures_train = fisherscore(sbl_addfeatures_train, sbl_addtrain_labels, 2000)\n",
    "print('sbl:',sbl_addfeatures_train.shape)\n",
    "\n",
    "sbl_addfeatures_test = conn_est.fit_transform(sbl_addfeatures_test)\n",
    "sbl_addfeatures_test = sym_matrix_to_vec(sbl_addfeatures_test)\n",
    "sbl_addfeatures_test = fisherscore(sbl_addfeatures_test, sbl_addtest_labels, 2000)\n",
    "print('sbl:',sbl_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载sdsu数据\n",
    "'''\n",
    "sdsu_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sdsu\\\\sdsu_train_features.npy')\n",
    "sdsu_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sdsu\\\\sdsu_train_labels.npy')\n",
    "sdsu_train_features = np.squeeze(sdsu_train_features)\n",
    "# print(\"sdsu_train_features.shape:\", sdsu_train_features.shape)\n",
    "# print(\"sdsu_train_labels.shape\", sdsu_train_labels.shape)\n",
    "\n",
    "sdsu_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sdsu\\\\sdsu_test_features.npy')\n",
    "sdsu_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\sdsu\\\\sdsu_test_labels.npy')\n",
    "sdsu_test_features = np.squeeze(sdsu_test_features)\n",
    "# print(\"sdsu_test_features.shape:\", sdsu_test_features.shape)\n",
    "# print(\"sdsu_test_labels.shape\", sdsu_test_labels.shape)\n",
    "\n",
    "#sdsu构建皮尔逊矩阵\n",
    "sdsu_addfeatures_train = []\n",
    "sdsu_addfeatures_test = []\n",
    "\n",
    "for i in range(30):\n",
    "    min_train = sdsu_train_features[i * 176:(i + 1) * 176]\n",
    "    sdsu_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(6):\n",
    "    min_test = sdsu_test_features[i * 176:(i + 1) * 176]\n",
    "    sdsu_addfeatures_test.append(min_test)\n",
    "\n",
    "sdsu_addfeatures_train = np.squeeze(np.array(sdsu_addfeatures_train))\n",
    "sdsu_addfeatures_train = sdsu_addfeatures_train.transpose(0,2,1)\n",
    "sdsu_addfeatures_test = np.squeeze(np.array(sdsu_addfeatures_test))\n",
    "sdsu_addfeatures_test = sdsu_addfeatures_test.transpose(0,2,1)\n",
    "sdsu_addtrain_labels = np.array([0] * 12 + [1] * 18)\n",
    "sdsu_addtest_labels = np.array([0] * 2 + [1] * 4)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "sdsu_addfeatures_train = conn_est.fit_transform(sdsu_addfeatures_train)\n",
    "sdsu_addfeatures_train = sym_matrix_to_vec(sdsu_addfeatures_train)\n",
    "sdsu_addfeatures_train = fisherscore(sdsu_addfeatures_train, sdsu_addtrain_labels, 2000)\n",
    "print('sdsu:',sdsu_addfeatures_train.shape)\n",
    "\n",
    "sdsu_addfeatures_test = conn_est.fit_transform(sdsu_addfeatures_test)\n",
    "sdsu_addfeatures_test = sym_matrix_to_vec(sdsu_addfeatures_test)\n",
    "sdsu_addfeatures_test = fisherscore(sdsu_addfeatures_test, sdsu_addtest_labels, 2000)\n",
    "print('sdsu:',sdsu_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载um数据\n",
    "'''\n",
    "um_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\um\\\\um_train_features.npy')\n",
    "um_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\um\\\\um_train_labels.npy')\n",
    "um_train_features = np.squeeze(um_train_features)\n",
    "# print(\"um_train_features.shape:\", um_train_features.shape)\n",
    "# print(\"um_train_labels.shape\", um_train_labels.shape)\n",
    "\n",
    "um_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\um\\\\um_test_features.npy')\n",
    "um_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\um\\\\um_test_labels.npy')\n",
    "um_test_features = np.squeeze(um_test_features)\n",
    "# print(\"um_test_features.shape:\", um_test_features.shape)\n",
    "# print(\"um_test_labels.shape\", um_test_labels.shape)\n",
    "\n",
    "#um构建皮尔逊矩阵\n",
    "um_addfeatures_train = []\n",
    "um_addfeatures_test = []\n",
    "\n",
    "for i in range(86):\n",
    "    min_train = um_train_features[i * 296:(i + 1) * 296]\n",
    "    um_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(20):\n",
    "    min_test = um_test_features[i * 296:(i + 1) * 296]\n",
    "    um_addfeatures_test.append(min_test)\n",
    "\n",
    "um_addfeatures_train = np.squeeze(np.array(um_addfeatures_train))\n",
    "um_addfeatures_train = um_addfeatures_train.transpose(0,2,1)\n",
    "um_addfeatures_test = np.squeeze(np.array(um_addfeatures_test))\n",
    "um_addfeatures_test = um_addfeatures_test.transpose(0,2,1)\n",
    "um_addtrain_labels = np.array([0] * 43 + [1] * 43)\n",
    "um_addtest_labels = np.array([0] * 10 + [1] * 10)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "um_addfeatures_train = conn_est.fit_transform(um_addfeatures_train)\n",
    "um_addfeatures_train = sym_matrix_to_vec(um_addfeatures_train)\n",
    "um_addfeatures_train = fisherscore(um_addfeatures_train, um_addtrain_labels, 2000)\n",
    "print('um:',um_addfeatures_train.shape)\n",
    "\n",
    "um_addfeatures_test = conn_est.fit_transform(um_addfeatures_test)\n",
    "um_addfeatures_test = sym_matrix_to_vec(um_addfeatures_test)\n",
    "um_addfeatures_test = fisherscore(um_addfeatures_test, um_addtest_labels, 2000)\n",
    "print('um:',um_addfeatures_test.shape)\n",
    "\n",
    "'''\n",
    "加载usm数据\n",
    "'''\n",
    "usm_train_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\usm\\\\usm_train_features.npy')\n",
    "usm_train_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\usm\\\\usm_train_labels.npy')\n",
    "usm_train_features = np.squeeze(usm_train_features)\n",
    "# print(\"usm_train_features.shape:\", usm_train_features.shape)\n",
    "# print(\"usm_train_labels.shape\", usm_train_labels.shape)\n",
    "\n",
    "usm_test_features = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\usm\\\\usm_test_features.npy')\n",
    "usm_test_labels = np.load('C:\\\\Users\\\\atr\\\\Desktop\\\\save\\\\single\\\\usm\\\\usm_test_labels.npy')\n",
    "usm_test_features = np.squeeze(usm_test_features)\n",
    "# print(\"usm_test_features.shape:\", usm_test_features.shape)\n",
    "# print(\"usm_test_labels.shape\", usm_test_labels.shape)\n",
    "\n",
    "#usm构建皮尔逊矩阵\n",
    "usm_addfeatures_train = []\n",
    "usm_addfeatures_test = []\n",
    "\n",
    "for i in range(57):\n",
    "    min_train = usm_train_features[i * 236:(i + 1) * 236]\n",
    "    usm_addfeatures_train.append(min_train)\n",
    "\n",
    "for i in range(14):\n",
    "    min_test = usm_test_features[i * 236:(i + 1) * 236]\n",
    "    usm_addfeatures_test.append(min_test)\n",
    "\n",
    "usm_addfeatures_train = np.squeeze(np.array(usm_addfeatures_train))\n",
    "usm_addfeatures_train = usm_addfeatures_train.transpose(0,2,1)\n",
    "usm_addfeatures_test = np.squeeze(np.array(usm_addfeatures_test))\n",
    "usm_addfeatures_test = usm_addfeatures_test.transpose(0,2,1)\n",
    "usm_addtrain_labels = np.array([0] * 37 + [1] * 20)\n",
    "usm_addtest_labels = np.array([0] * 9 + [1] * 5)\n",
    "\n",
    "\n",
    "#conn_est = ConnectivityMeasure(kind='tangent')\n",
    "usm_addfeatures_train = conn_est.fit_transform(usm_addfeatures_train)\n",
    "usm_addfeatures_train = sym_matrix_to_vec(usm_addfeatures_train)\n",
    "usm_addfeatures_train = fisherscore(usm_addfeatures_train, usm_addtrain_labels, 2000)\n",
    "print('usm:',usm_addfeatures_train.shape)\n",
    "\n",
    "usm_addfeatures_test = conn_est.fit_transform(usm_addfeatures_test)\n",
    "usm_addfeatures_test = sym_matrix_to_vec(usm_addfeatures_test)\n",
    "usm_addfeatures_test = fisherscore(usm_addfeatures_test, usm_addtest_labels, 2000)\n",
    "print('usm:',usm_addfeatures_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "构建多站点数据集\n",
    "'''\n",
    "\n",
    "train_data = np.vstack((caltech_addfeatures_train,\n",
    "                        leuven_addfeatures_train,\n",
    "                        nyu_addfeatures_train,\n",
    "                        ohsu_addfeatures_train,\n",
    "                        olin_addfeatures_train,\n",
    "                        pitt_addfeatures_train,\n",
    "                        sbl_addfeatures_train,\n",
    "                        sdsu_addfeatures_train,\n",
    "                        um_addfeatures_train,\n",
    "                        usm_addfeatures_train,\n",
    "                        ))\n",
    "train_label = np.hstack((caltech_addtrain_labels,\n",
    "                         leuven_addtrain_labels,\n",
    "                         nyu_addtrain_labels,\n",
    "                         ohsu_addtrain_labels,\n",
    "                         olin_addtrain_labels,\n",
    "                         pitt_addtrain_labels,\n",
    "                         sbl_addtrain_labels,\n",
    "                         sdsu_addtrain_labels,\n",
    "                         um_addtrain_labels,\n",
    "                         usm_addtrain_labels,\n",
    "                         ))\n",
    "\n",
    "test_data = np.vstack((caltech_addfeatures_test,\n",
    "                       leuven_addfeatures_test,\n",
    "                       nyu_addfeatures_test,\n",
    "                       ohsu_addfeatures_test,\n",
    "                       olin_addfeatures_test,\n",
    "                       pitt_addfeatures_test,\n",
    "                       sbl_addfeatures_test,\n",
    "                       sdsu_addfeatures_test,\n",
    "                       um_addfeatures_test,\n",
    "                       usm_addfeatures_test,\n",
    "                       ))\n",
    "test_label = np.hstack((caltech_addtest_labels,\n",
    "                        leuven_addtest_labels,\n",
    "                        nyu_addtest_labels,\n",
    "                        ohsu_addtest_labels,\n",
    "                        olin_addtest_labels,\n",
    "                        pitt_addtest_labels,\n",
    "                        sbl_addtest_labels,\n",
    "                        sdsu_addtest_labels,\n",
    "                        um_addtest_labels,\n",
    "                        usm_addtest_labels,\n",
    "                        ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"train_data:\", train_data.shape)\n",
    "print(\"train_label:\", train_label.shape)\n",
    "print(\"test_data:\", test_data.shape)\n",
    "print(\"test_label:\", test_label.shape)\n",
    "\n",
    "\n",
    "\n",
    "data = np.vstack((train_data, test_data))\n",
    "label = np.hstack((train_label,test_label))\n",
    "print('data', data.shape)\n",
    "print('label', label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-ccebc66d2bcc>\", line 2, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-ccebc66d2bcc>\", line 2, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-ccebc66d2bcc>\", line 2, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3348, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1418, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1318, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1186, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"D:\\anaconda\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-ccebc66d2bcc>\", line 2, in <module>\n",
      "    from keras.models import Sequential\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\__init__.py\", line 3, in <module>\n",
      "    from . import utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\__init__.py\", line 6, in <module>\n",
      "    from . import conv_utils\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\utils\\conv_utils.py\", line 9, in <module>\n",
      "    from .. import backend as K\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\__init__.py\", line 1, in <module>\n",
      "    from .load_backend import epsilon\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\load_backend.py\", line 90, in <module>\n",
      "    from .tensorflow_backend import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 5, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 99, in <module>\n",
      "    from tensorflow_core import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\__init__.py\", line 28, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"D:\\anaconda\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\", line 49, in <module>\n",
      "    from tensorflow.python import pywrap_tensorflow\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 74, in <module>\n",
      "    raise ImportError(msg)\n",
      "ImportError: Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3348, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1418, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1318, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1186, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: can only concatenate str (not \"list\") to str\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\", line 58, in <module>\n",
      "    from tensorflow.python.pywrap_tensorflow_internal import *\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n",
      "    _pywrap_tensorflow_internal = swig_import_helper()\n",
      "  File \"D:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n",
      "    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 242, in load_module\n",
      "    return load_dynamic(name, filename, file)\n",
      "  File \"D:\\anaconda\\lib\\imp.py\", line 342, in load_dynamic\n",
      "    return _load(spec)\n",
      "ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。\n",
      "\n",
      "\n",
      "Failed to load the native TensorFlow runtime.\n",
      "\n",
      "See https://www.tensorflow.org/install/errors\n",
      "\n",
      "for some common reasons and solutions.  Include the entire stack trace\n",
      "above this error message when asking for help.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: 动态链接库(DLL)初始化例程失败。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2043\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImportError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[0;32m   3346\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3347\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3348\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3349\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3350\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 2047\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1418\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1316\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1318\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1319\u001b[0m             )\n\u001b[0;32m   1320\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Minimal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1184\u001b[0m         \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1186\u001b[1;33m             \u001b[0mformatted_exceptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_chained_exception_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__cause__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1187\u001b[0m             \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Activation, MaxPooling1D\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_dim=2000))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "his = model.fit(x=train_data, y=train_label, batch_size=48, epochs=1000,shuffle=True, verbose=1, \n",
    "               validation_data=(test_data,test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOydd3hUVdrAf+9MGoGE3kMIAipNWkQRVFAUERW7wLoKFj537XWjq4jYcHfd1VUsoNh7xwVFUFERlBpakN5CT4AQAimTnO+Pe2fmzsydZCbJpJ7f88yTe889594zkJz3vuW8ryil0Gg0Go3GH0d1T0Cj0Wg0NRMtIDQajUZjixYQGo1Go7FFCwiNRqPR2KIFhEaj0Whs0QJCo9FoNLZoAaGp94hIiogoEYkKoe84EVlQFfPSaKobLSA0tQoR2SYihSLSwq893VzkU6pnZhpN3UMLCE1tZCswxn0iIr2ABtU3nZpBKBqQRhMOWkBoaiPvANdZzq8H3rZ2EJHGIvK2iBwQke0i8rCIOMxrThH5l4hkicgWYKTN2NdFZI+I7BKRJ0TEGcrEROQTEdkrIjki8rOI9LBcayAiz5rzyRGRBSLSwLw2WEQWishhEdkpIuPM9vkicpPlHj4mLlNrulVENgIbzbbnzXscEZFlInKmpb9TRB4Skc0ikmte7yAiU0XkWb/v8rWI3BXK99bUTbSA0NRGfgMSRaSbuXBfA7zr1+cFoDFwAnA2hkAZb167GbgI6AukAlf6jX0LcAFdzD7nAzcRGt8AXYFWwHLgPcu1fwH9gTOAZsADQImIJJvjXgBaAn2A9BCfB3ApcBrQ3TxfYt6jGfA+8ImIxJnX7sHQvi4EEoEbgGPmdx5jEaItgHOBD8KYh6auoZTSH/2pNR9gGzAMeBh4GrgAmAtEAQpIAZxAAdDdMu7/gPnm8Q/ALZZr55tjo4DW5tgGlutjgB/N43HAghDn2sS8b2OMl7HjQG+bfg8CXwS5x3zgJsu5z/PN+59TxjwOuZ8LrAdGBem3DjjPPL4NmF3d/9/6U70fbbPU1FbeAX4GOuFnXgJaADHAdkvbdqC9edwO2Ol3zU1HIBrYIyLuNodff1tMbeZJ4CoMTaDEMp9YIA7YbDO0Q5D2UPGZm4jci6HxtMMQIInmHMp61lvAtRgC91rg+QrMSVMH0CYmTa1EKbUdw1l9IfC53+UsoAhjsXeTDOwyj/dgLJTWa252YmgQLZRSTcxPolKqB2UzFhiFoeE0xtBmAMScUz7Q2WbcziDtAHlAvOW8jU0fT0pm09/wN+BqoKlSqgmQY86hrGe9C4wSkd5AN+DLIP009QQtIDS1mRsxzCt51kalVDHwMfCkiCSISEcM27vbT/ExcIeIJIlIUyDNMnYP8B3wrIgkiohDRDqLyNkhzCcBQ7hkYyzqT1nuWwLMAP4tIu1MZ/FAEYnF8FMME5GrRSRKRJqLSB9zaDpwuYjEi0gX8zuXNQcXcACIEpGJGBqEm9eAx0WkqxicIiLNzTlmYvgv3gE+U0odD+E7a+owWkBoai1Kqc1KqaVBLt+O8fa9BViA4aydYV6bDswBVmI4kv01kOswTFQZGPb7T4G2IUzpbQxz1S5z7G9+1+8DVmMswgeBZwCHUmoHhiZ0r9meDvQ2x/wHKAT2YZiA3qN05mA4vDeYc8nH1wT1bwwB+R1wBHgd3xDht4BeGEJCU88RpXTBII1GYyAiZ2FoWimm1qOpx2gNQqPRACAi0cCdwGtaOGhACwiNRgOISDfgMIYp7blqno6mhqBNTBqNRqOxRWsQGo1Go7GlzmyUa9GihUpJSanuaWg0Gk2tYtmyZVlKqZZ21+qMgEhJSWHp0mARjxqNRqOxQ0S2B7umTUwajUajsUULCI1Go9HYogWERqPRaGypMz4IO4qKisjMzCQ/P7+6p1JlxMXFkZSURHR0dHVPRaPR1HLqtIDIzMwkISGBlJQULKmb6yxKKbKzs8nMzKRTp07VPR2NRlPLqdMmpvz8fJo3b14vhAOAiNC8efN6pTFpNJrIUacFBFBvhIOb+vZ9NRpN5KjzAkKj0WjqGllHC/h2zZ6IP0cLiAiSnZ1Nnz596NOnD23atKF9+/ae88LCwpDuMX78eNavXx/hmWo0mtrE+DeWcMu7yzmSXxTR50TUSS0iF2DUtXVipBCe4nc9GaNASROzT5pSaraIpGAUUHevjL8ppW6J5FwjQfPmzUlPTwdg0qRJNGrUiPvuu8+nj7s4uMNhL6vfeOONiM9To9HULnYcPAZASUlkk61GTIMwC7hPBUYA3YExItLdr9vDwMdKqb7AaOAly7XNSqk+5qfWCYfS2LRpEz179uSWW26hX79+7NmzhwkTJpCamkqPHj2YPHmyp+/gwYNJT0/H5XLRpEkT0tLS6N27NwMHDmT//v3V+C00Gk11UVVZuCOpQQwANimltgCIyIcYBd0zLH0U3nq5jYHdkZrMY1+vJWP3kUq9Z/d2iTx6cSi17APJyMjgjTfe4JVXXgFgypQpNGvWDJfLxdChQ7nyyivp3t1Xnubk5HD22WczZcoU7rnnHmbMmEFaWprd7TUaTR3GLR6EyAalRNIH0R7fWriZZpuVScC1IpIJzMaoI+ymk4isEJGfRORMuweIyAQRWSoiSw8cOFCJU488nTt35tRTT/Wcf/DBB/Tr149+/fqxbt06MjIyAsY0aNCAESNGANC/f3+2bdtWVdPVaDQhoJTiP3M3sDUrr9R+OceLmPx1BoWuchbuq6IyPpHUIOxEm//XGgO8qZR6VkQGAu+ISE9gD5CslMoWkf7AlyLSQynlowIopaYB0wBSU1NL/Scr75t+pGjYsKHneOPGjTz//PMsXryYJk2acO2119ruZYiJifEcO51OXC5XlcxVo6mLlJQonv5mHWNP60inFsbf45pdOczN2Mfd551YrnseyC3g+e838vmKTH554Jyg/Z759g/e/30HPdolckX/pLCf417sSiJsaoqkBpEJdLCcJxFoQroR+BhAKbUIiANaKKUKlFLZZvsyYDNQvv+xWsCRI0dISEggMTGRPXv2MGfOnOqekkZT59mWncf0X7Zy89veMgGXvLiA57/fWG7nr3tUflHpmkF+YbFP/7CfYwqG2iwglgBdRaSTiMRgOKFn+vXZAZwLnpq4ccABEWlpOrkRkROArsCWCM61WunXrx/du3enZ8+e3HzzzQwaNKi6p6TR1CjW783l1veXU1RcTpNMKWzaf5THvl4LgFsuFJWU7zlus0lZ63ax2SHKUTEfQoSDmCJnYlJKuUTkNmAORgjrDKXUWhGZDCxVSs0E7gWmi8jdGMJ0nFJKichZwGQRcQHFwC1KqYORmmtVMGnSJM9xly5dPOGvYOx+fuedd2zHLViwwHN8+PBhz/Ho0aMZPXp05U9Uo6mB3P1ROhl7jvCXszvTs31jAJ6evY7k5vH86bSOnn6FrhIGP/MDxwuLWTHxPKKcwd+BrVkH3vh1m48Z2lWsiC3H6uher8uKMio2V3ZnOQVEqM+pKBHdB6GUmo3hfLa2TbQcZwABr8tKqc+AzyI5N41GU7t59WfDqPCn0zoyN2MfnyzdyZgByezPLQDgspcW8qfTkhk9INl2fLGflmA1K5VXU3GZ9yhr2Q5HQMxYsJWMPUf411W9PW1uuRBpDULvpNZoNLWem99eyncZ+9idc9zTtnpXDmmfrw46psAvgsh6XlRsrLwfLt7BZS/9GvQeGbuPkJI2i16T5nC8sJji4kDfwP2frOSp2es853tyjvPNmr0AOELInTb5fxl8uizTc37xCws4XuT2YdReH4RGo9FUKg9/uabU6wU2zuGPluwA4ILnfub1BVs97W4h4CbfXHQBXKZ2kfb5albsOEwwXpq/CYDcfBdbso56fAtWbeSTZZlM+9nrQv1ixS7Pcbg+CKUUq3fleM61BqHRaGo0P/yxj5S0WeQcM/ICHckvotODsxjx/C+c/c8fAcMRnJI2iw37cgPGvzx/MwOenBfSs9J3Gou1df+A1Q6f7yoOGPO3z1aTkjaLP/bm8vj/Muj04CyOFxYHmJGsY4tcoa28BT7zgKH/mm8cA1e/uojrZiwOGHO80PucIBl2guIv1Gptqg2NRlM/mPrjZgA27DcW/x3Zx1AK1u05wvZsI2eQO/PoV+m7uOTFBaSkzWLiV4Y28My3f7A/t4Bnvv0j5GfmFXj3AB057j3OLwwUEP4oBbe8u4yrXlnk024NTfWPYhr3xmIunRpoarIKiEKrwFGweOtBft4QuIH3mGWON7y5lMPHjMSd181YzPg3AgWKFX+hphQcK3RRYCMYK4M6XVFOo9FEHreVpKRE8df3ljF79V6f61e9spAl2w4BXmEC8Pai7Uwe1dNz/vL8zfztgpM950qpoPVNcvO9QiHnuDejaV4IAgLgJ5uF22pi8l+I5683+p/+1PfsPWJsYp08qgcFljH7jxR4jkvbn2CdL8CAp75nwxMjPMJk4eYsxk7/ne/vPdtH27CbV4lS/N87yziS7+KrWys/PF4LiAiSnZ3NueeeC8DevXtxOp20bNkSgMWLF/vsjC6NGTNmcOGFF9KmTZuIzVWjKS/Flsgdf+EAeISDHcu2+0av5xcVk77zMKOn/QbA46N68NOGLDL2+OZRO3DUm2nAmvLa6mMIlxHP/+I5LnIp27dyt3AAmP7LFpo3jPWc3/LuMs+xvwPcysqdvj6NQlcJKWmzPOdfmj6KJVsPknXUK3SUUvz1veU+Y79M38UvG7Po2Dw+6PMqghYQESSUdN+hMGPGDPr166cFhKbGoZRieSlO3LJ4yaJRAJz8yLece3Irz/l7v+/gj72Bfgt3umuAi15YEHC9osxcuYvpL5YubKIdjqC5lFxBfANHC1ylCg+Aj5caEUtRTgdx0U6fey7cnO3T97l5GwE4eDS0+jLhogVENfHWW28xdepUCgsLOeOMM3jxxRcpKSlh/PjxpKeno5RiwoQJtG7dmvT0dK655hoaNGgQluah0UQaq929PGkfvv8jMGW9tc1OOOQcL+LVnyKbWGFuxr4y+zgd4ut3KIPPlmVy7ycrQ+4f7RRio7xu4sPHghcHclRwR3Yw6o+A+CYN9gaPiS4XbXrBiCll9/NjzZo1fPHFFyxcuJCoqCgmTJjAhx9+SOfOncnKymL1amOehw8fpkmTJrzwwgu8+OKL9OnTp3Lnr9GEQVFxCTnHi2jRyGtW2ZfjNYGUOzNpmPR+7LuIPyMUURfldHC0IPSKbl+m7yq7k/X+DofHfAdwaimRXpHaUa2jmKqBefPmsWTJElJTU+nTpw8//fQTmzdvpkuXLqxfv54777yTOXPm0Lhx4+qeqkbj4cHPV5P6xDwfR+lZZhgrGKGsdYWs3IIy+0Q5xHbfhR0xUY6gDveg93dKyE73SAW71h8Nohxv+pFCKcUNN9zA448/HnBt1apVfPPNN/z3v//ls88+Y9q0adUwQ40mEPcGr6LiEqJtchw9MWtdQFu4iJSd6K4qCGVhjnJKmf4EN4WuElZlen01pyQ1ZlVmTikjDAHkH8UUlAj9m2kNohoYNmwYH3/8MVlZWYAR7bRjxw4OHDiAUoqrrrqKxx57jOXLjYiFhIQEcnMDbbEaTaTZefCYJ5LGbe44EMLbdXm5rG97fnlgKK0TY8vuXA7GDOjA69enVsq9Nu07GpZZzepDaBJfth/xmzV7yc4LzfmsNYg6RK9evXj00UcZNmwYJSUlREdH88orr+B0Ornxxhs98d/PPPMMAOPHj+emm27STmpNxFmx4xCxUU5aJsTSMiGWM//xI4lxUcy5+yxPn7P/OZ+PJpxO/45NK/35DaKddGgWz4/3DaH7xMqvi3LxKe3o1jax7I4hkFsQvGDXG+NP5bu1e/lg8U7b66H4DKz5l8oiUj4ILSCqCGu6b4CxY8cyduzYgH4rVqwIaLv66qu5+uqrIzU1jQaAXYePc9lLCz3nn/1lIABH8l0MfPoHn77XTPuN50dXftBEAzOsMz6m7KWpWcMYDvq9Yfdsn8hDI7ox9rXfbcec0aWFbXqKhjHOkO39odAwJsrWDOcmlCR9buy+pz+R0iC0iUmj0QC+O4kBrnh5UZCeBnd+mF7q9XBwb/RqEOMM2qd3km/QRrQzcJGNj46iWSNfDfv7e8/2ObcLCbWOaRAdfA6h4iopCboXojTiY5zE+AmWNolxvDCmb6njbhjUKexnhYIWEBpNPed/q3aTX1SMM8wom4oyflAKk0cZRXrO6NwcwGdjmD/NGvou/BPO6hzQx+kQn9BQgEY2lX8mnHWC3729Po9GccG1l/SJ5wW9ZsVVrDypv92c2bWF5ziY6Fj72HDm3eMr0JwOIbFBtOd87Gm+9S22Pn0h954fmYrMERUQInKBiKwXkU0ikmZzPVlEfhSRFSKySkQutFx70By3XkSGl3cOka64VNOob99XUzEydh/htvdXkPbZKk+K6/LQoVmDsMcoBdcNTGHblJEewWDdGOZPo7honzflGwcHvjVHOQMjf+xSavubmZpYFmA7gTIgpRnbpowMybkMcHKbBC7t2x6A6wYaFe/eHD/Acz3Y36mIEOWnGTkc4qMtNbbM1T0m3BDaUImYgDBrSk8FRgDdgTEi0t2v28PAx0qpvhg1q18yx3Y3z3sAFwAvuWtUh0NcXBzZ2dn1ZtFUSpGdnU1cXFx1T0VTS3CvK79tORiQSjoc3rnhtLDHWP8u3W/9pdVHaBTrZOLF/kuIL1EOoXPLRr5tNr4At/nn5DYJADS3mJgaxnqXml8eGIoI/G3EyYTKxidH0CoxjoGdm7Ntykgmj+rJtikjcTqE164LHkF117Cunu/g/52sZqfEOF8BEUki6aQeAGxSSm0BEJEPgVFAhqWPAtwhBY2B3ebxKOBDpVQBsFVENpn3K90o6kdSUhKZmZkcOBCYubGuEhcXR1JSUnVPQ1MLyDlexI1vLgGMJHSuCgiI+Fjf97fEuCiO5AeP8gHfYjeeEpyWhXDGuFRueHOp57yhjeP65jM7Mf0Xb84kp8NB04YxbJsy0pMAz85X4X7emAHJXH9GCg986k2BEe100DIhltvP6UKHZvFsfXpkqd/Dn9Kc006bubi5a9iJ5nfw7eMU8blnYgPvv0OkrYKRFBDtAWuMVybg/5oxCfhORG4HGgLDLGN/8xvb3v8BIjIBmACQnBxYdzY6OppOnSLjvNFoajP5RcUBKSv8ayCEg7/voH3TeI74ZWD1x5q7yX1sfXvul9yUJvHR9GzXmAWbsmhoY/p56MJuzM3YR5dWjZi3br+tBhJlU5Xn2tM78uWKXZzfo7X5fO+1+88/iTO6tAgYUxlY/TydWjTk1qFd+HrlbvomNwk6X4fDV2jkFbho2ziOPTn5pQqjyiCSAsJOtvm/oowB3lRKPSsiA4F3RKRniGNRSk0DpgGkpqbWDzuSRlMJvDR/c0BbRTSIuChfAZHUtAENY5ws3R481bd1UXY/27qANomPIX3i+Tz+vwwWbMry+AbuH34SG83KdCLC/PuH8lX6LkNA2Lyh22kQJ7VJYPVjXtemW0D988pTyi0cnrqsF3PWBqY7t9KvY1M6tWjIveefRJ8OhlC4sr+vxu+vZfg73i/o0ZZTkpowetpvREcoSZ+bSAqITKCD5TwJrwnJzY0YPgaUUotEJA5oEeJYjUYTIscLi/nHnD+IiXJwed8kjtls8nL5ZSZtmRAb8q5pu0X49etPpffkwMR6V/RL4o+9R7jlbG8kkbuWs10I6lHTVOXWIG4d2iWgz5CTWtGrfWOPmcZKKA5ctzITyv6EiRd1Z3t2Hm8t2m55hhFd5B9h5E+j2Ch+vG9IqX38tSCnw+HJf9U3uQnJzeM9dSmiS3HqVwaRFBBLgK4i0gnYheF09t8ZtgM4F3hTRLoBccABYCbwvoj8G2gHdAVKr8Wn0WhsKSouodvEbz3nwVJlW/MKdW7ZsEwfghX/RVgpiI6yX2z7dGjMs1f39mkrKcVJfcewruw9ks9FvdsGfX7jBtF8ffvgkOfrj1uD8Lf/23GDGT3lIyDK/eRAAn0Q0CupMeee3Ir7LzgJ8FbxszOfVSYRExBKKZeI3AbMAZzADKXUWhGZDCxVSs0E7gWmi8jdGCakccoIbVgrIh9jOLRdwK1KqcgUXdVo6jgLNmaF1G+86bAGeO+m07nkxeCFeN6/+TTGTrffrQxQ4Cq2tY9f0rsdV/QPDKJIG9GNEgXDewQWxWrfpAFv3TAgoD0c7jv/xIDoJituC055nb6VGWbqvx/F6XAQG+Xk9XGnBjzPTnOrTCKaakMpNRuY7dc20XKcAdgWUlVKPQk8Gcn5aTR1na/Sd/Hub9vL7mhhUJfmtE6MDbpYXtqnHWd0bsGki7uTsecIJ5gL7yMXdSf7aAEvzd/MkXxXgDbw2nWpDOve2vaebRrH8d8ydgtXhNvO6VrqdbcGEc5C/+ktA5m7bh+v/rSlUjUIh0O4dWhnlDJ8RXHRgYLWPc3a7KTWaDTVTHnSYTx6cQ9j85W57D16cXce+9qITu+d1JhHLjL2IozzS+9w4+BObNqfy0vzN5N7vAgR4cr+SXy6LJNubRODCoeagHuDXjhO39SUZpyS1IRXf9pCfCkpQsrD/cNPpqi4hGOFxbY+F0dd0CA0Gk3tw//N/6wTWzK4SwsWbMri8Ut70rxR8FTcjRsYG87cdvR/XnkKHZrGc3m/gCj1iPPpLQPZGGIRo0dGdqdlQiznhSnEYqIcpI04mWHdWpXdOUyinQ4mXdLD9pqjijQInYtJo6kD/PDHPi547mefam8AXVoFt7tbcedEAu+iIx5HqPDva3pz//CT6NW+9CqHLRNieXhkN14zay6ICHcO60qHZvGhfpVKIzWlGWMGlB5V5KZpwxgeHNHNdtd1Wdxydme6tEoIe1xFcEddaQGh0WjK5O9frOGPvbmkPjGPN3/17ixu0Si03EHnnOx9A/bfS+B0CK0S4rh1aJeQbPQ3nXkCHZs3DHHmmvLgzptlt++jMtECQqOpA7iTyOUcL2KS6S9YsyuH37YcDGl8UtN4/nbBydx8ZifaJBq5vNxLT6QSwWnKT6HLUCG0k1qj0ZRJ0/jABG6hViRb+rCR4eYvQwLTZ4POEFwTcWsQ/rUjKhutQWg0dQB/2/nmA0d5c+G2kMa2COJ07mz6L2IivFtXEz5uX5M2MWk0GnKOF5GSNou3F22zve7/lr9oc3aFn/ni2H68Mf5UWiXo9PE1DXdNiK4hBiGUFy0gNJpawM6DxwD4YPFO2+v+ViD/qmof3Hx62M9s3CCaoSdVfvimpuL079iMN8afyv3DQ69TUR60gNBoagHHzCpp/huyjha4KHSV+KTOBsg+6ptkb2Dn5rRtrDWBusTQk1pF3PynBYRGUws4XmQvIHo+Ooex038L0CD++8OmgHt8ffvggJKe/Sx1CDQaf7SA0GhqAccLjcyqDczCPLsPH6fQzL66dPuhAA3CjhaNYunUwnd/wjs3hl8qVFN/0GGuGk0NpdBVws5Dx+jcspHHxNQgxkl+UTFnTPnBJ31FYXH5qsHZVWnTaNxoDUKjqaE8OnMt5z77E1lHC8gvMgRAXJQhIAC+XumtobVix+FqmaOmbqMFhEZTQ1m02ajjkHO8yGNCcljKTxaVo0ToRacEL7qj0fijBYRGU0Nxl98sKVGWegXhC4ZrUr3Ve28d2oVVk86vvElq6jQRFRAicoGIrBeRTSKSZnP9PyKSbn42iMhhy7Viy7WZkZynRlMTcVcWc5Uoj1D4euVuFm0JrUKcm6cv7+U5FhES4wLTcmg0dkTMQyUiTmAqcB6QCSwRkZlmFTkAlFJ3W/rfDlhLSh1XSvWJ1Pw0mpqOu6ZCflExxWbundx8F3d/tDLke7RoFOPRRDSacImkBjEA2KSU2qKUKgQ+BEaV0n8M8EEE56PR1CrcVcPe+W27x0kdDr2TGrPk78NsrzVuEM3VqYG1oTUaK5GMcWsPWPMCZAK2Qdci0hHoBPxgaY4TkaWAC5iilPrSZtwEYAJAcnJohUE0mtpAXoGLjD1HAPh8+a7y7ZgVCZqqe+Wj2g+hKZtIahB2v5nBvGujgU+VUsWWtmSlVCowFnhORAJyESulpimlUpVSqS1btqz4jDWaGsLkrzN8zt2b4kLhjXGnVvZ0NPWUSAqITKCD5TwJ2B2k72j8zEtKqd3mzy3AfHz9ExpNnWZfbn65xzq1z0FTSURSQCwBuopIJxGJwRACAdFIInIS0BRYZGlrKiKx5nELYBCQ4T9Wo6mr+GdjDYcoU0BoMaGpKBHzQSilXCJyGzAHcAIzlFJrRWQysFQp5RYWY4APlW9C+27AqyJSgiHEplijnzSauo6rHJvg3OioJU1lEdFELEqp2cBsv7aJfueTbMYtBHr5t2s09YXiMMt83jWsK8/N2whoE5Om8tA7qTWaGsbCzVks3nowrDF3DTvRc+wOj9VyQlNRdCpHjaaGsGl/Ls9/v8knCV9ZXJ2aRNqIbj5tfTo04drTk5lwZkDgn0YTFlqD0GiqmNz8Iq58eSHbsvJ82u/9eGVYwuG5a/rwjyt706xhjE+70yE8cWkvkpvHV8p8NfUXLSA0mirm+3X7Wbr9EP+Zt8GnPdwkfJf2bV92J42mAmgTk0ZTxbg3N/v7ocMJbX1+dGCasm/uPJP0nbouhKby0AJCo6kmrOJAKcX6fbkhjx3VJ1B76NY2kW5tEythZhqNgTYxaTRVjDs/knXrjysM7WHWHYMrfU4ajR1aQGg0VYxd9Gk4G+N6tGtceZPRaEpBCwiNppqwioSikvDTeWs0kUb7IDSaKsZdPrTIkqG1IEi9hyiHeMxP39x5pifPkkZTFWgNQqOpQg7mFXLnh+kAfJexjxU7DgFw6pPzbPtHO71/ot3aJtK1dULkJ6nRmGgBodFUIXtyjvucL912iEN5hZ7zTi0a+lyPcmqNQVN9aAGh0VQh4ueifu/37fR9fK7n/LahXTj7RG/xK514T1OdaAGh0VQj27KP+ZxHOYXeHZp4zoee1Kqqp6TReNACQqOpRhrF+saJRDsd9GpvhLFOHtWDKVforPea6iOiAkJELhCR9cheszEAACAASURBVCKySUTSbK7/R0TSzc8GETlsuXa9iGw0P9dHcp4aTVVR4pdfw9+AFOUQzuvemgV/G8p1A1OIjXJW3eQ0Gj8iFuYqIk5gKnAeRn3qJSIy01oZTil1t6X/7Zh1p0WkGfAokIoRLr7MHHsoUvPVaKqC7X4mpeNFxT7n7qilpKY6E6um+omkBjEA2KSU2qKUKgQ+BEaV0n8M8IF5PByYq5Q6aAqFucAFEZyrRhNRft2UxbyMfdz6/nKfdv8UGzpqSVOTiORGufbATst5JnCaXUcR6Qh0An4oZazObayplazKPMyfXvs9pL7h5GTSaCJNmRqEiNwmIk3LcW+7V6Fgv/2jgU+VUm59O6SxIjJBRJaKyNIDBw6UY4oaTeTZd6Qg5L75hcW27Zfr2g+aaiAUE1MbDP/Bx6bTOVQdOBPoYDlPAoKVyxqN17wU8lil1DSlVKpSKrVly5b+lzWaakcpxRcrMkvt06JRrOfY3ycBsG3KSP59TWD9B40m0pQpIJRSDwNdgdeBccBGEXlKRMoqeLsE6CoinUQkBkMIzPTvJCInAU2BRZbmOcD5ItLU1F7ON9s0mhrJZ8syOZAbqCn8sTeX2av3ljq2T4cmfPaXgQAM6NQsIvPTaMpDSE5qZSSu32t+XBgL+qci8o9SxriA2zAW9nXAx0qptSIyWUQusXQdA3yoLMnxlVIHgccxhMwSYLLZptHUOHYdPs69n6zkNj8HNMC6PUfKHL/78HH6d2zGtikjdfSSpkZRppNaRO4ArgeygNeA+5VSRSLiADYCDwQbq5SaDcz2a5vodz4pyNgZwIyy5qfRVDe5+UUAHDpWGHCt0FV2Gu+iYp3qW1MzCUWDaAFcrpQarpT6RClVBKCUKgEuiujsNJpaQL6ZqjsuOnBTW2mL/4cTTgd05JKm5hKKgJgNeMw7IpIgIqcBKKXWRWpiGk1NYt+RfJ7+Zh3FNov5cTPyyF9AvPf7dn7ZmBX0nk3jY4DQtAyNpjoIZR/Ey0A/y3meTZtGU6d54NNV/LThAGef2JIzOrfwuXa8yAXA4q0HUUp5ak7//Ys1AfdZmHYO27Lz+HRZJokNjD8/O6Gj0dQEQhEQ4udALhERXYlOU69wmSVB7RbzowXe0NT9uQW0TowLep8op3BG5xac0bkFJSWKsaclM3ZAcuVPWKOpBEJZ6LeYjuqXzfO/AlsiNyWNpubhMLUCOwFRYNm7oGyUAYeAe1i0w2vVdTiEpy7T2Vo1NZdQfBC3AGcAu/Cmy5gQyUlpNDUNd+Ee/2ys4Cs0Plm6k4e/XO1z3Vo2VOda0tQmytQglFL7MTa5aTT1FqepQZSUwJu/biV952GeG90X8I1CenbuBgCeuNSrGRRYnNBWYaHR1HRC2QcRB9wI9AA8xlWl1A0RnJdGU6NwO56LlWLS10bGeo+AsAllPefZ+bb3idIlRDW1iFBeZ97ByMc0HPgJIy9SbiQnpdHUNNwv/iU2Pgi7fQxbDuQFuY8WEJraQygCootS6hEgTyn1FjAS0J41TZ1k/5F8UtJmsXCz7/4Frw8icEw4Yaqh57rUaKqfUKKYisyfh0WkJ0Y+ppSIzUijqUaWbTeKFr69cLvPfgf3wm4t+JOSNqtqJ6fRVDGhaBDTzIyqD2NkY80AnonorDSaakb5lR9xVuDN/9U/96/odDSaaqFUDcJMyHfELPv5M3BClcxKo6km3HLAP5q1Ir6DQV1alN1Jo6mBlCogzF3TtwEfV9F8NJpqxhAE/l6FirgOYqMcfHPnmTSK1QkINLWLUH5j54rIfcBHGHmYAE/NBo2mThFMEFTExBTlELq1TSz3eI2mughFQLj3O9xqaVNoc5OmDlOZJiYduaSprYRScrSTzSck4WDWsF4vIptEJC1In6tFJENE1orI+5b2YhFJNz8BpUo1mqrEofcvaOohoeykvs6uXSn1dhnjnMBU4DyMHE5LRGSmUirD0qcr8CAwSCl1SERaWW5xXCmlK7VrqhSvGPBVIWLKmSLjrmFdKzQfTQXYvhD2r4NTb/S2HdwCK96Dcx427IkHNsDaL+DsB+CPWbD8baP/icO9Y5SCn/4B3S+B7b9CXBPYkw4J7WDpDGNsiQuWvgFdzoVW3WB3Ovz+ChQdg4ueg6yNcNZ9cPwQfPsgdBwIrbrDzt8hqgG07Q1fTIC+14I4wFVg3OfrO2HwPTDsUWMui6cb4zb/AH3/BDmZxnfqPy4i/4ShmJhOtRzHAecCy4FSBQQwANiklNoCICIfAqMwwmTd3AxMNaOk3HmfNJoaR3lTZLRv0qCSZ6IJmTdGGD+tAuLdK+HgZkgdD42T4K2L4eheGHAzfPQno8/GOTApxzsmPwfmPwXL3oTc3YHP+fxm73Hm4sDr/7vL+FmQYwitg1uMZ1iJjjeEycIXAscv+LdXQMy+z9ue8SVkbzKOq0tAKKVut56LSGOM9Btl0R7YaTl3Z4K1cqJ5z18BJzBJKfWteS1ORJYCLmCKUupL/weIyATMzLLJyTqnvqbiuP0F/j6I8tb0OW5JBa6pAeQdMH46zKXPlV/2mPzDxs8SV8We7SqEvCAVBouOhX+/YPeqRMqjNx8DQtGb7V65/P/Mosx7DQHGAK+JSBPzWrJSKhUYCzwnIp0DbqbUNKVUqlIqtWXLlqHOX6MJil0679Larfx0/5CAtoN5hRWdkqYyKThi/CwJQ3AfMwM2o4MXggqZQvscXWVSYlOWtrz3CoNQfBBf413YHUB3QtsXkQl0sJwnAf76WSbwm1KqCNgqIusxBMYSpdRuAKXUFhGZD/QFNofwXI2m3LiT8fmLg1AEREJcdEDbIS0gqoZdyyAqDlr3sLm2HBa9CN0u8bbtWwuN23vP/QXGopcMf4EzBjK+MtqiKmguzN4Iqpwa5Z50yNnp21ZSZN+3EgnFB/Evy7EL2K6Uygxh3BKgq4h0wig2NBpDG7DyJYbm8KaItMAwOW0xU3scU0oVmO2DgH+E8EyNpkJ8u3avbXtZCfmaxkcTGxWokJ/XvU2lzEtTBtPPMX5afQeea0ONn2s+87a9f5XR1x2CnO83bs6DgfdxBr4AhMXuFeUf6/4OVUwoAmIHsEcplQ8gIg1EJEUpta20QUopl7kLew6Gf2GGUmqtiEwGliqlZprXzheRDKAYuF8plS0iZwCvikgJhtYyxRr9pNFEgrcWbuOrdEPJVUrxzeo9pKY0I6/AxU8bDgQdd+3pyTxxaS+fuhAJcVGsnjQ86BhNTcEUEHkhxMcUHY/sVNy06xu+MFGqYtv9gxCKgPgEo+Som2Kz7VT77l6UUrOB2X5tEy3HCrjH/Fj7LESnFNdUIQWuYh6dudZznldQzF/eW07P9oms2XXEdsyTl/Xk71+swVVsaBdRllBYd5umhuNeVPOCvwB4KI8juTxEx4c/prgIomIqfSqhOKmjlFIeQ6p5XPkz0WiqkEJXCa/+tJki861//5ECn+vu6CN/4XBJ73ae46EnGdt2xp4WGEH32CU2tvDahqsQsiPo9jt2EHL9THpZG6HYZdj/j+yBA+sNB+2+DNg4FzbNM8ZtXwh7VsGO32HPSu/4Td8bc96zquzn71wCx7KN43Vfl90/d0/o361ClEMT2P5r5U+D0DSIAyJyiWkSQkRGAZGPr9JoIsgbv27l6W/+4NWft/DUZT1p3ijW53q+TXhqq4RY/jumL9uy87iyfxLtmjRg25SRPn16d2jCqN7tuPrUDgHjax2z7oYV78IDWyG+WeXf/9mToLjQ6zfIyYQXU2HgbbDkNW8I6tC/wy/PhhaS+u7loT//9WHe49WfhD7OStfzYeN35RsblHJon7PuhTuWl90vTELRIG4BHhKRHSKyA/gb8H+VPhONpgo5WmDEtB/MK+SWd5dz+JhvRIjd/oX9uYaWMfO2wVw3MMX2vl/dOogbBneq3MlWFxvnGT9DWZjLQ7FfhFeBWcl443e+z9z6s3Eek1Cx56XeUHafcLnyDWjexbetZTdDqNrhjJDxJaZhRG4bSi6mzUqp0zHCW3sopc5QSm2KyGw0mirCX4n/7/cbfc4zD1WRQ7Im4wmjrKI8VOI0fvrH9xeb84htVLH7dzqrYuPtiG0EzfxS0yW2Da5x+QuTSptHBYVnEMoUECLylIg0UUodVUrlikhTEXkiIrPRaKqJ1btswiPrO8XmzuGK7iAuKQ4tAqjY9AMVHPVtP25uVKvoW3JMBQVMMPz3UDhKCYeN1BwipEGE4oMYoZR6yH1iJtW7EKMEqUajqYu8eZGROwjguZ7Q62q4Ynr57vXeVbD5e+MN/so34J9+SREmNfY9L/AT1lkbjJ/lie6xEqnFWfntcnan8UAI8Cdk+2qqtrTqBkf3h9bXTdPImDVDERBOEYlVShWAsQ8CiC1jjEZT53j80p7VPYWqY9svvuerPy6/gNj8vfFz68+Bu4HDIbqCO5kbNIVzJ8L3k8s3/i+LjGgha8I8CNwd7X6bvyfDyBDb5hSYcb7RdvyQt98N33nbb5xraGp5B6DrcDiyC/atMRb+V8+0n88FUwzzmyqBfrZJtytMKALiXeB7EXnDPB8PvBWR2Wg0VUU5NhX9+fSOEZhIPcNRgbKrpZluQsEZDR0Hl3986+7GJ0BA+GkJbn9AYjsjS2wwki25SzsM8L3WvLPxsdLzSljzqfd8wP+Bo3xp6EMlFCf1P4AngG4YjupvAf2XoqkVuIpLuOylX312Qi/cnBXglLZyQsvI2HPrJf6Lp6vAvl8wrALFWcGa3g5nxcYHw9/EFCGHccB9IywcIPRsrnuBEuAKjHoQ6yI2I42mEsnOK2TFjsPc94mxmSq/qJix038vdczVqYF7GD7/6xk2PesYRfkwbSj8J0gSg8nNDX/Byo/gv/285pK5E432SY29Dtvcvcb5f/w2DL52bnhzirYI64poH2BqIBHY4e7vpG5YSmbpiiT8i6v6uuZB/8VF5ESMBHtjgGzgI0CUUtWTNUqjKQcuM8lelEN4ft5GXppfdoR2Qpzvn8U/rjiFfslNIzK/GkVOJuwuZbOVO5rpiwnGzy3zocdl8Ovz3j7HDkKjlrDmc+P8yK6Kzcn6llwZJqYQsvKGzJ+/MH5afRADb4MBEwL73vIrfPM3GPksvGQxLY39BKJCdOm6newdB8HpfynfnMOkNA3iDwxt4WKl1GCl1AsYeZg0mlpDsZkTySHCf+ZtoMBlk1ffj/gYX1NEndgVHQpWB2oo2IWuurOihpO3aOS/g18TJ5x8Udn38I9Q6np+YB9HFB4NoqKb7gA6mxlkrSamIQ/a50Rq0xPGz4JWJ/u2n3g+nHB2aM9za1BJqdDt4vDnWw5KExBXYJiWfhSR6SJyLlW2Y0ajqRyKzEIrUc7Qf3VjnBGyVdd03PsNQsVOQLjvEc7u69Ji+B1Oo0YzBNr6rfg/L9bGHONwejWI+ErUCK3zqqgZrDTc6caLI18Hwk3Qb6OU+gL4QkQaApcCdwOtReRl4AulVGUnINFoKp1CU2NwhlFTOsamrkOdZsW7kDHTqNEcDrPuMZLpWfnsRji8I7z7lLaoOqIsEWelmIf8N/PZhsRa9iXENw9/nkGfbTGsRMoRDt40HTVBQLhRSuUB7wHviUgz4CogDdACQlPjcQuILQdCL88YHYa2USf46tbyj138qu95eRZdZzScepORhXXnYmjfzziObgCp441qcQAnjwyeGG/MR7DgP5Ay2Hijt6brOPkiQ9DENIL2qdDlPDhvMix8AVa+b3+/pFMhcwmc84ghOPeu9l67+Hk4bNnPMWoqzLgAupwbmgZx5n3QJDADcFAuexX2Z8Ap18D6b2Dw3aGPrSBh6UNKqYPAq+ZHo6nxhOJzeOjCk3lq9h+e89ioempiCgVHVPlSb9y5Ep7vHeSe0YbzNhifjDd+xjQynMCLXgzsc9IFxsfN3Ine45Qz4fRbzGfFwbXmXoLLXg4UEBc/D1/fCS1Phpvmedt7j/Ye9x/nO6btKfB3/2rKpXDuI6H39X/2dV+GN7aCRFSXFpELRGS9iGwSkbQgfa4WkQwRWSsi71varxeRjebn+kjOU1N3KQxBQHRt7euwbNzAGy0z/74hlT2lmoUrzJrZie3K7mOHnU/ATVlv3aH4IPyxhpOGY/Zx+ygiUJ2tNhIxASEiTmAqMAJjg90YEenu16cr8CAwSCnVA7jLbG8GPAqcBgwAHjXrVGs0YVFYXHbgXUtLLYhfHhhK80beKJSUFrV001zuPthgmmPWfW2En/qzcR7sXxvYXhqJ7cs3n6i44NfK2gDnERBhlNW0+iAknGXO7efQAgIiq0EMADYppbaYVeg+BEb59bkZmKqUOgSglHIXhh0OzFVKHTSvzQUuQKMJQtbRAjbuy/Vp256dR1Zu2W/IPdt7k8V1aBZPk/gKxtvXBN69HN6/CnJ2wUfXwsc2uXreuwKmDQnvvl3PC34tuiH0vdb+mjMGnLGG/XzYJKMtoa3xs3UZ1YX7/dn4mXy6t80/xbY/J13oPT5hSPB+7VONcNXoeBh0F3QyQ06tZp16TARjsmgPWDNzZWJoBFZOBBCRXwEnMEkp9W2QsQGvLiIyAZgAkJwchtNHU+e45tVFbD6QR8bk4ew8eJz8omJGTQ2vDGP3toYZxO2DOLNri0qfZ5VxaLvx86hZ0nOfn6YQaiTMsMdg3qPG8d+2Q4MmcMadMCUZivLg+q/hhydg5+/w58+NRXzUVO94d6ZWhxMe2e9tD8fR2uksb9U5N/2uh8F3BWaCddPyxMAxdtz8fWBbKOPqCZEUEHY6mn+cWhTQFRgCJAG/iEjPEMeilJoGTANITU3VVeLrMZvNKKWX52/mhR82ce7JrcIav+zhYcTHeP8clvx9WMCO6lpFTEMozDXqOkPgPoFQ6jOAb3oHd5ilM8pr149q4LXbl7ZLubJt+tpHUCVE8i8gE7BuQU0C/F39mcBvSqkiYKuIrMcQGJkYQsM6dn7EZurm8A4jPjpCxTc0kUFZFqaFm40i9N//sT9Yd1v8a1K3TKhFGe3zso0UGc07w8Et0Kyz93fYnf2z6BhsW2DsdHbGGPUGQsHqXLY6k912fWcdMMdpghJJAbEE6CoinYBdGHmdxvr1+RIj19ObItICw+S0BdgMPGVxTJ+P4cyOLM/1gvb94eYfIv4oTeWRc9xrLsnNL9t04nQIxSV1SOH85n5Y85n3vMWJEGMW11n7hbf9zZHh37u1pQaGNRrIo0HEwonDIXMxNC6nAzscOpwOvABt+/i1aSJBxJzUSikXcBswByP768dKqbUiMllELjG7zQGyRSQD+BG4XymVbe63eBxDyCwBJpttkce9KUdTa9h7xGs+yc0vO0b/psHe6ltX9Q9z93BNxG1GcpOzq/TQ0UF3QgeLO7DFSd7jEf/wHt+73jd3kDUayF0/2hkDg++BezeEt/mrvHS7yJiXO3/RA1vhuq8i/9x6SkSNrEqp2cBsv7aJlmMF3GN+/MfOAGZEcn6ausHeHK+A2JNTdg6gBy/sxqs/bwHgn1cF2bxVm/DPoVSUV/qegZbdjDDYnWba85YnQpaZMqN5F+NnXBNIaOM7zmr3dwsgR5SRcTWhdfnnHy7WecU3q7rn1kPqWdKZ2sWR/CK+Sq9guuR6wMG8MDd71TXssrAW5Aa2uRGBaMu+BOumsrgmxs+y0mJHMueQpsagBYSbyswTX0n87dNV3PlhOn/sPVLdU6mxHCt08a8568vuaDK8h/Gme8c5XXx2TNdK5j5qhHnabYI7uCX4uEatfM1K0XHejWxu30Xr7oHjrLQ1Na9QaxloaiW1OI6vkvGvClUD2G2aS44X1ry51RSWbDvk+XcKhQdHdAPgnvNP4p7zTyqjdw3n1+eMnyVlOOavegs+MbPVXPqysTGs0xBY+7mRkM4RDbcvh2NZRg6iK14PrJHsz2Wvwu4VgWYof+5cCYVh1IbQ1Ci0BuFG1dxFuObpNjWHYwXhJY7zLwZUZ4gzN4y5TURWelzqPe5mxoc4HNDdTGwgYkQgte1tHPe6smyHc2wj6HRm2fNqmlK2NqKpsWgNwk0N1CA8WfDriYRQSvHiD5u4tG97OjSL97m2J+c4z8/bSKvEOLq3TeC7jH20b9KAlObh7VmJj62jv/IJ7Yw9DontIP9w8H7WHEVO0zxUX37BNGFTR/9aykEN1CDq22bRrVl5PDt3Az+s388Xfx3kc+27tfv4cMnOgDHjzkgJ6xkNomuRBrH5R3jnUmjYEuJbmNFJ5rUcv7oLjdvDgXWGf2F/RvB72u5lKCWRnqZeowWEmxqoQdQ3Dh0zbOkrdhzm2zV7KS5RvL5gC9efkRJ0A1woG96incLpJzSnTWJcWJXlqp1ZZvR33gHjY0ezE+DEC6Dvn43CNr2uNormrPwIkk8zCvEA/PU3w99g5eSLIGsDDLi59Hnc/GPpTm9NnUULCDfh5JqvcirXBPDxkp2IwFWpHcruXIXst2x4u+Vd74bF5TvSuWFQJ7shHC8qW7BPOOsE7h9+cpn9ahyhpKm+Y4X3+OLnjZ8pg+Cs+337tepmfKwktIYRz5T9jPb9jI+m3qEFhJt6pEE88NkqoOYJCOuOaH9m/LrVtj2U1BpRjloaixFK+UqNJoLU0r+cCFATfRARvv8dH6wou1MVUpqACMactfvK7NOgNkUuuQrh24eMBHxSi+atqZNoAeGmBmgQ03/ewgOfrgxoDzfIZOz03/hm9Z4y+81c6Ztc97VftpCSNounv1kX3gMrif1HCir1fh2bxzPhrBPCdmRXK+tmwm9T4ftJRihqMKLj4bzHq2xamvqJFhBuaoAG8eTsdXy8NDOgPdTEo58vz+TcZ+ezcHM2f3lveUhjrKmyn5hlCIZXfwrPIfnf7zcy4e2lYY0BuOqVhbz723bP+dEw9zSURdvGcTx0YTfialPkkjtFhlKlaxADb4NBd1TNnDT1Fi0g3JRU7uJUmVz96iKWbbfJt+PHPR+v9BTOCTVYp7xprx//XwY3vmlExfx77ga+yyjb1OPPkm2HePjLNRWeix0TL+rOC2NqoWPVXdgnOr70fEdh1VnWaMqH/i1zU2KJYtr8o28e/WpCLBshyso3pPzsUNHO0P5rDx0rIiVtFilps3zaH/9fBje9tZSUtFl8u2ZvwLjXF2wNKMrz2i9b6PnoHM/5pv25pKTNYsO+wMRx1vn++XUjq6irnALi+dF9eOuGATRrGONpu2Fwp5pf9OfABtjxm3FceAwWvQTzpxjn0XE69bym2tECwo3VxPTOpfDJuIj7JZRSpKTNYtrPm22vW5WAshy4D32x2uc8Jiq0/9otB47atr++YCvz1hlawQs/bCS/qJiUtFl87LdZraTE10R1tMDlaZu92hAsX67Yxab9R0lJm8WqTGOXb4HLK5B/2ZhFStosft5gH+uf3CyeGeNSgy74Azo14+wTWzLztkG212ssU0+FGcON483fw5wHvbugXX4Zah21PLGgplaiBYQbO2FwvJSUBaWQV+AiJW0W7/2+3fb63px8UtJm8b9VhiP5qdl/lHlPfw3BSkraLD5Y7Ltwx4YoIBwh2KLW7j7CO4uM7/LKT77C7OCxwFTbhcUlXDdjMf+euwEwhIE7bfm8dYbW8ehXa0OaHxi+hHNObh3UWR8fbYSDJjWNt+9QG8jL8j0/6meye3i/b/lPjaYKiKiAEJELRGS9iGwSkTSb6+NE5ICIpJufmyzXii3tMyM5T8DWSa2OZeMqNt50U9JmkWbuHyiLrVmGH+DvX6whJW0Wi7f6pmNeab5Ff7hkR8DYYISqEbjJOlrIa7+U7WwudIW2QXC6ea8tWXlkHfVGG9llmj1WWOyjDeQXFXsK+TRvGMML32/ko6WBaTOsXJ3qrfTWqUXp+Zb8w1hrXRrvYldgTYcjfuXbHQ6dM0lT5URsJ46IOIGpwHlAJrBERGYqpfwTxXyklLrN5hbHlVJ9bNojg40Gsez7j3lxlfCfa/owxJHO3mXpcErZztiSzMMMcWzwnG//bR8DXN5NaYmZWQxxbKFnQWOiHTlG48YYhjjSjfHro3E4hG55a2jkMFIltymIg432hXHc4/xZ93MGtAn8J7T2j9+exxDHxjK/E3nQ3ZRRGT/lMMRhbFxzbHIxxLHGp+s9T6YzxCLP2uzbjkIxxHGYH/+Xbs6h9MeNiDvEfoehtTzW3Qkb9zFIrSDHEbgxLmar1/fw46hCmsYDG+eW/Z1qCutmwh6/8OZD9hsDPdS3RF2aakFKM11U6MYiA4FJSqnh5vmDAEqppy19xgGpdgJCRI4qpRqF+rzU1FS1dGn4oZYedi2D6eeUf7xGU1kktINciwaRmAT3rIXp58Iu83f8T59C1/OqZ36aOoWILFNKpdpdi+Re/vaA1Y6QCZxm0+8KETkL2ADcrZRyj4kTkaWAC5iilPrSf6CITAAmACQnV7BgujuKadgko6CKM5YHPviVDXuP8tioHh6b+Ze3lu0Inb16N9N+9r4BpqY0Zem2QzRvFEP20UIGd23Bgo1Z9EluQvqOw577Xjr1VwDev/k0XvtlKz/4RQn5P39vTr5PziI7Xhzbl6Sm8RwtcHHoWAEdmjb0PKcm8/ionjzy1RoaN4jirRuMX5vr31hMzrEiXr8+lWNFLm5/P512TeN4aWz/ap5teREjEsH9kuaurxybCIe2Gak2GraEBmaNh2s/g5ydRmbXxLbVMWFNPSOSAsJOB/ZXV74GPlBKFYjILcBbgPs1PlkptVtETgB+EJHVSikfD6lSahowDQwNokKzdfsg2pzC9pguJDWNZ31UNivVYbbGdiNdmXb3JFtB68O2TZtIV167ePpWgOZgRnsePdyITaoJ6dsBWgBQ0q4/6cowX+W37sfPx0pIVzZOyaRU8gpc5BW62F1ynHSVU+pcBr+Xy/TrTuJmcyPblMs7ka66lPkdqhtHcirpKp/GKtrzb75WDpOlGuFimwAAEitJREFUCnC160/u0ULS1VGORyeE9H9S62jYIrCtQROvsNBoqoBIOqkzAWs2uCTAx/OmlMpWyr3yMh3ob7m22/y5BZgP9I3gXD0+iN1HCjn7n/N55afNnsihuz6yt/H7k19UzNasPNbuLr2GtF3g0I6D3rKMRcUlbD8YvEzjqKm/MuDJ78kPsRTpih1eB2ja56tL6VlzaNHICGm1bp5z15OOj3F69jyc261V1U9Oo6knRFKDWAJ0FZFOwC5gNDDW2kFE2iql3EmDLgHWme1NgWOmZtECGAT8I4Jz9WgQmUeKACfLtx+iJET/TFFxCT/+sZ/bP1jhE98fjIN5gc7mG97y5upfvv0QeaWkndi039i7cCxEAVGJG5QjwpgByXyw2BvR9ftD53oEwMW9vaaUxy7pwR3ndiUhLpqEuGgWPXgOrRJ0sRuNJlJETEAopVwichswB3ACM5RSa0VkMrBUKTUTuENELsHwMxwExpnDuwGvikgJhpYzxSb6qXIxU23kFRqraUJcFHtKt954+OGP/fzfO6Hves06GiggtpgpMoBS8yhZN7btyw0t+2mogq66aJ0Y63duLPorHjmPhDjvr2iU0+G5BtC2cQM0Gk3kiGjCeaXUbGC2X9tEy/GDwIM24xYCvSI5twBMJ/WxImMxPVpQTMae0k1FbrJtFvxIcc6zP3mO//7FmlJ6evndbx+GHYO6NOfXTdkB7TFOB0UlJeUKwe/csqEnN1RpWPd4WCu+NbWkztBoNFWP3kntxjQxuZTxT+JOMxEKR0IoWlOdrNwZuCP8BL/NZw2i7d8VMiYPZ/nDoYdTXtHPu8HtwwkDmXfPWWWOibHkjapNFUE1mrqOFhBuTCe1qxxlemaHUHuhJnDt6d5Q4HGDUnyuFZfY+06inA4Sw9iZ/NCF3tKeLRNiiY8JFDyz7zjT5zzWko67MjO6ajSaiqEFhMkfe4xIH7cGEYwDuQXkFxUz8as1fJW+i5fnb2ZVZojOimqmWUOvrf+6gSk+1y7o2SboOKffa/3kUT2843q08dESmjWMoVnDGO4ffhJgnyLE6lcAiHYIl/drD9R8h7pGU5/QAsLk+blGOu3ftpaeoO/UJ+dx/6ereHvRdu78MJ1nvi070V5F6JdceXHvyc2CJ7MbeUo7tk0ZSdvG9lFB26aM9By7U4mPPKUtr/y5P7FRXg1ARFj+yHncOtTYaxFtVkVLiPUKBf8CPg4Rnr68al1OGo2mbLSAMHFimFhW77FPf23la79SnQB3DevKC2OMrRrRTiEu2vinfdzyth0ufxnSmc//Wv4U1u63coBGsVGc1721bb9tU0bSyFzAQ3FGuwWEO613bHTwX6OGsU4S46KYdIn338G/v4ivH0Kj0dQMIhrFVJtwmAKipJwyMzbK6VOkJ7/IuJ81LNOOnu0TOZBbwD6beszu8NRHLurO4/8LPcq3T4cmpO88zJ9O68i/r/ZN1tetbSLjzRrN9553Ist2+GYRfWxUDx6buZbdOcFDaKOd4jO/WGfwymdRTgerJhk1D7LzCvjhj/0BqcgdIogIXVs14uYzTwjtS2o0moijBYSJW4MoDiIgEmKjyC1l81pslIOYKGPhFCPBDgBtgphsAE5qncDLf+pPh2bxARXdwHMLbhzcKSwBMfHi7vzt01V0a5sQcO2bO70O4tvP7RpwfXiPNgzv0cZ+PibuBd6tbTSKi+KsE1ty3ekdS53XhLM6M+GszgG1LUwrFHPvObvU8RqNpmrRAsLEKcEFRIdmDUhqEs+iLd59AvExTp+dzHHRTqLcK51Ag2gnx4uKbTWIu4Z1Jcoh3HZO4AIdLlEOIblZPFuyvPsN+iU3jehi261tIj3bJ/InUyA4HcLbNwwIebz4pap26NTVGk2NRBt+TTwahAo0lzhEAiJ5+nRowspHz/ecRzvFx8T0/s2ncVnf9p6cQlbuGnZiSMLB+p49ZoA3rdXY07zhqlFO4bXrU4P6F8rL+EEpvHKtfZbUjs0b8r/bz+TsE1tWyrP8BYZGo6kZaAFhUpqJSQgM9WwQ7STK0lbgKvGYmAD6JjflP9f0CRgXDlZTzNOXn+I5vue8E0lqaqSZiHY4OKFlI6ZfV7kZTR+9uEepoa+Vid4cp9HUTLSAAJZtP1iqkzomyhGw0MfFOIlyetuOFbp8NAg77h9+Em+FYYoJtiegRaNYz9u9dQ61kdGndqh07Uej0VQO2gcBXPf6Yq70aBCBC+60P6fyxKx1Pm1xURafA+AqUTQ3zUnB6jy79waUxtSx/Xhr4TYWbzsYIHBe+lM/iswa2e4NaE7LHP51VW+axkeuHvPUsf0qLfHfU5f1IqlpA86qJDOVRqOpfLSAMAlmYnI6hJQWDWnsl26idWKsj1Yx7owU4qLswz1njEvlaEHpqbm/unUQqzIPM/KUtgzr3opnv9vAbef4CpQLe3lTX7v3DURbNIgr+ycRSUaeUnlVzKx+FI1GUzPRAgJoRg7NxUiX4TYx3Tq0M1N/3Ox5I594UXc+W57pGdO5pW+5bHfOoWev6s0JLX0T4Z1zctkmlN4dmtC7g7FrOjbKyUMXdiu1v9uvW9tNTBqNpuaiBQTwi2OCxxvj1iDuH34yHZs15NRORp3gxn6mm1NTmtne64oIv8W7cYeG6poIGo0mUmgB4YfVSX31qR1s+/zrqt4kNw+e16gq6NAsnimX92KYdvBqNJoIEdEoJhG5QETWi8gmEUmzuT5ORA6ISLr5ucly7XoR2Wh+ro/kPK0E20ltJdK2/lAZPSDZdp+FRqPRVAYR0yBExAlMBc4DMoElIjLTpnToR0qp2/zGNgMeBVIx9ostM8ceIsKEIiA0Go2mPhBJE9MAYJNSaguAiHwIjAJCSSo0HJirlDpojp0LXAB8EKG5elClCIi5d5+FSxcs0Gg09YRIvi63B3ZazjPNNn+uEJFVIvKpiLiN/iGNFZEJIrJURJYeOHCgsuYdlK6tE+jWNjHiz9FoNJqaQCQFhF38pf/r99dAilLqFGAe8FYYY1FKTVNKpSqlUlu2rL4NVzpVhEajqYtE0sSUCVjDgJIAn0o7Sqlsy+l04BnL2CF+Y+dX+gwrgZ/uH0LDWB0MptFo6h6R1CCWAF1FpJOIxACjgZnWDiJi3Zp7CeDOZzEHOF9EmopIU+B8s63G0bF5Qx1JpNFo6iQRe/VVSrlE5DaMhd0JzFBKrRWRycBSpdRM4A4RuQRwAQeBcebYgyLyOIaQAZjsdlhHkolFVRZNq9FoNDWeiNpGlFKzgdl+bRMtxw8CDwYZOwOYEcn5uTmkGvFV8Rm8XTy8Kh6n0Wg0tQId9A84KfbZQR0Xrf9ZNBqNRq+EGJlcXTg5zcy7NO6MTtU8I41Go6l+dPgNEEUJxTho0ziODU+M8EmhrdFoNPUVLSAwTEwunCjlLcSj0Wg09R29GipFlPx/e3cbI9VVx3H8+2MXKeWhQKGIhXbBUrU2LZBNBXwMaq1N076QWLCJBEkaG01RG7XEF42tbzDGVtKmFrU1mtoaa1XCi2Kz1iZGhS4RK5Qiy4MWpbJoS2NTkV3+vrhnl3G5u+ywM9ydO79PcjP3nHt29vznbPKf+7DnnOQkY2q2WpqZWRk4QUS2klxPjDn9X7XNzJqYE8TJHgB6aWH10rZi+2JmNoo4QaQEMWf6JNoHWSXOzKwZOUGkBKExvl9vZlbJCeJkLwCtY8eeoaGZWXNxgkgJYmyrE4SZWSUniPFT+HjvPRy4aFnRPTEzG1WaPkH00MK2E29FE99cdFfMzEaVpk8Qrx/PLjFNGNdScE/MzEaXpk8QADdcNYv5MycV3Q0zs1GlrglC0nWS9kjqknTnEO2WSwpJ7ancJukNSTvS9u169fGC88dy/ycW8f7Li1vT2sxsNKrbw/+SWoAHgA+TrTH9nKRNEfHCgHaTgNuBrQPeYl9ELKhX/8zMbGj1PIO4BuiKiP0R8V/gceCmnHb3AF8H/lPHvpiZWZXqmSAuBl6qKB9Kdf0kLQTmRMTmnJ+fK+kPkp6V9N68XyDpVkmdkjq7u7tr1nEzM6tvgshbdad/wlRJY4B7gTty2h0GLomIhcAXgB9Jmnzam0VsjIj2iGifMcP3EMzMaqmeCeIQMKeiPBv4e0V5EnAl8GtJB4HFwCZJ7RFxPCL+CRAR24F9wOV17KuZmQ1QzwTxHDBf0lxJbwJWAJv6DkbEsYiYHhFtEdEG/B64MSI6Jc1IN7mRNA+YD+yvY1/NzGyAuj3FFBE9kj4LbAFagIcjYpeku4HOiNg0xI+/D7hbUg/QC3w6Iv5Vr76amdnpFCVZZrO9vT06OzuL7oaZWUORtD0i2nOPlSVBSOoG/jKCt5gOHK1RdxqFYy6/ZosXHHO1Lo2I3Kd8SpMgRkpS52BZtKwcc/k1W7zgmGvJczGZmVkuJwgzM8vlBHHKxqI7UADHXH7NFi845prxPQgzM8vlMwgzM8vlBGFmZrmaPkEMd1GjRiNpjqRnJO2WtEvS2lQ/TdLTkvam16mpXpI2pM/heUmLio3g7ElqSTMBb07luZK2pph/nKZ+QdK4VO5Kx9uK7PfZkjRF0hOSXkzjvaTs4yzp8+nveqekxySdV7ZxlvSwpCOSdlbUVT2uklal9nslraqmD02dICoWNfoocAWwUtIVxfaqZnqAOyLiHWQTIX4mxXYn0BER84GOVIbsM5iftluBB899l2tmLbC7orweuDfF/AqwJtWvAV6JiMvIZhZef057WTvfAp6KiLcDV5PFXtpxlnQx2SJj7RFxJdlUPiso3zh/H7huQF1V4yppGnAX8C6yNXru6ksqwxIRTbsBS4AtFeV1wLqi+1WnWH9BtrrfHmBWqpsF7En7DwErK9r3t2ukjWzW4A5gGbCZbNr5o0DrwDEnmydsSdpvTe1UdAxVxjsZODCw32UeZ06tNTMtjdtm4CNlHGegDdh5tuMKrAQeqqj/v3Zn2pr6DIJhLGpUBumUeiHZsq4zI+IwQHq9KDUry2dxH/Al4GQqXwi8GhE9qVwZV3/M6fix1L6RzAO6gUfSZbXvSppAicc5Iv4GfAP4K9naMceA7ZR7nPtUO64jGu9mTxBDLmpUBpImAj8FPhcRrw3VNKeuoT4LSTcARyJbQ6S/OqdpDONYo2gFFgEPRrbA1uucuuyQp+FjTpdIbgLmAm8BJpBdYhmoTON8JoPFOKLYmz1BnGlRo4YmaSxZcng0Ip5M1f+QNCsdnwUcSfVl+CzeDdyYFqB6nOwy033AFEl9U9tXxtUfczp+AdBo08ofAg5FxNZUfoIsYZR5nD8EHIiI7og4ATwJLKXc49yn2nEd0Xg3e4IYclGjRiZJwPeA3RHxzYpDm4C+JxlWkd2b6Kv/ZHoaYjFwrO9UtlFExLqImB3ZAlQrgF9FxC3AM8Dy1GxgzH2fxfLUvqG+WUbEy8BLkt6Wqj4IvECJx5ns0tJiSeenv/O+mEs7zhWqHdctwLWSpqYzr2tT3fAUfROm6A24Hvgz2bKmXym6PzWM6z1kp5LPAzvSdj3ZtdcOYG96nZbai+yJrn3An8ieECk8jhHE/wFgc9qfB2wDuoCfAONS/Xmp3JWOzyu632cZ6wKgM431z4GpZR9n4KvAi8BO4IfAuLKNM/AY2T2WE2RnAmvOZlyBT6XYu4DV1fTBU22YmVmuZr/EZGZmg3CCMDOzXE4QZmaWywnCzMxyOUGYmVkuJwizKkjqlbSjYqvZDMCS2ipn7jQrWuuZm5hZhTciYkHRnTA7F3wGYVYDkg5KWi9pW9ouS/WXSupIc/R3SLok1c+U9DNJf0zb0vRWLZK+k9Y6+KWk8YUFZU3PCcKsOuMHXGK6ueLYaxFxDXA/2RxQpP0fRMRVwKPAhlS/AXg2Iq4mmztpV6qfDzwQEe8EXgU+Vud4zAbl/6Q2q4Kkf0fExJz6g8CyiNifJkl8OSIulHSUbP7+E6n+cERMl9QNzI6I4xXv0QY8HdliMEj6MjA2Ir5W/8jMTuczCLPaiUH2B2uT53jFfi++T2gFcoIwq52bK15/l/Z/SzazLMAtwG/SfgdwG/SvoT35XHXSbLj87cSsOuMl7agoPxURfY+6jpO0leyL18pUdzvwsKQvkq38tjrVrwU2SlpDdqZwG9nMnWajhu9BmNVAugfRHhFHi+6LWa34EpOZmeXyGYSZmeXyGYSZmeVygjAzs1xOEGZmlssJwszMcjlBmJlZrv8BFuR4PC4I/S8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(his.history['accuracy'])\n",
    "plt.plot(his.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 绘制训练 & 验证的损失值\n",
    "plt.plot(his.history['loss'])\n",
    "plt.plot(his.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69574095\n",
      "Iteration 2, loss = 0.69425916\n",
      "Iteration 3, loss = 0.69363412\n",
      "Iteration 4, loss = 0.69153250\n",
      "Iteration 5, loss = 0.69011119\n",
      "Iteration 6, loss = 0.68832007\n",
      "Iteration 7, loss = 0.68819703\n",
      "Iteration 8, loss = 0.68611179\n",
      "Iteration 9, loss = 0.68626928\n",
      "Iteration 10, loss = 0.68490772\n",
      "Iteration 11, loss = 0.68470455\n",
      "Iteration 12, loss = 0.68427359\n",
      "Iteration 13, loss = 0.68398524\n",
      "Iteration 14, loss = 0.68273149\n",
      "Iteration 15, loss = 0.68229449\n",
      "Iteration 16, loss = 0.68341625\n",
      "Iteration 17, loss = 0.68187523\n",
      "Iteration 18, loss = 0.68297108\n",
      "Iteration 19, loss = 0.68336388\n",
      "Iteration 20, loss = 0.68212396\n",
      "Iteration 21, loss = 0.68057425\n",
      "Iteration 22, loss = 0.68388512\n",
      "Iteration 23, loss = 0.68227366\n",
      "Iteration 24, loss = 0.68106120\n",
      "Iteration 25, loss = 0.68047955\n",
      "Iteration 26, loss = 0.68119256\n",
      "Iteration 27, loss = 0.68108408\n",
      "Iteration 28, loss = 0.68007533\n",
      "Iteration 29, loss = 0.67976560\n",
      "Iteration 30, loss = 0.68095474\n",
      "Iteration 31, loss = 0.67961423\n",
      "Iteration 32, loss = 0.67915208\n",
      "Iteration 33, loss = 0.67879032\n",
      "Iteration 34, loss = 0.67911530\n",
      "Iteration 35, loss = 0.67863974\n",
      "Iteration 36, loss = 0.67831887\n",
      "Iteration 37, loss = 0.67846153\n",
      "Iteration 38, loss = 0.67836182\n",
      "Iteration 39, loss = 0.67983024\n",
      "Iteration 40, loss = 0.67905810\n",
      "Iteration 41, loss = 0.67796985\n",
      "Iteration 42, loss = 0.67749778\n",
      "Iteration 43, loss = 0.67784916\n",
      "Iteration 44, loss = 0.67752731\n",
      "Iteration 45, loss = 0.67809051\n",
      "Iteration 46, loss = 0.67981808\n",
      "Iteration 47, loss = 0.67950296\n",
      "Iteration 48, loss = 0.67952571\n",
      "Iteration 49, loss = 0.68053598\n",
      "Iteration 50, loss = 0.67991195\n",
      "Iteration 51, loss = 0.67695874\n",
      "Iteration 52, loss = 0.67807880\n",
      "Iteration 53, loss = 0.67619591\n",
      "Iteration 54, loss = 0.67655484\n",
      "Iteration 55, loss = 0.67781139\n",
      "Iteration 56, loss = 0.67578101\n",
      "Iteration 57, loss = 0.67575520\n",
      "Iteration 58, loss = 0.67701992\n",
      "Iteration 59, loss = 0.67524459\n",
      "Iteration 60, loss = 0.67664048\n",
      "Iteration 61, loss = 0.67607139\n",
      "Iteration 62, loss = 0.67534859\n",
      "Iteration 63, loss = 0.67459148\n",
      "Iteration 64, loss = 0.67583036\n",
      "Iteration 65, loss = 0.67401420\n",
      "Iteration 66, loss = 0.67564283\n",
      "Iteration 67, loss = 0.67479560\n",
      "Iteration 68, loss = 0.67405347\n",
      "Iteration 69, loss = 0.67379349\n",
      "Iteration 70, loss = 0.67435388\n",
      "Iteration 71, loss = 0.67482324\n",
      "Iteration 72, loss = 0.67573453\n",
      "Iteration 73, loss = 0.67424637\n",
      "Iteration 74, loss = 0.68147313\n",
      "Iteration 75, loss = 0.67451816\n",
      "Iteration 76, loss = 0.67586903\n",
      "Iteration 77, loss = 0.67407516\n",
      "Iteration 78, loss = 0.67434623\n",
      "Iteration 79, loss = 0.67328509\n",
      "Iteration 80, loss = 0.67246742\n",
      "Iteration 81, loss = 0.67297732\n",
      "Iteration 82, loss = 0.67360452\n",
      "Iteration 83, loss = 0.67357873\n",
      "Iteration 84, loss = 0.67167457\n",
      "Iteration 85, loss = 0.67211448\n",
      "Iteration 86, loss = 0.67178757\n",
      "Iteration 87, loss = 0.67129122\n",
      "Iteration 88, loss = 0.67144137\n",
      "Iteration 89, loss = 0.67124657\n",
      "Iteration 90, loss = 0.67115253\n",
      "Iteration 91, loss = 0.67299207\n",
      "Iteration 92, loss = 0.67368663\n",
      "Iteration 93, loss = 0.67085443\n",
      "Iteration 94, loss = 0.67225873\n",
      "Iteration 95, loss = 0.67220600\n",
      "Iteration 96, loss = 0.67135280\n",
      "Iteration 97, loss = 0.67161684\n",
      "Iteration 98, loss = 0.67207256\n",
      "Iteration 99, loss = 0.67092246\n",
      "Iteration 100, loss = 0.67034179\n",
      "Iteration 101, loss = 0.67039952\n",
      "Iteration 102, loss = 0.66939924\n",
      "Iteration 103, loss = 0.66955219\n",
      "Iteration 104, loss = 0.67064338\n",
      "Iteration 105, loss = 0.66941960\n",
      "Iteration 106, loss = 0.66976823\n",
      "Iteration 107, loss = 0.66878622\n",
      "Iteration 108, loss = 0.66813227\n",
      "Iteration 109, loss = 0.66824849\n",
      "Iteration 110, loss = 0.66859867\n",
      "Iteration 111, loss = 0.66776512\n",
      "Iteration 112, loss = 0.66732085\n",
      "Iteration 113, loss = 0.66724731\n",
      "Iteration 114, loss = 0.66695293\n",
      "Iteration 115, loss = 0.66698394\n",
      "Iteration 116, loss = 0.66782859\n",
      "Iteration 117, loss = 0.66586104\n",
      "Iteration 118, loss = 0.66773518\n",
      "Iteration 119, loss = 0.66667257\n",
      "Iteration 120, loss = 0.67206295\n",
      "Iteration 121, loss = 0.66636355\n",
      "Iteration 122, loss = 0.66778170\n",
      "Iteration 123, loss = 0.67112468\n",
      "Iteration 124, loss = 0.66638871\n",
      "Iteration 125, loss = 0.66551216\n",
      "Iteration 126, loss = 0.66449495\n",
      "Iteration 127, loss = 0.66539325\n",
      "Iteration 128, loss = 0.66575105\n",
      "Iteration 129, loss = 0.66436778\n",
      "Iteration 130, loss = 0.66403918\n",
      "Iteration 131, loss = 0.66342047\n",
      "Iteration 132, loss = 0.66299074\n",
      "Iteration 133, loss = 0.66339932\n",
      "Iteration 134, loss = 0.66304833\n",
      "Iteration 135, loss = 0.66494109\n",
      "Iteration 136, loss = 0.66319903\n",
      "Iteration 137, loss = 0.66183252\n",
      "Iteration 138, loss = 0.66180879\n",
      "Iteration 139, loss = 0.66184991\n",
      "Iteration 140, loss = 0.66116463\n",
      "Iteration 141, loss = 0.66081352\n",
      "Iteration 142, loss = 0.66090238\n",
      "Iteration 143, loss = 0.66081194\n",
      "Iteration 144, loss = 0.66100354\n",
      "Iteration 145, loss = 0.66144554\n",
      "Iteration 146, loss = 0.66066124\n",
      "Iteration 147, loss = 0.66209267\n",
      "Iteration 148, loss = 0.65903906\n",
      "Iteration 149, loss = 0.66552670\n",
      "Iteration 150, loss = 0.66143976\n",
      "Iteration 151, loss = 0.66140907\n",
      "Iteration 152, loss = 0.65750539\n",
      "Iteration 153, loss = 0.65928490\n",
      "Iteration 154, loss = 0.65780469\n",
      "Iteration 155, loss = 0.65678130\n",
      "Iteration 156, loss = 0.65700742\n",
      "Iteration 157, loss = 0.65751345\n",
      "Iteration 158, loss = 0.65874236\n",
      "Iteration 159, loss = 0.65721065\n",
      "Iteration 160, loss = 0.65601830\n",
      "Iteration 161, loss = 0.65555991\n",
      "Iteration 162, loss = 0.65575727\n",
      "Iteration 163, loss = 0.65409149\n",
      "Iteration 164, loss = 0.65420242\n",
      "Iteration 165, loss = 0.65347838\n",
      "Iteration 166, loss = 0.65895036\n",
      "Iteration 167, loss = 0.65298078\n",
      "Iteration 168, loss = 0.65436856\n",
      "Iteration 169, loss = 0.65402214\n",
      "Iteration 170, loss = 0.65388855\n",
      "Iteration 171, loss = 0.65190606\n",
      "Iteration 172, loss = 0.65366860\n",
      "Iteration 173, loss = 0.65372780\n",
      "Iteration 174, loss = 0.65056280\n",
      "Iteration 175, loss = 0.64973775\n",
      "Iteration 176, loss = 0.65124525\n",
      "Iteration 177, loss = 0.65343252\n",
      "Iteration 178, loss = 0.65051226\n",
      "Iteration 179, loss = 0.64894028\n",
      "Iteration 180, loss = 0.64712448\n",
      "Iteration 181, loss = 0.64974153\n",
      "Iteration 182, loss = 0.64860208\n",
      "Iteration 183, loss = 0.65503776\n",
      "Iteration 184, loss = 0.64657104\n",
      "Iteration 185, loss = 0.64641175\n",
      "Iteration 186, loss = 0.64964870\n",
      "Iteration 187, loss = 0.64537291\n",
      "Iteration 188, loss = 0.64448787\n",
      "Iteration 189, loss = 0.64490538\n",
      "Iteration 190, loss = 0.64507848\n",
      "Iteration 191, loss = 0.64444026\n",
      "Iteration 192, loss = 0.64275356\n",
      "Iteration 193, loss = 0.64204780\n",
      "Iteration 194, loss = 0.64273188\n",
      "Iteration 195, loss = 0.65289012\n",
      "Iteration 196, loss = 0.64075980\n",
      "Iteration 197, loss = 0.64356066\n",
      "Iteration 198, loss = 0.63897752\n",
      "Iteration 199, loss = 0.63847872\n",
      "Iteration 200, loss = 0.63750233\n",
      "Iteration 201, loss = 0.63896685\n",
      "Iteration 202, loss = 0.63603709\n",
      "Iteration 203, loss = 0.63598708\n",
      "Iteration 204, loss = 0.63702128\n",
      "Iteration 205, loss = 0.63482031\n",
      "Iteration 206, loss = 0.63660355\n",
      "Iteration 207, loss = 0.63471568\n",
      "Iteration 208, loss = 0.63436295\n",
      "Iteration 209, loss = 0.63461213\n",
      "Iteration 210, loss = 0.63252402\n",
      "Iteration 211, loss = 0.63226196\n",
      "Iteration 212, loss = 0.63068255\n",
      "Iteration 213, loss = 0.62995553\n",
      "Iteration 214, loss = 0.62957291\n",
      "Iteration 215, loss = 0.62981580\n",
      "Iteration 216, loss = 0.62693125\n",
      "Iteration 217, loss = 0.62956523\n",
      "Iteration 218, loss = 0.62449301\n",
      "Iteration 219, loss = 0.62404705\n",
      "Iteration 220, loss = 0.62978556\n",
      "Iteration 221, loss = 0.62838931\n",
      "Iteration 222, loss = 0.62172806\n",
      "Iteration 223, loss = 0.62055662\n",
      "Iteration 224, loss = 0.62364717\n",
      "Iteration 225, loss = 0.62815365\n",
      "Iteration 226, loss = 0.61689769\n",
      "Iteration 227, loss = 0.61784967\n",
      "Iteration 228, loss = 0.63968581\n",
      "Iteration 229, loss = 0.62031731\n",
      "Iteration 230, loss = 0.61680961\n",
      "Iteration 231, loss = 0.61999417\n",
      "Iteration 232, loss = 0.61224381\n",
      "Iteration 233, loss = 0.61161583\n",
      "Iteration 234, loss = 0.61495610\n",
      "Iteration 235, loss = 0.61544959\n",
      "Iteration 236, loss = 0.62554181\n",
      "Iteration 237, loss = 0.61434458\n",
      "Iteration 238, loss = 0.61196468\n",
      "Iteration 239, loss = 0.60456361\n",
      "Iteration 240, loss = 0.60649219\n",
      "Iteration 241, loss = 0.60203949\n",
      "Iteration 242, loss = 0.60046710\n",
      "Iteration 243, loss = 0.60066732\n",
      "Iteration 244, loss = 0.61345849\n",
      "Iteration 245, loss = 0.60618544\n",
      "Iteration 246, loss = 0.60369395\n",
      "Iteration 247, loss = 0.59544596\n",
      "Iteration 248, loss = 0.59274759\n",
      "Iteration 249, loss = 0.59550206\n",
      "Iteration 250, loss = 0.59012733\n",
      "Iteration 251, loss = 0.62602654\n",
      "Iteration 252, loss = 0.59227611\n",
      "Iteration 253, loss = 0.59444513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 254, loss = 0.59365415\n",
      "Iteration 255, loss = 0.58597712\n",
      "Iteration 256, loss = 0.58728526\n",
      "Iteration 257, loss = 0.58321028\n",
      "Iteration 258, loss = 0.58658345\n",
      "Iteration 259, loss = 0.59543038\n",
      "Iteration 260, loss = 0.57766801\n",
      "Iteration 261, loss = 0.58110174\n",
      "Iteration 262, loss = 0.57198658\n",
      "Iteration 263, loss = 0.56904810\n",
      "Iteration 264, loss = 0.57283524\n",
      "Iteration 265, loss = 0.57250617\n",
      "Iteration 266, loss = 0.56903355\n",
      "Iteration 267, loss = 0.56962065\n",
      "Iteration 268, loss = 0.56992611\n",
      "Iteration 269, loss = 0.57016655\n",
      "Iteration 270, loss = 0.55782236\n",
      "Iteration 271, loss = 0.55650096\n",
      "Iteration 272, loss = 0.55841283\n",
      "Iteration 273, loss = 0.55930665\n",
      "Iteration 274, loss = 0.58357404\n",
      "Iteration 275, loss = 0.58848407\n",
      "Iteration 276, loss = 0.54955736\n",
      "Iteration 277, loss = 0.56336814\n",
      "Iteration 278, loss = 0.56989951\n",
      "Iteration 279, loss = 0.61163661\n",
      "Iteration 280, loss = 0.54376734\n",
      "Iteration 281, loss = 0.54785401\n",
      "Iteration 282, loss = 0.56595107\n",
      "Iteration 283, loss = 0.53613942\n",
      "Iteration 284, loss = 0.53955836\n",
      "Iteration 285, loss = 0.56756326\n",
      "Iteration 286, loss = 0.53021564\n",
      "Iteration 287, loss = 0.56846303\n",
      "Iteration 288, loss = 0.53839878\n",
      "Iteration 289, loss = 0.52560286\n",
      "Iteration 290, loss = 0.53648365\n",
      "Iteration 291, loss = 0.52247254\n",
      "Iteration 292, loss = 0.53613500\n",
      "Iteration 293, loss = 0.51249754\n",
      "Iteration 294, loss = 0.50859348\n",
      "Iteration 295, loss = 0.50767592\n",
      "Iteration 296, loss = 0.50547501\n",
      "Iteration 297, loss = 0.50637596\n",
      "Iteration 298, loss = 0.50342317\n",
      "Iteration 299, loss = 0.58206862\n",
      "Iteration 300, loss = 0.50156650\n",
      "Iteration 301, loss = 0.50526611\n",
      "Iteration 302, loss = 0.50322009\n",
      "Iteration 303, loss = 0.48692776\n",
      "Iteration 304, loss = 0.50399693\n",
      "Iteration 305, loss = 0.64595159\n",
      "Iteration 306, loss = 0.52922755\n",
      "Iteration 307, loss = 0.53294295\n",
      "Iteration 308, loss = 0.48922192\n",
      "Iteration 309, loss = 0.48437990\n",
      "Iteration 310, loss = 0.49140414\n",
      "Iteration 311, loss = 0.48996251\n",
      "Iteration 312, loss = 0.49654619\n",
      "Iteration 313, loss = 0.52642667\n",
      "Iteration 314, loss = 0.48605293\n",
      "Iteration 315, loss = 0.46750513\n",
      "Iteration 316, loss = 0.46381594\n",
      "Iteration 317, loss = 0.46104164\n",
      "Iteration 318, loss = 0.50672274\n",
      "Iteration 319, loss = 0.56967359\n",
      "Iteration 320, loss = 0.45559890\n",
      "Iteration 321, loss = 0.44800948\n",
      "Iteration 322, loss = 0.46206256\n",
      "Iteration 323, loss = 0.46509908\n",
      "Iteration 324, loss = 0.44500467\n",
      "Iteration 325, loss = 0.43535618\n",
      "Iteration 326, loss = 0.43705942\n",
      "Iteration 327, loss = 0.48452672\n",
      "Iteration 328, loss = 0.44503815\n",
      "Iteration 329, loss = 0.52055422\n",
      "Iteration 330, loss = 0.42685454\n",
      "Iteration 331, loss = 0.43888626\n",
      "Iteration 332, loss = 0.42312132\n",
      "Iteration 333, loss = 0.43853205\n",
      "Iteration 334, loss = 0.60697199\n",
      "Iteration 335, loss = 0.42087967\n",
      "Iteration 336, loss = 0.42956926\n",
      "Iteration 337, loss = 0.48844609\n",
      "Iteration 338, loss = 0.42767181\n",
      "Iteration 339, loss = 0.41352516\n",
      "Iteration 340, loss = 0.45138962\n",
      "Iteration 341, loss = 0.49097409\n",
      "Iteration 342, loss = 0.44586472\n",
      "Iteration 343, loss = 0.40177215\n",
      "Iteration 344, loss = 0.42040467\n",
      "Iteration 345, loss = 0.41771406\n",
      "Iteration 346, loss = 0.42974884\n",
      "Iteration 347, loss = 0.40639536\n",
      "Iteration 348, loss = 0.50794988\n",
      "Iteration 349, loss = 0.41887323\n",
      "Iteration 350, loss = 0.39643079\n",
      "Iteration 351, loss = 0.38687062\n",
      "Iteration 352, loss = 0.38367468\n",
      "Iteration 353, loss = 0.40673537\n",
      "Iteration 354, loss = 0.37488596\n",
      "Iteration 355, loss = 0.37153529\n",
      "Iteration 356, loss = 0.42339535\n",
      "Iteration 357, loss = 0.37220351\n",
      "Iteration 358, loss = 0.41631701\n",
      "Iteration 359, loss = 0.36696590\n",
      "Iteration 360, loss = 0.36109054\n",
      "Iteration 361, loss = 0.48483494\n",
      "Iteration 362, loss = 0.36420989\n",
      "Iteration 363, loss = 0.35816535\n",
      "Iteration 364, loss = 0.38528730\n",
      "Iteration 365, loss = 0.44766808\n",
      "Iteration 366, loss = 0.37757801\n",
      "Iteration 367, loss = 0.35621557\n",
      "Iteration 368, loss = 0.38171012\n",
      "Iteration 369, loss = 0.43033038\n",
      "Iteration 370, loss = 0.37729909\n",
      "Iteration 371, loss = 0.35601801\n",
      "Iteration 372, loss = 0.35118931\n",
      "Iteration 373, loss = 0.35985097\n",
      "Iteration 374, loss = 0.33890102\n",
      "Iteration 375, loss = 0.35977279\n",
      "Iteration 376, loss = 0.44117856\n",
      "Iteration 377, loss = 0.33213863\n",
      "Iteration 378, loss = 0.36144440\n",
      "Iteration 379, loss = 0.37190825\n",
      "Iteration 380, loss = 0.34856006\n",
      "Iteration 381, loss = 0.32293512\n",
      "Iteration 382, loss = 0.33567063\n",
      "Iteration 383, loss = 0.31705881\n",
      "Iteration 384, loss = 0.35015033\n",
      "Iteration 385, loss = 0.42352279\n",
      "Iteration 386, loss = 0.34241293\n",
      "Iteration 387, loss = 0.31513262\n",
      "Iteration 388, loss = 0.30513107\n",
      "Iteration 389, loss = 0.36122631\n",
      "Iteration 390, loss = 0.31332964\n",
      "Iteration 391, loss = 0.30428646\n",
      "Iteration 392, loss = 0.30889635\n",
      "Iteration 393, loss = 0.48582069\n",
      "Iteration 394, loss = 0.32416690\n",
      "Iteration 395, loss = 0.37107023\n",
      "Iteration 396, loss = 0.35402370\n",
      "Iteration 397, loss = 0.30780884\n",
      "Iteration 398, loss = 0.30393605\n",
      "Iteration 399, loss = 0.31950364\n",
      "Iteration 400, loss = 0.35931567\n",
      "Iteration 401, loss = 0.29604350\n",
      "Iteration 402, loss = 0.33046194\n",
      "Iteration 403, loss = 0.32428407\n",
      "Iteration 404, loss = 0.31873032\n",
      "Iteration 405, loss = 0.36634023\n",
      "Iteration 406, loss = 0.38548030\n",
      "Iteration 407, loss = 0.31877593\n",
      "Iteration 408, loss = 0.31301032\n",
      "Iteration 409, loss = 0.30696252\n",
      "Iteration 410, loss = 0.28381112\n",
      "Iteration 411, loss = 0.27811987\n",
      "Iteration 412, loss = 0.29605090\n",
      "Iteration 413, loss = 0.40541137\n",
      "Iteration 414, loss = 0.37480095\n",
      "Iteration 415, loss = 0.29067412\n",
      "Iteration 416, loss = 0.28429377\n",
      "Iteration 417, loss = 0.29389290\n",
      "Iteration 418, loss = 0.31652480\n",
      "Iteration 419, loss = 0.31776107\n",
      "Iteration 420, loss = 0.31548698\n",
      "Iteration 421, loss = 0.27578828\n",
      "Iteration 422, loss = 0.33714763\n",
      "Iteration 423, loss = 0.33066323\n",
      "Iteration 424, loss = 0.26004924\n",
      "Iteration 425, loss = 0.27023891\n",
      "Iteration 426, loss = 0.42596822\n",
      "Iteration 427, loss = 0.28415742\n",
      "Iteration 428, loss = 0.27768088\n",
      "Iteration 429, loss = 0.32536027\n",
      "Iteration 430, loss = 0.27997705\n",
      "Iteration 431, loss = 0.27684645\n",
      "Iteration 432, loss = 0.26632331\n",
      "Iteration 433, loss = 0.33676219\n",
      "Iteration 434, loss = 0.25915683\n",
      "Iteration 435, loss = 0.33234574\n",
      "Iteration 436, loss = 0.25532471\n",
      "Iteration 437, loss = 0.26125332\n",
      "Iteration 438, loss = 0.33926181\n",
      "Iteration 439, loss = 0.25870519\n",
      "Iteration 440, loss = 0.37747820\n",
      "Iteration 441, loss = 0.27279372\n",
      "Iteration 442, loss = 0.25806644\n",
      "Iteration 443, loss = 0.27163249\n",
      "Iteration 444, loss = 0.26709706\n",
      "Iteration 445, loss = 0.24551981\n",
      "Iteration 446, loss = 0.39349008\n",
      "Iteration 447, loss = 0.26200718\n",
      "Iteration 448, loss = 0.29310685\n",
      "Iteration 449, loss = 0.27409285\n",
      "Iteration 450, loss = 0.25187813\n",
      "Iteration 451, loss = 0.24463147\n",
      "Iteration 452, loss = 0.24729510\n",
      "Iteration 453, loss = 0.27928814\n",
      "Iteration 454, loss = 0.23913144\n",
      "Iteration 455, loss = 0.30083333\n",
      "Iteration 456, loss = 0.59438598\n",
      "Iteration 457, loss = 0.26322548\n",
      "Iteration 458, loss = 0.24894050\n",
      "Iteration 459, loss = 0.24724328\n",
      "Iteration 460, loss = 0.26045767\n",
      "Iteration 461, loss = 0.25532553\n",
      "Iteration 462, loss = 0.27288406\n",
      "Iteration 463, loss = 0.24878407\n",
      "Iteration 464, loss = 0.24773756\n",
      "Iteration 465, loss = 0.24872830\n",
      "Iteration 466, loss = 0.26653425\n",
      "Iteration 467, loss = 0.28712216\n",
      "Iteration 468, loss = 0.25442193\n",
      "Iteration 469, loss = 0.23062563\n",
      "Iteration 470, loss = 0.26430656\n",
      "Iteration 471, loss = 0.23546007\n",
      "Iteration 472, loss = 0.24047615\n",
      "Iteration 473, loss = 0.22752350\n",
      "Iteration 474, loss = 0.25183701\n",
      "Iteration 475, loss = 0.34241250\n",
      "Iteration 476, loss = 0.23875148\n",
      "Iteration 477, loss = 0.22493582\n",
      "Iteration 478, loss = 0.22222788\n",
      "Iteration 479, loss = 0.22115826\n",
      "Iteration 480, loss = 0.24479202\n",
      "Iteration 481, loss = 0.26530246\n",
      "Iteration 482, loss = 0.41426893\n",
      "Iteration 483, loss = 0.23938229\n",
      "Iteration 484, loss = 0.42887564\n",
      "Iteration 485, loss = 0.38042084\n",
      "Iteration 486, loss = 0.22732041\n",
      "Iteration 487, loss = 0.23093510\n",
      "Iteration 488, loss = 0.22410969\n",
      "Iteration 489, loss = 0.30890958\n",
      "Iteration 490, loss = 0.26649545\n",
      "Iteration 491, loss = 0.22557032\n",
      "Iteration 492, loss = 0.23303807\n",
      "Iteration 493, loss = 0.23180370\n",
      "Iteration 494, loss = 0.21758256\n",
      "Iteration 495, loss = 0.21649544\n",
      "Iteration 496, loss = 0.23113668\n",
      "Iteration 497, loss = 0.24869099\n",
      "Iteration 498, loss = 0.25424115\n",
      "Iteration 499, loss = 0.31379664\n",
      "Iteration 500, loss = 0.21038599\n",
      "Iteration 501, loss = 0.21858552\n",
      "Iteration 502, loss = 0.31186586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 503, loss = 0.33062367\n",
      "Iteration 504, loss = 0.21349045\n",
      "Iteration 505, loss = 0.23410984\n",
      "Iteration 506, loss = 0.27812241\n",
      "Iteration 507, loss = 0.20883769\n",
      "Iteration 508, loss = 0.24468122\n",
      "Iteration 509, loss = 0.21720079\n",
      "Iteration 510, loss = 0.23540946\n",
      "Iteration 511, loss = 0.33094279\n",
      "Iteration 512, loss = 0.21391450\n",
      "Iteration 513, loss = 0.20436208\n",
      "Iteration 514, loss = 0.20438716\n",
      "Iteration 515, loss = 0.30765281\n",
      "Iteration 516, loss = 0.29258277\n",
      "Iteration 517, loss = 0.31656717\n",
      "Iteration 518, loss = 0.20838064\n",
      "Iteration 519, loss = 0.22553764\n",
      "Iteration 520, loss = 0.28923613\n",
      "Iteration 521, loss = 0.19958387\n",
      "Iteration 522, loss = 0.19584279\n",
      "Iteration 523, loss = 0.20109872\n",
      "Iteration 524, loss = 0.22556482\n",
      "Iteration 525, loss = 0.27589150\n",
      "Iteration 526, loss = 0.61385750\n",
      "Iteration 527, loss = 0.22945554\n",
      "Iteration 528, loss = 0.21343286\n",
      "Iteration 529, loss = 0.21545690\n",
      "Iteration 530, loss = 0.21308302\n",
      "Iteration 531, loss = 0.26199017\n",
      "Iteration 532, loss = 0.21960397\n",
      "Iteration 533, loss = 0.26023901\n",
      "Iteration 534, loss = 0.27802914\n",
      "Iteration 535, loss = 0.21330879\n",
      "Iteration 536, loss = 0.21778463\n",
      "Iteration 537, loss = 0.21425937\n",
      "Iteration 538, loss = 0.20512166\n",
      "Iteration 539, loss = 0.31691968\n",
      "Iteration 540, loss = 0.21446849\n",
      "Iteration 541, loss = 0.20579154\n",
      "Iteration 542, loss = 0.19665643\n",
      "Iteration 543, loss = 0.20546035\n",
      "Iteration 544, loss = 0.38742765\n",
      "Iteration 545, loss = 0.19938544\n",
      "Iteration 546, loss = 0.20645369\n",
      "Iteration 547, loss = 0.21843539\n",
      "Iteration 548, loss = 0.29462267\n",
      "Iteration 549, loss = 0.22563687\n",
      "Iteration 550, loss = 0.25248775\n",
      "Iteration 551, loss = 0.20693973\n",
      "Iteration 552, loss = 0.21518768\n",
      "Iteration 553, loss = 0.19861967\n",
      "Iteration 554, loss = 0.28041895\n",
      "Iteration 555, loss = 0.19809129\n",
      "Iteration 556, loss = 0.22677350\n",
      "Iteration 557, loss = 0.23346304\n",
      "Iteration 558, loss = 0.21439359\n",
      "Iteration 559, loss = 0.23565876\n",
      "Iteration 560, loss = 0.39430930\n",
      "Iteration 561, loss = 0.19407220\n",
      "Iteration 562, loss = 0.22008395\n",
      "Iteration 563, loss = 0.20131988\n",
      "Iteration 564, loss = 0.21723755\n",
      "Iteration 565, loss = 0.18911086\n",
      "Iteration 566, loss = 0.27228584\n",
      "Iteration 567, loss = 0.19557950\n",
      "Iteration 568, loss = 0.30681582\n",
      "Iteration 569, loss = 0.19201872\n",
      "Iteration 570, loss = 0.31928600\n",
      "Iteration 571, loss = 0.27183752\n",
      "Iteration 572, loss = 0.19279256\n",
      "Iteration 573, loss = 0.18956548\n",
      "Iteration 574, loss = 0.20800678\n",
      "Iteration 575, loss = 0.18761277\n",
      "Iteration 576, loss = 0.18534301\n",
      "Iteration 577, loss = 0.25313852\n",
      "Iteration 578, loss = 0.22707164\n",
      "Iteration 579, loss = 0.20743979\n",
      "Iteration 580, loss = 0.18040038\n",
      "Iteration 581, loss = 0.37702728\n",
      "Iteration 582, loss = 0.31391074\n",
      "Iteration 583, loss = 0.25575644\n",
      "Iteration 584, loss = 0.19808038\n",
      "Iteration 585, loss = 0.28337285\n",
      "Iteration 586, loss = 0.28030885\n",
      "Iteration 587, loss = 0.29453500\n",
      "Iteration 588, loss = 0.20989077\n",
      "Iteration 589, loss = 0.20452428\n",
      "Iteration 590, loss = 0.19165966\n",
      "Iteration 591, loss = 0.20150099\n",
      "Iteration 592, loss = 0.18565871\n",
      "Iteration 593, loss = 0.21302666\n",
      "Iteration 594, loss = 0.19721877\n",
      "Iteration 595, loss = 0.18033722\n",
      "Iteration 596, loss = 0.17848933\n",
      "Iteration 597, loss = 0.27763785\n",
      "Iteration 598, loss = 0.21672946\n",
      "Iteration 599, loss = 0.17226071\n",
      "Iteration 600, loss = 0.19580635\n",
      "Iteration 601, loss = 0.20675160\n",
      "Iteration 602, loss = 0.21895343\n",
      "Iteration 603, loss = 0.24218502\n",
      "Iteration 604, loss = 0.19692803\n",
      "Iteration 605, loss = 0.21589579\n",
      "Iteration 606, loss = 0.18183264\n",
      "Iteration 607, loss = 0.23137412\n",
      "Iteration 608, loss = 0.47697580\n",
      "Iteration 609, loss = 0.19063847\n",
      "Iteration 610, loss = 0.19797206\n",
      "Iteration 611, loss = 0.24393817\n",
      "Iteration 612, loss = 0.19353548\n",
      "Iteration 613, loss = 0.18148212\n",
      "Iteration 614, loss = 0.21801031\n",
      "Iteration 615, loss = 0.17753968\n",
      "Iteration 616, loss = 0.18273554\n",
      "Iteration 617, loss = 0.17474189\n",
      "Iteration 618, loss = 0.17225766\n",
      "Iteration 619, loss = 0.20319232\n",
      "Iteration 620, loss = 0.29115074\n",
      "Iteration 621, loss = 0.26852750\n",
      "Iteration 622, loss = 0.19543792\n",
      "Iteration 623, loss = 0.17701206\n",
      "Iteration 624, loss = 0.22716687\n",
      "Iteration 625, loss = 0.20948432\n",
      "Iteration 626, loss = 0.29470274\n",
      "Iteration 627, loss = 0.20059841\n",
      "Iteration 628, loss = 0.34813534\n",
      "Iteration 629, loss = 0.18099511\n",
      "Iteration 630, loss = 0.22500618\n",
      "Iteration 631, loss = 0.17097988\n",
      "Iteration 632, loss = 0.16834432\n",
      "Iteration 633, loss = 0.19605787\n",
      "Iteration 634, loss = 0.29728713\n",
      "Iteration 635, loss = 0.31743386\n",
      "Iteration 636, loss = 0.16906535\n",
      "Iteration 637, loss = 0.17260082\n",
      "Iteration 638, loss = 0.16613856\n",
      "Iteration 639, loss = 0.20395303\n",
      "Iteration 640, loss = 0.30133651\n",
      "Iteration 641, loss = 0.19865875\n",
      "Iteration 642, loss = 0.17173923\n",
      "Iteration 643, loss = 0.16596292\n",
      "Iteration 644, loss = 0.25412601\n",
      "Iteration 645, loss = 0.16319147\n",
      "Iteration 646, loss = 0.18046567\n",
      "Iteration 647, loss = 0.18566315\n",
      "Iteration 648, loss = 0.19444205\n",
      "Iteration 649, loss = 0.19796637\n",
      "Iteration 650, loss = 0.42587153\n",
      "Iteration 651, loss = 0.19106986\n",
      "Iteration 652, loss = 0.22997851\n",
      "Iteration 653, loss = 0.18744391\n",
      "Iteration 654, loss = 0.17781743\n",
      "Iteration 655, loss = 0.24563110\n",
      "Iteration 656, loss = 0.18118952\n",
      "Iteration 657, loss = 0.16319571\n",
      "Iteration 658, loss = 0.24189413\n",
      "Iteration 659, loss = 0.50756524\n",
      "Iteration 660, loss = 0.20396398\n",
      "Iteration 661, loss = 0.18964969\n",
      "Iteration 662, loss = 0.18226861\n",
      "Iteration 663, loss = 0.19865356\n",
      "Iteration 664, loss = 0.22852270\n",
      "Iteration 665, loss = 0.17516452\n",
      "Iteration 666, loss = 0.17302351\n",
      "Iteration 667, loss = 0.17915889\n",
      "Iteration 668, loss = 0.19002196\n",
      "Iteration 669, loss = 0.16851936\n",
      "Iteration 670, loss = 0.25716158\n",
      "Iteration 671, loss = 0.23882512\n",
      "Iteration 672, loss = 0.19449083\n",
      "Iteration 673, loss = 0.20463293\n",
      "Iteration 674, loss = 0.22282000\n",
      "Iteration 675, loss = 0.33206133\n",
      "Iteration 676, loss = 0.24659952\n",
      "Iteration 677, loss = 0.16807006\n",
      "Iteration 678, loss = 0.18002614\n",
      "Iteration 679, loss = 0.16377962\n",
      "Iteration 680, loss = 0.17076149\n",
      "Iteration 681, loss = 0.17496860\n",
      "Iteration 682, loss = 0.16777451\n",
      "Iteration 683, loss = 0.28719465\n",
      "Iteration 684, loss = 0.22152646\n",
      "Iteration 685, loss = 0.22363949\n",
      "Iteration 686, loss = 0.17920374\n",
      "Iteration 687, loss = 0.15917594\n",
      "Iteration 688, loss = 0.16856294\n",
      "Iteration 689, loss = 0.19784787\n",
      "Iteration 690, loss = 0.16040685\n",
      "Iteration 691, loss = 0.21721962\n",
      "Iteration 692, loss = 0.16157636\n",
      "Iteration 693, loss = 0.17460349\n",
      "Iteration 694, loss = 0.25871172\n",
      "Iteration 695, loss = 0.21615942\n",
      "Iteration 696, loss = 0.17982829\n",
      "Iteration 697, loss = 0.16158482\n",
      "Iteration 698, loss = 0.15464067\n",
      "Iteration 699, loss = 0.17567559\n",
      "Iteration 700, loss = 0.15596499\n",
      "Iteration 701, loss = 0.14991217\n",
      "Iteration 702, loss = 0.15492418\n",
      "Iteration 703, loss = 0.17470442\n",
      "Iteration 704, loss = 0.32874837\n",
      "Iteration 705, loss = 0.41662728\n",
      "Iteration 706, loss = 0.24732187\n",
      "Iteration 707, loss = 0.18080456\n",
      "Iteration 708, loss = 0.17731975\n",
      "Iteration 709, loss = 0.21881897\n",
      "Iteration 710, loss = 0.17020816\n",
      "Iteration 711, loss = 0.17417759\n",
      "Iteration 712, loss = 0.31215495\n",
      "Iteration 713, loss = 0.17807441\n",
      "Iteration 714, loss = 0.15897142\n",
      "Iteration 715, loss = 0.19110324\n",
      "Iteration 716, loss = 0.22776980\n",
      "Iteration 717, loss = 0.15695112\n",
      "Iteration 718, loss = 0.17237491\n",
      "Iteration 719, loss = 0.17295755\n",
      "Iteration 720, loss = 0.15453938\n",
      "Iteration 721, loss = 0.20828914\n",
      "Iteration 722, loss = 0.29952214\n",
      "Iteration 723, loss = 0.29306266\n",
      "Iteration 724, loss = 0.15354802\n",
      "Iteration 725, loss = 0.15213347\n",
      "Iteration 726, loss = 0.22949282\n",
      "Iteration 727, loss = 0.40472347\n",
      "Iteration 728, loss = 0.18562797\n",
      "Iteration 729, loss = 0.16113401\n",
      "Iteration 730, loss = 0.15363883\n",
      "Iteration 731, loss = 0.16532118\n",
      "Iteration 732, loss = 0.15539212\n",
      "Iteration 733, loss = 0.17616467\n",
      "Iteration 734, loss = 0.25721908\n",
      "Iteration 735, loss = 0.14920477\n",
      "Iteration 736, loss = 0.24987888\n",
      "Iteration 737, loss = 0.27919658\n",
      "Iteration 738, loss = 0.14324486\n",
      "Iteration 739, loss = 0.14213141\n",
      "Iteration 740, loss = 0.16040225\n",
      "Iteration 741, loss = 0.14272375\n",
      "Iteration 742, loss = 0.41127728\n",
      "Iteration 743, loss = 0.32611217\n",
      "Iteration 744, loss = 0.17651723\n",
      "Iteration 745, loss = 0.22458169\n",
      "Iteration 746, loss = 0.16516974\n",
      "Iteration 747, loss = 0.15200246\n",
      "Iteration 748, loss = 0.15078905\n",
      "Iteration 749, loss = 0.18064779\n",
      "Iteration 750, loss = 0.22372896\n",
      "Iteration 751, loss = 0.16285907\n",
      "Iteration 752, loss = 0.18047253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 753, loss = 0.19510544\n",
      "Iteration 754, loss = 0.37542684\n",
      "Iteration 755, loss = 0.21047030\n",
      "Iteration 756, loss = 0.14662522\n",
      "Iteration 757, loss = 0.14656899\n",
      "Iteration 758, loss = 0.16650228\n",
      "Iteration 759, loss = 0.21167485\n",
      "Iteration 760, loss = 0.14666690\n",
      "Iteration 761, loss = 0.15390444\n",
      "Iteration 762, loss = 0.23276051\n",
      "Iteration 763, loss = 0.23830372\n",
      "Iteration 764, loss = 0.14146824\n",
      "Iteration 765, loss = 0.14588290\n",
      "Iteration 766, loss = 0.14220663\n",
      "Iteration 767, loss = 0.20454611\n",
      "Iteration 768, loss = 0.14206165\n",
      "Iteration 769, loss = 0.59227941\n",
      "Iteration 770, loss = 0.16893259\n",
      "Iteration 771, loss = 0.15517981\n",
      "Iteration 772, loss = 0.20611574\n",
      "Iteration 773, loss = 0.19612534\n",
      "Iteration 774, loss = 0.22973788\n",
      "Iteration 775, loss = 0.14678904\n",
      "Iteration 776, loss = 0.20178118\n",
      "Iteration 777, loss = 0.14188318\n",
      "Iteration 778, loss = 0.21700971\n",
      "Iteration 779, loss = 0.14772917\n",
      "Iteration 780, loss = 0.14646551\n",
      "Iteration 781, loss = 0.15615667\n",
      "Iteration 782, loss = 0.20256350\n",
      "Iteration 783, loss = 0.31962449\n",
      "Iteration 784, loss = 0.18883692\n",
      "Iteration 785, loss = 0.15557432\n",
      "Iteration 786, loss = 0.23604164\n",
      "Iteration 787, loss = 0.28451592\n",
      "Iteration 788, loss = 0.21069130\n",
      "Iteration 789, loss = 0.16677614\n",
      "Iteration 790, loss = 0.20909469\n",
      "Iteration 791, loss = 0.18189479\n",
      "Iteration 792, loss = 0.14618972\n",
      "Iteration 793, loss = 0.13196303\n",
      "Iteration 794, loss = 0.13541688\n",
      "Iteration 795, loss = 0.26765319\n",
      "Iteration 796, loss = 0.16604884\n",
      "Iteration 797, loss = 0.13258117\n",
      "Iteration 798, loss = 0.30044656\n",
      "Iteration 799, loss = 0.13505086\n",
      "Iteration 800, loss = 0.24274071\n",
      "Iteration 801, loss = 0.16560434\n",
      "Iteration 802, loss = 0.17079151\n",
      "Iteration 803, loss = 0.16004021\n",
      "Iteration 804, loss = 0.13139847\n",
      "Iteration 805, loss = 0.19699199\n",
      "Iteration 806, loss = 0.13107130\n",
      "Iteration 807, loss = 0.33239739\n",
      "Iteration 808, loss = 0.23333953\n",
      "Iteration 809, loss = 0.18955464\n",
      "Iteration 810, loss = 0.15009352\n",
      "Iteration 811, loss = 0.14118184\n",
      "Iteration 812, loss = 0.13177538\n",
      "Iteration 813, loss = 0.15773394\n",
      "Iteration 814, loss = 0.12476917\n",
      "Iteration 815, loss = 0.15539381\n",
      "Iteration 816, loss = 0.20892804\n",
      "Iteration 817, loss = 0.21334399\n",
      "Iteration 818, loss = 0.13679395\n",
      "Iteration 819, loss = 0.26280528\n",
      "Iteration 820, loss = 0.23519930\n",
      "Iteration 821, loss = 0.21817471\n",
      "Iteration 822, loss = 0.23405942\n",
      "Iteration 823, loss = 0.46315943\n",
      "Iteration 824, loss = 0.13666771\n",
      "Iteration 825, loss = 0.13394110\n",
      "Iteration 826, loss = 0.20218602\n",
      "Iteration 827, loss = 0.13575607\n",
      "Iteration 828, loss = 0.14712200\n",
      "Iteration 829, loss = 0.19733336\n",
      "Iteration 830, loss = 0.25358150\n",
      "Iteration 831, loss = 0.13648661\n",
      "Iteration 832, loss = 0.14298382\n",
      "Iteration 833, loss = 0.16164432\n",
      "Iteration 834, loss = 0.20502041\n",
      "Iteration 835, loss = 0.25207366\n",
      "Iteration 836, loss = 0.27143798\n",
      "Iteration 837, loss = 0.14784667\n",
      "Iteration 838, loss = 0.12706575\n",
      "Iteration 839, loss = 0.14796365\n",
      "Iteration 840, loss = 0.14183384\n",
      "Iteration 841, loss = 0.19275910\n",
      "Iteration 842, loss = 0.12884574\n",
      "Iteration 843, loss = 0.24854458\n",
      "Iteration 844, loss = 0.28982201\n",
      "Iteration 845, loss = 0.14566156\n",
      "Iteration 846, loss = 0.12631941\n",
      "Iteration 847, loss = 0.17739801\n",
      "Iteration 848, loss = 0.14394987\n",
      "Iteration 849, loss = 0.16299133\n",
      "Iteration 850, loss = 0.12315099\n",
      "Iteration 851, loss = 0.11976491\n",
      "Iteration 852, loss = 0.17257349\n",
      "Iteration 853, loss = 0.20202996\n",
      "Iteration 854, loss = 0.32117643\n",
      "Iteration 855, loss = 0.13418498\n",
      "Iteration 856, loss = 0.11611927\n",
      "Iteration 857, loss = 0.23901624\n",
      "Iteration 858, loss = 0.12986428\n",
      "Iteration 859, loss = 0.12117857\n",
      "Iteration 860, loss = 0.15713741\n",
      "Iteration 861, loss = 0.22669279\n",
      "Iteration 862, loss = 0.40766334\n",
      "Iteration 863, loss = 0.14330069\n",
      "Iteration 864, loss = 0.12654426\n",
      "Iteration 865, loss = 0.14861560\n",
      "Iteration 866, loss = 0.12416738\n",
      "Iteration 867, loss = 0.13034409\n",
      "Iteration 868, loss = 0.11406719\n",
      "Iteration 869, loss = 0.11293982\n",
      "Iteration 870, loss = 0.11696405\n",
      "Iteration 871, loss = 0.24015628\n",
      "Iteration 872, loss = 0.33609540\n",
      "Iteration 873, loss = 0.16338573\n",
      "Iteration 874, loss = 0.15368721\n",
      "Iteration 875, loss = 0.18719107\n",
      "Iteration 876, loss = 0.12021631\n",
      "Iteration 877, loss = 0.11497542\n",
      "Iteration 878, loss = 0.12508542\n",
      "Iteration 879, loss = 0.13872215\n",
      "Iteration 880, loss = 0.14944051\n",
      "Iteration 881, loss = 0.18152493\n",
      "Iteration 882, loss = 0.10991530\n",
      "Iteration 883, loss = 0.14111486\n",
      "Iteration 884, loss = 0.25765845\n",
      "Iteration 885, loss = 0.21649414\n",
      "Iteration 886, loss = 0.26756678\n",
      "Iteration 887, loss = 0.35020440\n",
      "Iteration 888, loss = 0.44824361\n",
      "Iteration 889, loss = 0.13767507\n",
      "Iteration 890, loss = 0.13875572\n",
      "Iteration 891, loss = 0.13459270\n",
      "Iteration 892, loss = 0.12299575\n",
      "Iteration 893, loss = 0.11760696\n",
      "Iteration 894, loss = 0.13264043\n",
      "Iteration 895, loss = 0.24479114\n",
      "Iteration 896, loss = 0.18280897\n",
      "Iteration 897, loss = 0.25956703\n",
      "Iteration 898, loss = 0.13655306\n",
      "Iteration 899, loss = 0.15528271\n",
      "Iteration 900, loss = 0.12901295\n",
      "Iteration 901, loss = 0.11444151\n",
      "Iteration 902, loss = 0.16068327\n",
      "Iteration 903, loss = 0.17968280\n",
      "Iteration 904, loss = 0.39314559\n",
      "Iteration 905, loss = 0.16032119\n",
      "Iteration 906, loss = 0.25863763\n",
      "Iteration 907, loss = 0.13922364\n",
      "Iteration 908, loss = 0.11770332\n",
      "Iteration 909, loss = 0.13158516\n",
      "Iteration 910, loss = 0.15384510\n",
      "Iteration 911, loss = 0.12592284\n",
      "Iteration 912, loss = 0.21021480\n",
      "Iteration 913, loss = 0.12277698\n",
      "Iteration 914, loss = 0.26492858\n",
      "Iteration 915, loss = 0.22926896\n",
      "Iteration 916, loss = 0.16487967\n",
      "Iteration 917, loss = 0.35425742\n",
      "Iteration 918, loss = 0.17027159\n",
      "Iteration 919, loss = 0.20777363\n",
      "Iteration 920, loss = 0.12092505\n",
      "Iteration 921, loss = 0.12966764\n",
      "Iteration 922, loss = 0.22474808\n",
      "Iteration 923, loss = 0.11680435\n",
      "Iteration 924, loss = 0.11991332\n",
      "Iteration 925, loss = 0.26583836\n",
      "Iteration 926, loss = 0.12988352\n",
      "Iteration 927, loss = 0.11228634\n",
      "Iteration 928, loss = 0.10412822\n",
      "Iteration 929, loss = 0.14584428\n",
      "Iteration 930, loss = 0.30157649\n",
      "Iteration 931, loss = 0.13143276\n",
      "Iteration 932, loss = 0.37279323\n",
      "Iteration 933, loss = 0.14881563\n",
      "Iteration 934, loss = 0.13462174\n",
      "Iteration 935, loss = 0.12392265\n",
      "Iteration 936, loss = 0.18058473\n",
      "Iteration 937, loss = 0.12130182\n",
      "Iteration 938, loss = 0.10787026\n",
      "Iteration 939, loss = 0.21734474\n",
      "Iteration 940, loss = 0.17794379\n",
      "Iteration 941, loss = 0.32050936\n",
      "Iteration 942, loss = 0.29672662\n",
      "Iteration 943, loss = 0.15252831\n",
      "Iteration 944, loss = 0.12844755\n",
      "Iteration 945, loss = 0.11849815\n",
      "Iteration 946, loss = 0.11616541\n",
      "Iteration 947, loss = 0.21453481\n",
      "Iteration 948, loss = 0.12905994\n",
      "Iteration 949, loss = 0.11997188\n",
      "Iteration 950, loss = 0.13585084\n",
      "Iteration 951, loss = 0.12263880\n",
      "Iteration 952, loss = 0.14602383\n",
      "Iteration 953, loss = 0.32957047\n",
      "Iteration 954, loss = 0.18179001\n",
      "Iteration 955, loss = 0.11534565\n",
      "Iteration 956, loss = 0.23541972\n",
      "Iteration 957, loss = 0.19783886\n",
      "Iteration 958, loss = 0.10920291\n",
      "Iteration 959, loss = 0.13032371\n",
      "Iteration 960, loss = 0.10404106\n",
      "Iteration 961, loss = 0.10527783\n",
      "Iteration 962, loss = 0.18731670\n",
      "Iteration 963, loss = 0.12992208\n",
      "Iteration 964, loss = 0.15328931\n",
      "Iteration 965, loss = 0.21616275\n",
      "Iteration 966, loss = 0.48598044\n",
      "Iteration 967, loss = 0.16217392\n",
      "Iteration 968, loss = 0.13280765\n",
      "Iteration 969, loss = 0.13758326\n",
      "Iteration 970, loss = 0.12117872\n",
      "Iteration 971, loss = 0.11578464\n",
      "Iteration 972, loss = 0.25222119\n",
      "Iteration 973, loss = 0.15569500\n",
      "Iteration 974, loss = 0.24472425\n",
      "Iteration 975, loss = 0.11818294\n",
      "Iteration 976, loss = 0.12292488\n",
      "Iteration 977, loss = 0.12229726\n",
      "Iteration 978, loss = 0.26248484\n",
      "Iteration 979, loss = 0.31928450\n",
      "Iteration 980, loss = 0.12666463\n",
      "Iteration 981, loss = 0.11796609\n",
      "Iteration 982, loss = 0.11412972\n",
      "Iteration 983, loss = 0.12297506\n",
      "Iteration 984, loss = 0.11611124\n",
      "Iteration 985, loss = 0.22776721\n",
      "Iteration 986, loss = 0.23064349\n",
      "Iteration 987, loss = 0.31610175\n",
      "Iteration 988, loss = 0.12183571\n",
      "Iteration 989, loss = 0.13388644\n",
      "Iteration 990, loss = 0.15460492\n",
      "Iteration 991, loss = 0.24004538\n",
      "Iteration 992, loss = 0.27240181\n",
      "Iteration 993, loss = 0.15049018\n",
      "Iteration 994, loss = 0.12009521\n",
      "Iteration 995, loss = 0.12102358\n",
      "Iteration 996, loss = 0.11882085\n",
      "Iteration 997, loss = 0.16555472\n",
      "Iteration 998, loss = 0.12298292\n",
      "Iteration 999, loss = 0.13246451\n",
      "Iteration 1000, loss = 0.10488366\n",
      "Iteration 1001, loss = 0.13230075\n",
      "Iteration 1002, loss = 0.18878091\n",
      "Iteration 1003, loss = 0.29780238\n",
      "Iteration 1004, loss = 0.18337660\n",
      "Iteration 1005, loss = 0.10405556\n",
      "Iteration 1006, loss = 0.15869448\n",
      "Iteration 1007, loss = 0.11112628\n",
      "Iteration 1008, loss = 0.17935281\n",
      "Iteration 1009, loss = 0.10440717\n",
      "Iteration 1010, loss = 0.10454073\n",
      "Iteration 1011, loss = 0.16031755\n",
      "Iteration 1012, loss = 0.27588421\n",
      "Iteration 1013, loss = 0.17990154\n",
      "Iteration 1014, loss = 0.18398186\n",
      "Iteration 1015, loss = 0.10712459\n",
      "Iteration 1016, loss = 0.11360390\n",
      "Iteration 1017, loss = 0.10985908\n",
      "Iteration 1018, loss = 0.20368285\n",
      "Iteration 1019, loss = 0.12058983\n",
      "Iteration 1020, loss = 0.19560015\n",
      "Iteration 1021, loss = 0.10320096\n",
      "Iteration 1022, loss = 0.18778558\n",
      "Iteration 1023, loss = 0.32941466\n",
      "Iteration 1024, loss = 0.35806517\n",
      "Iteration 1025, loss = 0.21135475\n",
      "Iteration 1026, loss = 0.14865610\n",
      "Iteration 1027, loss = 0.13610478\n",
      "Iteration 1028, loss = 0.12480000\n",
      "Iteration 1029, loss = 0.16310261\n",
      "Iteration 1030, loss = 0.17050635\n",
      "Iteration 1031, loss = 0.11418482\n",
      "Iteration 1032, loss = 0.12057225\n",
      "Iteration 1033, loss = 0.11467165\n",
      "Iteration 1034, loss = 0.15981837\n",
      "Iteration 1035, loss = 0.27001717\n",
      "Iteration 1036, loss = 0.19435126\n",
      "Iteration 1037, loss = 0.15621151\n",
      "Iteration 1038, loss = 0.12727263\n",
      "Iteration 1039, loss = 0.12636841\n",
      "Iteration 1040, loss = 0.13456462\n",
      "Iteration 1041, loss = 0.14561090\n",
      "Iteration 1042, loss = 0.20152418\n",
      "Iteration 1043, loss = 0.27857466\n",
      "Iteration 1044, loss = 0.38141369\n",
      "Iteration 1045, loss = 0.10652940\n",
      "Iteration 1046, loss = 0.12378616\n",
      "Iteration 1047, loss = 0.11545215\n",
      "Iteration 1048, loss = 0.12361257\n",
      "Iteration 1049, loss = 0.14508819\n",
      "Iteration 1050, loss = 0.20319762\n",
      "Iteration 1051, loss = 0.10427222\n",
      "Iteration 1052, loss = 0.14468264\n",
      "Iteration 1053, loss = 0.21439452\n",
      "Iteration 1054, loss = 0.11621272\n",
      "Iteration 1055, loss = 0.12905641\n",
      "Iteration 1056, loss = 0.10266390\n",
      "Iteration 1057, loss = 0.10226071\n",
      "Iteration 1058, loss = 0.10233246\n",
      "Iteration 1059, loss = 0.09873340\n",
      "Iteration 1060, loss = 0.12287188\n",
      "Iteration 1061, loss = 0.11605802\n",
      "Iteration 1062, loss = 0.21540373\n",
      "Iteration 1063, loss = 0.12855654\n",
      "Iteration 1064, loss = 0.10887494\n",
      "Iteration 1065, loss = 0.13135643\n",
      "Iteration 1066, loss = 0.29972456\n",
      "Iteration 1067, loss = 0.12283250\n",
      "Iteration 1068, loss = 0.10537057\n",
      "Iteration 1069, loss = 0.11813678\n",
      "Iteration 1070, loss = 0.12342189\n",
      "Iteration 1071, loss = 0.32066214\n",
      "Iteration 1072, loss = 0.23948618\n",
      "Iteration 1073, loss = 0.17649659\n",
      "Iteration 1074, loss = 0.11170505\n",
      "Iteration 1075, loss = 0.09473208\n",
      "Iteration 1076, loss = 0.10088828\n",
      "Iteration 1077, loss = 0.09208564\n",
      "Iteration 1078, loss = 0.13601053\n",
      "Iteration 1079, loss = 0.33979899\n",
      "Iteration 1080, loss = 0.12817847\n",
      "Iteration 1081, loss = 0.11649393\n",
      "Iteration 1082, loss = 0.35882276\n",
      "Iteration 1083, loss = 0.24451218\n",
      "Iteration 1084, loss = 0.13511701\n",
      "Iteration 1085, loss = 0.10613399\n",
      "Iteration 1086, loss = 0.10361134\n",
      "Iteration 1087, loss = 0.16466191\n",
      "Iteration 1088, loss = 0.16937010\n",
      "Iteration 1089, loss = 0.12683174\n",
      "Iteration 1090, loss = 0.09164418\n",
      "Iteration 1091, loss = 0.12468205\n",
      "Iteration 1092, loss = 0.10075357\n",
      "Iteration 1093, loss = 0.09302484\n",
      "Iteration 1094, loss = 0.11783361\n",
      "Iteration 1095, loss = 0.32639492\n",
      "Iteration 1096, loss = 0.13052264\n",
      "Iteration 1097, loss = 0.09252330\n",
      "Iteration 1098, loss = 0.13423475\n",
      "Iteration 1099, loss = 0.21206297\n",
      "Iteration 1100, loss = 0.19319239\n",
      "Iteration 1101, loss = 0.17444374\n",
      "Iteration 1102, loss = 0.12873104\n",
      "Iteration 1103, loss = 0.13900289\n",
      "Iteration 1104, loss = 0.31608404\n",
      "Iteration 1105, loss = 0.11959944\n",
      "Iteration 1106, loss = 0.10607929\n",
      "Iteration 1107, loss = 0.12299313\n",
      "Iteration 1108, loss = 0.09863015\n",
      "Iteration 1109, loss = 0.10763373\n",
      "Iteration 1110, loss = 0.09200773\n",
      "Iteration 1111, loss = 0.09434965\n",
      "Iteration 1112, loss = 0.23959514\n",
      "Iteration 1113, loss = 0.08623654\n",
      "Iteration 1114, loss = 0.14877166\n",
      "Iteration 1115, loss = 0.13696100\n",
      "Iteration 1116, loss = 0.18257621\n",
      "Iteration 1117, loss = 0.25924298\n",
      "Iteration 1118, loss = 0.13722809\n",
      "Iteration 1119, loss = 0.10745098\n",
      "Iteration 1120, loss = 0.28099046\n",
      "Iteration 1121, loss = 0.14163160\n",
      "Iteration 1122, loss = 0.10960514\n",
      "Iteration 1123, loss = 0.20482645\n",
      "Iteration 1124, loss = 0.09751351\n",
      "Iteration 1125, loss = 0.10949507\n",
      "Iteration 1126, loss = 0.26934913\n",
      "Iteration 1127, loss = 0.14300505\n",
      "Iteration 1128, loss = 0.10338382\n",
      "Iteration 1129, loss = 0.09000225\n",
      "Iteration 1130, loss = 0.08954375\n",
      "Iteration 1131, loss = 0.09596603\n",
      "Iteration 1132, loss = 0.08446696\n",
      "Iteration 1133, loss = 0.09234192\n",
      "Iteration 1134, loss = 0.11543442\n",
      "Iteration 1135, loss = 0.48957392\n",
      "Iteration 1136, loss = 0.18259268\n",
      "Iteration 1137, loss = 0.09231045\n",
      "Iteration 1138, loss = 0.08584140\n",
      "Iteration 1139, loss = 0.12494077\n",
      "Iteration 1140, loss = 0.15596192\n",
      "Iteration 1141, loss = 0.12275836\n",
      "Iteration 1142, loss = 0.08475619\n",
      "Iteration 1143, loss = 0.09232791\n",
      "Iteration 1144, loss = 0.17993639\n",
      "Iteration 1145, loss = 0.11626827\n",
      "Iteration 1146, loss = 0.09668773\n",
      "Iteration 1147, loss = 0.21458937\n",
      "Iteration 1148, loss = 0.41756373\n",
      "Iteration 1149, loss = 0.34971947\n",
      "Iteration 1150, loss = 0.10094089\n",
      "Iteration 1151, loss = 0.10025238\n",
      "Iteration 1152, loss = 0.10082683\n",
      "Iteration 1153, loss = 0.09696640\n",
      "Iteration 1154, loss = 0.19832041\n",
      "Iteration 1155, loss = 0.21276408\n",
      "Iteration 1156, loss = 0.30094347\n",
      "Iteration 1157, loss = 0.14648796\n",
      "Iteration 1158, loss = 0.09677111\n",
      "Iteration 1159, loss = 0.10592936\n",
      "Iteration 1160, loss = 0.08753987\n",
      "Iteration 1161, loss = 0.08635510\n",
      "Iteration 1162, loss = 0.09608654\n",
      "Iteration 1163, loss = 0.10494961\n",
      "Iteration 1164, loss = 0.10594128\n",
      "Iteration 1165, loss = 0.15647699\n",
      "Iteration 1166, loss = 0.24313584\n",
      "Iteration 1167, loss = 0.18608235\n",
      "Iteration 1168, loss = 0.23788142\n",
      "Iteration 1169, loss = 0.10090688\n",
      "Iteration 1170, loss = 0.17654247\n",
      "Iteration 1171, loss = 0.30752445\n",
      "Iteration 1172, loss = 0.38522356\n",
      "Iteration 1173, loss = 0.12151645\n",
      "Iteration 1174, loss = 0.11573117\n",
      "Iteration 1175, loss = 0.11074729\n",
      "Iteration 1176, loss = 0.10735782\n",
      "Iteration 1177, loss = 0.09922636\n",
      "Iteration 1178, loss = 0.09244654\n",
      "Iteration 1179, loss = 0.08915339\n",
      "Iteration 1180, loss = 0.11301672\n",
      "Iteration 1181, loss = 0.23984257\n",
      "Iteration 1182, loss = 0.12393362\n",
      "Iteration 1183, loss = 0.13424168\n",
      "Iteration 1184, loss = 0.40115277\n",
      "Iteration 1185, loss = 0.15067348\n",
      "Iteration 1186, loss = 0.12097856\n",
      "Iteration 1187, loss = 0.10942253\n",
      "Iteration 1188, loss = 0.09825589\n",
      "Iteration 1189, loss = 0.10434146\n",
      "Iteration 1190, loss = 0.09714914\n",
      "Iteration 1191, loss = 0.21777554\n",
      "Iteration 1192, loss = 0.09623954\n",
      "Iteration 1193, loss = 0.21909122\n",
      "Iteration 1194, loss = 0.26845156\n",
      "Iteration 1195, loss = 0.16332434\n",
      "Iteration 1196, loss = 0.09367931\n",
      "Iteration 1197, loss = 0.09184013\n",
      "Iteration 1198, loss = 0.08668836\n",
      "Iteration 1199, loss = 0.08930077\n",
      "Iteration 1200, loss = 0.09389172\n",
      "Iteration 1201, loss = 0.15231152\n",
      "Iteration 1202, loss = 0.44092153\n",
      "Iteration 1203, loss = 0.30652968\n",
      "Iteration 1204, loss = 0.13491594\n",
      "Iteration 1205, loss = 0.13853352\n",
      "Iteration 1206, loss = 0.09966975\n",
      "Iteration 1207, loss = 0.11000300\n",
      "Iteration 1208, loss = 0.15222030\n",
      "Iteration 1209, loss = 0.15385120\n",
      "Iteration 1210, loss = 0.35583178\n",
      "Iteration 1211, loss = 0.39121354\n",
      "Iteration 1212, loss = 0.11130047\n",
      "Iteration 1213, loss = 0.13145286\n",
      "Iteration 1214, loss = 0.10392006\n",
      "Iteration 1215, loss = 0.11300166\n",
      "Iteration 1216, loss = 0.10005830\n",
      "Iteration 1217, loss = 0.09718869\n",
      "Iteration 1218, loss = 0.08925050\n",
      "Iteration 1219, loss = 0.09511801\n",
      "Iteration 1220, loss = 0.09559052\n",
      "Iteration 1221, loss = 0.08698037\n",
      "Iteration 1222, loss = 0.09089767\n",
      "Iteration 1223, loss = 0.08272415\n",
      "Iteration 1224, loss = 0.13860036\n",
      "Iteration 1225, loss = 0.18369859\n",
      "Iteration 1226, loss = 0.68820402\n",
      "Iteration 1227, loss = 0.16707677\n",
      "Iteration 1228, loss = 0.12064057\n",
      "Iteration 1229, loss = 0.12365596\n",
      "Iteration 1230, loss = 0.12920238\n",
      "Iteration 1231, loss = 0.10300253\n",
      "Iteration 1232, loss = 0.18430533\n",
      "Iteration 1233, loss = 0.18296129\n",
      "Iteration 1234, loss = 0.11452781\n",
      "Iteration 1235, loss = 0.12344522\n",
      "Iteration 1236, loss = 0.13671813\n",
      "Iteration 1237, loss = 0.20256222\n",
      "Iteration 1238, loss = 0.11536923\n",
      "Iteration 1239, loss = 0.10654450\n",
      "Iteration 1240, loss = 0.20172771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1241, loss = 0.15727681\n",
      "Iteration 1242, loss = 0.20152318\n",
      "Iteration 1243, loss = 0.15547809\n",
      "Iteration 1244, loss = 0.19975605\n",
      "Iteration 1245, loss = 0.10512817\n",
      "Iteration 1246, loss = 0.12473939\n",
      "Iteration 1247, loss = 0.11845090\n",
      "Iteration 1248, loss = 0.12660905\n",
      "Iteration 1249, loss = 0.10323296\n",
      "Iteration 1250, loss = 0.09432985\n",
      "Iteration 1251, loss = 0.10668888\n",
      "Iteration 1252, loss = 0.23104314\n",
      "Iteration 1253, loss = 0.27228600\n",
      "Iteration 1254, loss = 0.26623130\n",
      "Iteration 1255, loss = 0.09168618\n",
      "Iteration 1256, loss = 0.08927400\n",
      "Iteration 1257, loss = 0.10346493\n",
      "Iteration 1258, loss = 0.10622320\n",
      "Iteration 1259, loss = 0.10248035\n",
      "Iteration 1260, loss = 0.08630782\n",
      "Iteration 1261, loss = 0.12287358\n",
      "Iteration 1262, loss = 0.09424076\n",
      "Iteration 1263, loss = 0.16814490\n",
      "Iteration 1264, loss = 0.31207779\n",
      "Iteration 1265, loss = 0.08476171\n",
      "Iteration 1266, loss = 0.13614617\n",
      "Iteration 1267, loss = 0.30792838\n",
      "Iteration 1268, loss = 0.19460713\n",
      "Iteration 1269, loss = 0.11459096\n",
      "Iteration 1270, loss = 0.09632501\n",
      "Iteration 1271, loss = 0.09318410\n",
      "Iteration 1272, loss = 0.08966680\n",
      "Iteration 1273, loss = 0.08929192\n",
      "Iteration 1274, loss = 0.08230338\n",
      "Iteration 1275, loss = 0.08124739\n",
      "Iteration 1276, loss = 0.08007789\n",
      "Iteration 1277, loss = 0.07799473\n",
      "Iteration 1278, loss = 0.08098639\n",
      "Iteration 1279, loss = 0.12563883\n",
      "Iteration 1280, loss = 0.07769482\n",
      "Iteration 1281, loss = 0.10656100\n",
      "Iteration 1282, loss = 0.10338540\n",
      "Iteration 1283, loss = 0.24303525\n",
      "Iteration 1284, loss = 0.14530890\n",
      "Iteration 1285, loss = 0.44648890\n",
      "Iteration 1286, loss = 0.16620155\n",
      "Iteration 1287, loss = 0.14639259\n",
      "Iteration 1288, loss = 0.17444761\n",
      "Iteration 1289, loss = 0.12512399\n",
      "Iteration 1290, loss = 0.11464357\n",
      "Iteration 1291, loss = 0.09955190\n",
      "Iteration 1292, loss = 0.09750861\n",
      "Iteration 1293, loss = 0.17944369\n",
      "Iteration 1294, loss = 0.37415221\n",
      "Iteration 1295, loss = 0.12301752\n",
      "Iteration 1296, loss = 0.14421802\n",
      "Iteration 1297, loss = 0.12224943\n",
      "Iteration 1298, loss = 0.15583389\n",
      "Iteration 1299, loss = 0.09614783\n",
      "Iteration 1300, loss = 0.24555009\n",
      "Iteration 1301, loss = 0.09636356\n",
      "Iteration 1302, loss = 0.13527187\n",
      "Iteration 1303, loss = 0.20923976\n",
      "Iteration 1304, loss = 0.30069005\n",
      "Iteration 1305, loss = 0.10101966\n",
      "Iteration 1306, loss = 0.10594439\n",
      "Iteration 1307, loss = 0.09005558\n",
      "Iteration 1308, loss = 0.17469474\n",
      "Iteration 1309, loss = 0.11138114\n",
      "Iteration 1310, loss = 0.10215400\n",
      "Iteration 1311, loss = 0.29454758\n",
      "Iteration 1312, loss = 0.08415316\n",
      "Iteration 1313, loss = 0.13380734\n",
      "Iteration 1314, loss = 0.11475328\n",
      "Iteration 1315, loss = 0.25876888\n",
      "Iteration 1316, loss = 0.36376354\n",
      "Iteration 1317, loss = 0.10214470\n",
      "Iteration 1318, loss = 0.10825827\n",
      "Iteration 1319, loss = 0.09574703\n",
      "Iteration 1320, loss = 0.09129164\n",
      "Iteration 1321, loss = 0.08419790\n",
      "Iteration 1322, loss = 0.20546430\n",
      "Iteration 1323, loss = 0.42739534\n",
      "Iteration 1324, loss = 0.11951951\n",
      "Iteration 1325, loss = 0.10664613\n",
      "Iteration 1326, loss = 0.15981703\n",
      "Iteration 1327, loss = 0.18350984\n",
      "Iteration 1328, loss = 0.29857205\n",
      "Iteration 1329, loss = 0.16099077\n",
      "Iteration 1330, loss = 0.10354003\n",
      "Iteration 1331, loss = 0.10979158\n",
      "Iteration 1332, loss = 0.08944674\n",
      "Iteration 1333, loss = 0.10076133\n",
      "Iteration 1334, loss = 0.12788748\n",
      "Iteration 1335, loss = 0.19258816\n",
      "Iteration 1336, loss = 0.11623359\n",
      "Iteration 1337, loss = 0.08479791\n",
      "Iteration 1338, loss = 0.10545208\n",
      "Iteration 1339, loss = 0.24387271\n",
      "Iteration 1340, loss = 0.52018467\n",
      "Iteration 1341, loss = 0.08908930\n",
      "Iteration 1342, loss = 0.09814676\n",
      "Iteration 1343, loss = 0.09213654\n",
      "Iteration 1344, loss = 0.10217327\n",
      "Iteration 1345, loss = 0.13623843\n",
      "Iteration 1346, loss = 0.11886427\n",
      "Iteration 1347, loss = 0.08307177\n",
      "Iteration 1348, loss = 0.09060053\n",
      "Iteration 1349, loss = 0.08559834\n",
      "Iteration 1350, loss = 0.07804456\n",
      "Iteration 1351, loss = 0.07558678\n",
      "Iteration 1352, loss = 0.07467211\n",
      "Iteration 1353, loss = 0.07862789\n",
      "Iteration 1354, loss = 0.15138990\n",
      "Iteration 1355, loss = 0.07110962\n",
      "Iteration 1356, loss = 0.12547350\n",
      "Iteration 1357, loss = 0.40899256\n",
      "Iteration 1358, loss = 0.12016401\n",
      "Iteration 1359, loss = 0.13250441\n",
      "Iteration 1360, loss = 0.11777130\n",
      "Iteration 1361, loss = 0.15723372\n",
      "Iteration 1362, loss = 0.10228470\n",
      "Iteration 1363, loss = 0.24430445\n",
      "Iteration 1364, loss = 0.09745936\n",
      "Iteration 1365, loss = 0.07939350\n",
      "Iteration 1366, loss = 0.07372066\n",
      "Iteration 1367, loss = 0.07582678\n",
      "Iteration 1368, loss = 0.07464834\n",
      "Iteration 1369, loss = 0.08508718\n",
      "Iteration 1370, loss = 0.33628817\n",
      "Iteration 1371, loss = 0.32248418\n",
      "Iteration 1372, loss = 0.12037058\n",
      "Iteration 1373, loss = 0.19206927\n",
      "Iteration 1374, loss = 0.15569010\n",
      "Iteration 1375, loss = 0.14423966\n",
      "Iteration 1376, loss = 0.12716487\n",
      "Iteration 1377, loss = 0.08406788\n",
      "Iteration 1378, loss = 0.12542428\n",
      "Iteration 1379, loss = 0.42362419\n",
      "Iteration 1380, loss = 0.10423141\n",
      "Iteration 1381, loss = 0.18519295\n",
      "Iteration 1382, loss = 0.24017759\n",
      "Iteration 1383, loss = 0.14583164\n",
      "Iteration 1384, loss = 0.11606755\n",
      "Iteration 1385, loss = 0.23565351\n",
      "Iteration 1386, loss = 0.10682323\n",
      "Iteration 1387, loss = 0.08589079\n",
      "Iteration 1388, loss = 0.08382342\n",
      "Iteration 1389, loss = 0.09749392\n",
      "Iteration 1390, loss = 0.12024617\n",
      "Iteration 1391, loss = 0.13449911\n",
      "Iteration 1392, loss = 0.23890585\n",
      "Iteration 1393, loss = 0.16269645\n",
      "Iteration 1394, loss = 0.08102939\n",
      "Iteration 1395, loss = 0.08124267\n",
      "Iteration 1396, loss = 0.09062634\n",
      "Iteration 1397, loss = 0.07650156\n",
      "Iteration 1398, loss = 0.07671925\n",
      "Iteration 1399, loss = 0.11670883\n",
      "Iteration 1400, loss = 0.29593479\n",
      "Iteration 1401, loss = 0.43473611\n",
      "Iteration 1402, loss = 0.15227675\n",
      "Iteration 1403, loss = 0.09505316\n",
      "Iteration 1404, loss = 0.15541140\n",
      "Iteration 1405, loss = 0.09273261\n",
      "Iteration 1406, loss = 0.07940033\n",
      "Iteration 1407, loss = 0.10952396\n",
      "Iteration 1408, loss = 0.08669278\n",
      "Iteration 1409, loss = 0.06975061\n",
      "Iteration 1410, loss = 0.23411954\n",
      "Iteration 1411, loss = 0.24883054\n",
      "Iteration 1412, loss = 0.09835511\n",
      "Iteration 1413, loss = 0.14597278\n",
      "Iteration 1414, loss = 0.08558410\n",
      "Iteration 1415, loss = 0.09839604\n",
      "Iteration 1416, loss = 0.17875515\n",
      "Iteration 1417, loss = 0.29289679\n",
      "Iteration 1418, loss = 0.23613841\n",
      "Iteration 1419, loss = 0.11932002\n",
      "Iteration 1420, loss = 0.12094711\n",
      "Iteration 1421, loss = 0.12421286\n",
      "Iteration 1422, loss = 0.10729642\n",
      "Iteration 1423, loss = 0.08779738\n",
      "Iteration 1424, loss = 0.07554862\n",
      "Iteration 1425, loss = 0.07199736\n",
      "Iteration 1426, loss = 0.12292020\n",
      "Iteration 1427, loss = 0.23945642\n",
      "Iteration 1428, loss = 0.23003082\n",
      "Iteration 1429, loss = 0.09869876\n",
      "Iteration 1430, loss = 0.08310067\n",
      "Iteration 1431, loss = 0.11022578\n",
      "Iteration 1432, loss = 0.10579514\n",
      "Iteration 1433, loss = 0.23248057\n",
      "Iteration 1434, loss = 0.40546481\n",
      "Iteration 1435, loss = 0.13606315\n",
      "Iteration 1436, loss = 0.10443055\n",
      "Iteration 1437, loss = 0.09330489\n",
      "Iteration 1438, loss = 0.11664322\n",
      "Iteration 1439, loss = 0.10988273\n",
      "Iteration 1440, loss = 0.08480020\n",
      "Iteration 1441, loss = 0.08484857\n",
      "Iteration 1442, loss = 0.08298374\n",
      "Iteration 1443, loss = 0.07860386\n",
      "Iteration 1444, loss = 0.14572613\n",
      "Iteration 1445, loss = 0.14108166\n",
      "Iteration 1446, loss = 0.10775447\n",
      "Iteration 1447, loss = 0.20206724\n",
      "Iteration 1448, loss = 0.34759812\n",
      "Iteration 1449, loss = 0.51756225\n",
      "Iteration 1450, loss = 0.13225371\n",
      "Iteration 1451, loss = 0.14592608\n",
      "Iteration 1452, loss = 0.11050254\n",
      "Iteration 1453, loss = 0.15531976\n",
      "Iteration 1454, loss = 0.17417079\n",
      "Iteration 1455, loss = 0.13312026\n",
      "Iteration 1456, loss = 0.16343567\n",
      "Iteration 1457, loss = 0.11619326\n",
      "Iteration 1458, loss = 0.18584624\n",
      "Iteration 1459, loss = 0.17114144\n",
      "Iteration 1460, loss = 0.10255431\n",
      "Iteration 1461, loss = 0.29376488\n",
      "Iteration 1462, loss = 0.18989621\n",
      "Iteration 1463, loss = 0.11869367\n",
      "Iteration 1464, loss = 0.11401432\n",
      "Iteration 1465, loss = 0.10753276\n",
      "Iteration 1466, loss = 0.12256249\n",
      "Iteration 1467, loss = 0.10114963\n",
      "Iteration 1468, loss = 0.10982874\n",
      "Iteration 1469, loss = 0.11614245\n",
      "Iteration 1470, loss = 0.38041108\n",
      "Iteration 1471, loss = 0.12067176\n",
      "Iteration 1472, loss = 0.11944235\n",
      "Iteration 1473, loss = 0.11804794\n",
      "Iteration 1474, loss = 0.09480793\n",
      "Iteration 1475, loss = 0.13058542\n",
      "Iteration 1476, loss = 0.09096526\n",
      "Iteration 1477, loss = 0.14321228\n",
      "Iteration 1478, loss = 0.14454376\n",
      "Iteration 1479, loss = 0.13389357\n",
      "Iteration 1480, loss = 0.27997711\n",
      "Iteration 1481, loss = 0.16078603\n",
      "Iteration 1482, loss = 0.17723009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1483, loss = 0.12083452\n",
      "Iteration 1484, loss = 0.22168377\n",
      "Iteration 1485, loss = 0.09551492\n",
      "Iteration 1486, loss = 0.09440830\n",
      "Iteration 1487, loss = 0.08853532\n",
      "Iteration 1488, loss = 0.18083912\n",
      "Iteration 1489, loss = 0.11024673\n",
      "Iteration 1490, loss = 0.09177682\n",
      "Iteration 1491, loss = 0.12691861\n",
      "Iteration 1492, loss = 0.15792029\n",
      "Iteration 1493, loss = 0.42687860\n",
      "Iteration 1494, loss = 0.27262049\n",
      "Iteration 1495, loss = 0.20523517\n",
      "Iteration 1496, loss = 0.11368389\n",
      "Iteration 1497, loss = 0.10936975\n",
      "Iteration 1498, loss = 0.11952225\n",
      "Iteration 1499, loss = 0.23821208\n",
      "Iteration 1500, loss = 0.11901993\n",
      "Iteration 1501, loss = 0.09332103\n",
      "Iteration 1502, loss = 0.15663861\n",
      "Iteration 1503, loss = 0.23989579\n",
      "Iteration 1504, loss = 0.09864515\n",
      "Iteration 1505, loss = 0.10179921\n",
      "Iteration 1506, loss = 0.24288784\n",
      "Iteration 1507, loss = 0.31611375\n",
      "Iteration 1508, loss = 0.10388375\n",
      "Iteration 1509, loss = 0.09842767\n",
      "Iteration 1510, loss = 0.20869892\n",
      "Iteration 1511, loss = 0.31659761\n",
      "Iteration 1512, loss = 0.12220782\n",
      "Iteration 1513, loss = 0.12049343\n",
      "Iteration 1514, loss = 0.11621774\n",
      "Iteration 1515, loss = 0.09468720\n",
      "Iteration 1516, loss = 0.09832989\n",
      "Iteration 1517, loss = 0.09338703\n",
      "Iteration 1518, loss = 0.11595918\n",
      "Iteration 1519, loss = 0.27084924\n",
      "Iteration 1520, loss = 0.34979989\n",
      "Iteration 1521, loss = 0.10872386\n",
      "Iteration 1522, loss = 0.11398075\n",
      "Iteration 1523, loss = 0.15335337\n",
      "Iteration 1524, loss = 0.11703861\n",
      "Iteration 1525, loss = 0.10447987\n",
      "Iteration 1526, loss = 0.09498591\n",
      "Iteration 1527, loss = 0.10890158\n",
      "Iteration 1528, loss = 0.16862795\n",
      "Iteration 1529, loss = 0.22233768\n",
      "Iteration 1530, loss = 0.22353284\n",
      "Iteration 1531, loss = 0.18132636\n",
      "Iteration 1532, loss = 0.13339181\n",
      "Iteration 1533, loss = 0.20047246\n",
      "Iteration 1534, loss = 0.10910718\n",
      "Iteration 1535, loss = 0.08995010\n",
      "Iteration 1536, loss = 0.10871160\n",
      "Iteration 1537, loss = 0.19002689\n",
      "Iteration 1538, loss = 0.14460747\n",
      "Iteration 1539, loss = 0.10724321\n",
      "Iteration 1540, loss = 0.19644135\n",
      "Iteration 1541, loss = 0.43226138\n",
      "Iteration 1542, loss = 0.13990159\n",
      "Iteration 1543, loss = 0.13636769\n",
      "Iteration 1544, loss = 0.19286071\n",
      "Iteration 1545, loss = 0.15557771\n",
      "Iteration 1546, loss = 0.09123668\n",
      "Iteration 1547, loss = 0.08691707\n",
      "Iteration 1548, loss = 0.13189663\n",
      "Iteration 1549, loss = 0.19650812\n",
      "Iteration 1550, loss = 0.08294521\n",
      "Iteration 1551, loss = 0.08212396\n",
      "Iteration 1552, loss = 0.08467824\n",
      "Iteration 1553, loss = 0.10388706\n",
      "Iteration 1554, loss = 0.42438530\n",
      "Iteration 1555, loss = 0.24178017\n",
      "Iteration 1556, loss = 0.11523071\n",
      "Iteration 1557, loss = 0.09801733\n",
      "Iteration 1558, loss = 0.09953607\n",
      "Iteration 1559, loss = 0.08574848\n",
      "Iteration 1560, loss = 0.17894224\n",
      "Iteration 1561, loss = 0.11931400\n",
      "Iteration 1562, loss = 0.18870646\n",
      "Iteration 1563, loss = 0.14873008\n",
      "Iteration 1564, loss = 0.10303198\n",
      "Iteration 1565, loss = 0.11728709\n",
      "Iteration 1566, loss = 0.26687345\n",
      "Iteration 1567, loss = 0.14186499\n",
      "Iteration 1568, loss = 0.28929523\n",
      "Iteration 1569, loss = 0.11750983\n",
      "Iteration 1570, loss = 0.14236574\n",
      "Iteration 1571, loss = 0.08629745\n",
      "Iteration 1572, loss = 0.08872764\n",
      "Iteration 1573, loss = 0.08743086\n",
      "Iteration 1574, loss = 0.09269315\n",
      "Iteration 1575, loss = 0.10672315\n",
      "Iteration 1576, loss = 0.28806609\n",
      "Iteration 1577, loss = 0.29858023\n",
      "Iteration 1578, loss = 0.17753442\n",
      "Iteration 1579, loss = 0.13867844\n",
      "Iteration 1580, loss = 0.14702924\n",
      "Iteration 1581, loss = 0.09144914\n",
      "Iteration 1582, loss = 0.08651876\n",
      "Iteration 1583, loss = 0.08729509\n",
      "Iteration 1584, loss = 0.19491816\n",
      "Iteration 1585, loss = 0.35377952\n",
      "Iteration 1586, loss = 0.15916196\n",
      "Iteration 1587, loss = 0.10955832\n",
      "Iteration 1588, loss = 0.11200939\n",
      "Iteration 1589, loss = 0.11541766\n",
      "Iteration 1590, loss = 0.09521791\n",
      "Iteration 1591, loss = 0.08777590\n",
      "Iteration 1592, loss = 0.11029292\n",
      "Iteration 1593, loss = 0.15816514\n",
      "Iteration 1594, loss = 0.29797831\n",
      "Iteration 1595, loss = 0.10274793\n",
      "Iteration 1596, loss = 0.08672684\n",
      "Iteration 1597, loss = 0.09005317\n",
      "Iteration 1598, loss = 0.14135394\n",
      "Iteration 1599, loss = 0.09529313\n",
      "Iteration 1600, loss = 0.10312707\n",
      "Iteration 1601, loss = 0.16359366\n",
      "Iteration 1602, loss = 0.14724232\n",
      "Iteration 1603, loss = 0.09598934\n",
      "Iteration 1604, loss = 0.08898489\n",
      "Iteration 1605, loss = 0.10112523\n",
      "Iteration 1606, loss = 0.09994136\n",
      "Iteration 1607, loss = 0.07442812\n",
      "Iteration 1608, loss = 0.18154722\n",
      "Iteration 1609, loss = 0.15157631\n",
      "Iteration 1610, loss = 0.07476733\n",
      "Iteration 1611, loss = 0.07813883\n",
      "Iteration 1612, loss = 0.43059270\n",
      "Iteration 1613, loss = 0.11445177\n",
      "Iteration 1614, loss = 0.08610640\n",
      "Iteration 1615, loss = 0.09864823\n",
      "Iteration 1616, loss = 0.22785432\n",
      "Iteration 1617, loss = 0.12705980\n",
      "Iteration 1618, loss = 0.20232181\n",
      "Iteration 1619, loss = 0.17410162\n",
      "Iteration 1620, loss = 0.25726561\n",
      "Iteration 1621, loss = 0.09275184\n",
      "Iteration 1622, loss = 0.12117974\n",
      "Iteration 1623, loss = 0.17302746\n",
      "Iteration 1624, loss = 0.18222258\n",
      "Iteration 1625, loss = 0.09855431\n",
      "Iteration 1626, loss = 0.10434872\n",
      "Iteration 1627, loss = 0.15227622\n",
      "Iteration 1628, loss = 0.28742588\n",
      "Iteration 1629, loss = 0.23259246\n",
      "Iteration 1630, loss = 0.32447356\n",
      "Iteration 1631, loss = 0.13491719\n",
      "Iteration 1632, loss = 0.12289303\n",
      "Iteration 1633, loss = 0.08353612\n",
      "Iteration 1634, loss = 0.08330749\n",
      "Iteration 1635, loss = 0.08744560\n",
      "Iteration 1636, loss = 0.09360458\n",
      "Iteration 1637, loss = 0.17581733\n",
      "Iteration 1638, loss = 0.42468597\n",
      "Iteration 1639, loss = 0.11382468\n",
      "Iteration 1640, loss = 0.07951047\n",
      "Iteration 1641, loss = 0.07539736\n",
      "Iteration 1642, loss = 0.08225721\n",
      "Iteration 1643, loss = 0.36660886\n",
      "Iteration 1644, loss = 0.11644036\n",
      "Iteration 1645, loss = 0.17445247\n",
      "Iteration 1646, loss = 0.11244429\n",
      "Iteration 1647, loss = 0.08392319\n",
      "Iteration 1648, loss = 0.08725386\n",
      "Iteration 1649, loss = 0.16873574\n",
      "Iteration 1650, loss = 0.11198192\n",
      "Iteration 1651, loss = 0.20851583\n",
      "Iteration 1652, loss = 0.09711055\n",
      "Iteration 1653, loss = 0.09392699\n",
      "Iteration 1654, loss = 0.07492113\n",
      "Iteration 1655, loss = 0.06934067\n",
      "Iteration 1656, loss = 0.09868687\n",
      "Iteration 1657, loss = 0.29207079\n",
      "Iteration 1658, loss = 0.48677204\n",
      "Iteration 1659, loss = 0.08983967\n",
      "Iteration 1660, loss = 0.07698006\n",
      "Iteration 1661, loss = 0.11362833\n",
      "Iteration 1662, loss = 0.11887605\n",
      "Iteration 1663, loss = 0.11122850\n",
      "Iteration 1664, loss = 0.07796060\n",
      "Iteration 1665, loss = 0.07076329\n",
      "Iteration 1666, loss = 0.06338090\n",
      "Iteration 1667, loss = 0.07665940\n",
      "Iteration 1668, loss = 0.40386685\n",
      "Iteration 1669, loss = 0.32986710\n",
      "Iteration 1670, loss = 0.13350812\n",
      "Iteration 1671, loss = 0.19690155\n",
      "Iteration 1672, loss = 0.10383236\n",
      "Iteration 1673, loss = 0.11462215\n",
      "Iteration 1674, loss = 0.09230157\n",
      "Iteration 1675, loss = 0.08936784\n",
      "Iteration 1676, loss = 0.10974402\n",
      "Iteration 1677, loss = 0.11098661\n",
      "Iteration 1678, loss = 0.10459579\n",
      "Iteration 1679, loss = 0.16658947\n",
      "Iteration 1680, loss = 0.11155186\n",
      "Iteration 1681, loss = 0.29319866\n",
      "Iteration 1682, loss = 0.09855566\n",
      "Iteration 1683, loss = 0.11176559\n",
      "Iteration 1684, loss = 0.08548217\n",
      "Iteration 1685, loss = 0.08839985\n",
      "Iteration 1686, loss = 0.13653887\n",
      "Iteration 1687, loss = 0.08822453\n",
      "Iteration 1688, loss = 0.11634855\n",
      "Iteration 1689, loss = 0.34845526\n",
      "Iteration 1690, loss = 0.07837568\n",
      "Iteration 1691, loss = 0.07597553\n",
      "Iteration 1692, loss = 0.06688096\n",
      "Iteration 1693, loss = 0.08017853\n",
      "Iteration 1694, loss = 0.23321278\n",
      "Iteration 1695, loss = 0.15392680\n",
      "Iteration 1696, loss = 0.11768112\n",
      "Iteration 1697, loss = 0.21538329\n",
      "Iteration 1698, loss = 0.13270775\n",
      "Iteration 1699, loss = 0.08944107\n",
      "Iteration 1700, loss = 0.16800008\n",
      "Iteration 1701, loss = 0.08786732\n",
      "Iteration 1702, loss = 0.10923146\n",
      "Iteration 1703, loss = 0.08733096\n",
      "Iteration 1704, loss = 0.09541108\n",
      "Iteration 1705, loss = 0.19747977\n",
      "Iteration 1706, loss = 0.08693144\n",
      "Iteration 1707, loss = 0.10120309\n",
      "Iteration 1708, loss = 0.06850584\n",
      "Iteration 1709, loss = 0.08037512\n",
      "Iteration 1710, loss = 0.09504124\n",
      "Iteration 1711, loss = 0.40798957\n",
      "Iteration 1712, loss = 0.11013339\n",
      "Iteration 1713, loss = 0.16774488\n",
      "Iteration 1714, loss = 0.24810869\n",
      "Iteration 1715, loss = 0.09905536\n",
      "Iteration 1716, loss = 0.13387745\n",
      "Iteration 1717, loss = 0.20411793\n",
      "Iteration 1718, loss = 0.26533339\n",
      "Iteration 1719, loss = 0.22903804\n",
      "Iteration 1720, loss = 0.12194391\n",
      "Iteration 1721, loss = 0.09246528\n",
      "Iteration 1722, loss = 0.09043169\n",
      "Iteration 1723, loss = 0.10901880\n",
      "Iteration 1724, loss = 0.13332967\n",
      "Iteration 1725, loss = 0.21198564\n",
      "Iteration 1726, loss = 0.07703892\n",
      "Iteration 1727, loss = 0.14220523\n",
      "Iteration 1728, loss = 0.11641894\n",
      "Iteration 1729, loss = 0.22997562\n",
      "Iteration 1730, loss = 0.08795305\n",
      "Iteration 1731, loss = 0.08348377\n",
      "Iteration 1732, loss = 0.12659301\n",
      "Iteration 1733, loss = 0.06878670\n",
      "Iteration 1734, loss = 0.07159850\n",
      "Iteration 1735, loss = 0.06545890\n",
      "Iteration 1736, loss = 0.06943935\n",
      "Iteration 1737, loss = 0.06467378\n",
      "Iteration 1738, loss = 0.05831525\n",
      "Iteration 1739, loss = 0.05695434\n",
      "Iteration 1740, loss = 0.07177283\n",
      "Iteration 1741, loss = 0.26195145\n",
      "Iteration 1742, loss = 0.27955838\n",
      "Iteration 1743, loss = 0.47294299\n",
      "Iteration 1744, loss = 0.14739384\n",
      "Iteration 1745, loss = 0.14257082\n",
      "Iteration 1746, loss = 0.08979516\n",
      "Iteration 1747, loss = 0.07791020\n",
      "Iteration 1748, loss = 0.07476896\n",
      "Iteration 1749, loss = 0.22665103\n",
      "Iteration 1750, loss = 0.40913637\n",
      "Iteration 1751, loss = 0.11722042\n",
      "Iteration 1752, loss = 0.09781548\n",
      "Iteration 1753, loss = 0.09607165\n",
      "Iteration 1754, loss = 0.09033559\n",
      "Iteration 1755, loss = 0.16514002\n",
      "Iteration 1756, loss = 0.08569044\n",
      "Iteration 1757, loss = 0.09187280\n",
      "Iteration 1758, loss = 0.07401017\n",
      "Iteration 1759, loss = 0.06896418\n",
      "Iteration 1760, loss = 0.11584571\n",
      "Iteration 1761, loss = 0.26220191\n",
      "Iteration 1762, loss = 0.24820627\n",
      "Iteration 1763, loss = 0.09454775\n",
      "Iteration 1764, loss = 0.08820605\n",
      "Iteration 1765, loss = 0.07001135\n",
      "Iteration 1766, loss = 0.06729633\n",
      "Iteration 1767, loss = 0.06727698\n",
      "Iteration 1768, loss = 0.06743787\n",
      "Iteration 1769, loss = 0.28449985\n",
      "Iteration 1770, loss = 0.49289727\n",
      "Iteration 1771, loss = 0.31888551\n",
      "Iteration 1772, loss = 0.14387260\n",
      "Iteration 1773, loss = 0.14375172\n",
      "Iteration 1774, loss = 0.12691991\n",
      "Iteration 1775, loss = 0.13190370\n",
      "Iteration 1776, loss = 0.13449206\n",
      "Iteration 1777, loss = 0.11489409\n",
      "Iteration 1778, loss = 0.14110288\n",
      "Iteration 1779, loss = 0.11383432\n",
      "Iteration 1780, loss = 0.12330331\n",
      "Iteration 1781, loss = 0.12448895\n",
      "Iteration 1782, loss = 0.10806266\n",
      "Iteration 1783, loss = 0.21654415\n",
      "Iteration 1784, loss = 0.11448718\n",
      "Iteration 1785, loss = 0.10546523\n",
      "Iteration 1786, loss = 0.12850277\n",
      "Iteration 1787, loss = 0.15206509\n",
      "Iteration 1788, loss = 0.41217354\n",
      "Iteration 1789, loss = 0.15812375\n",
      "Iteration 1790, loss = 0.12753994\n",
      "Iteration 1791, loss = 0.14523172\n",
      "Iteration 1792, loss = 0.13189040\n",
      "Iteration 1793, loss = 0.15610392\n",
      "Iteration 1794, loss = 0.17473459\n",
      "Iteration 1795, loss = 0.21643683\n",
      "Iteration 1796, loss = 0.21140121\n",
      "Iteration 1797, loss = 0.14336087\n",
      "Iteration 1798, loss = 0.12593142\n",
      "Iteration 1799, loss = 0.11136010\n",
      "Iteration 1800, loss = 0.11912860\n",
      "Iteration 1801, loss = 0.13698390\n",
      "Iteration 1802, loss = 0.11726628\n",
      "Iteration 1803, loss = 0.21199595\n",
      "Iteration 1804, loss = 0.24990226\n",
      "Iteration 1805, loss = 0.11022226\n",
      "Iteration 1806, loss = 0.12947771\n",
      "Iteration 1807, loss = 0.19864265\n",
      "Iteration 1808, loss = 0.40311724\n",
      "Iteration 1809, loss = 0.12210866\n",
      "Iteration 1810, loss = 0.17173759\n",
      "Iteration 1811, loss = 0.20997045\n",
      "Iteration 1812, loss = 0.13777187\n",
      "Iteration 1813, loss = 0.13658956\n",
      "Iteration 1814, loss = 0.17538940\n",
      "Iteration 1815, loss = 0.24925549\n",
      "Iteration 1816, loss = 0.30539891\n",
      "Iteration 1817, loss = 0.12125691\n",
      "Iteration 1818, loss = 0.10939443\n",
      "Iteration 1819, loss = 0.11077483\n",
      "Iteration 1820, loss = 0.09871012\n",
      "Iteration 1821, loss = 0.10901865\n",
      "Iteration 1822, loss = 0.10119282\n",
      "Iteration 1823, loss = 0.09412430\n",
      "Iteration 1824, loss = 0.11740678\n",
      "Iteration 1825, loss = 0.23847024\n",
      "Iteration 1826, loss = 0.15127556\n",
      "Iteration 1827, loss = 0.12239466\n",
      "Iteration 1828, loss = 0.13573689\n",
      "Iteration 1829, loss = 0.10007439\n",
      "Iteration 1830, loss = 0.11791071\n",
      "Iteration 1831, loss = 0.14477830\n",
      "Iteration 1832, loss = 0.29894827\n",
      "Iteration 1833, loss = 0.22128817\n",
      "Iteration 1834, loss = 0.13030830\n",
      "Iteration 1835, loss = 0.11189410\n",
      "Iteration 1836, loss = 0.10680542\n",
      "Iteration 1837, loss = 0.10153594\n",
      "Iteration 1838, loss = 0.10325437\n",
      "Iteration 1839, loss = 0.19780284\n",
      "Iteration 1840, loss = 0.27240756\n",
      "Iteration 1841, loss = 0.10379257\n",
      "Iteration 1842, loss = 0.09663198\n",
      "Iteration 1843, loss = 0.10189478\n",
      "Iteration 1844, loss = 0.10304742\n",
      "Iteration 1845, loss = 0.09169450\n",
      "Iteration 1846, loss = 0.12068961\n",
      "Iteration 1847, loss = 0.37519948\n",
      "Iteration 1848, loss = 0.52492768\n",
      "Iteration 1849, loss = 0.36115452\n",
      "Iteration 1850, loss = 0.12884442\n",
      "Iteration 1851, loss = 0.11562961\n",
      "Iteration 1852, loss = 0.11489843\n",
      "Iteration 1853, loss = 0.10463376\n",
      "Iteration 1854, loss = 0.11123261\n",
      "Iteration 1855, loss = 0.16776814\n",
      "Iteration 1856, loss = 0.14330246\n",
      "Iteration 1857, loss = 0.11312256\n",
      "Iteration 1858, loss = 0.09751439\n",
      "Iteration 1859, loss = 0.13578601\n",
      "Iteration 1860, loss = 0.16730477\n",
      "Iteration 1861, loss = 0.22051624\n",
      "Iteration 1862, loss = 0.23613131\n",
      "Iteration 1863, loss = 0.17654006\n",
      "Iteration 1864, loss = 0.20685543\n",
      "Iteration 1865, loss = 0.20424942\n",
      "Iteration 1866, loss = 0.17401471\n",
      "Iteration 1867, loss = 0.10851585\n",
      "Iteration 1868, loss = 0.15041036\n",
      "Iteration 1869, loss = 0.10585709\n",
      "Iteration 1870, loss = 0.09868206\n",
      "Iteration 1871, loss = 0.11799043\n",
      "Iteration 1872, loss = 0.25564138\n",
      "Iteration 1873, loss = 0.10457864\n",
      "Iteration 1874, loss = 0.16024824\n",
      "Iteration 1875, loss = 0.10348031\n",
      "Iteration 1876, loss = 0.12253083\n",
      "Iteration 1877, loss = 0.10394934\n",
      "Iteration 1878, loss = 0.09560552\n",
      "Iteration 1879, loss = 0.10396370\n",
      "Iteration 1880, loss = 0.10403881\n",
      "Iteration 1881, loss = 0.31165764\n",
      "Iteration 1882, loss = 0.17954901\n",
      "Iteration 1883, loss = 0.33149385\n",
      "Iteration 1884, loss = 0.32870693\n",
      "Iteration 1885, loss = 0.16271642\n",
      "Iteration 1886, loss = 0.11627006\n",
      "Iteration 1887, loss = 0.16364898\n",
      "Iteration 1888, loss = 0.14777868\n",
      "Iteration 1889, loss = 0.15503032\n",
      "Iteration 1890, loss = 0.18416079\n",
      "Iteration 1891, loss = 0.11498737\n",
      "Iteration 1892, loss = 0.14820779\n",
      "Iteration 1893, loss = 0.13310964\n",
      "Iteration 1894, loss = 0.16406769\n",
      "Iteration 1895, loss = 0.23486182\n",
      "Iteration 1896, loss = 0.11559580\n",
      "Iteration 1897, loss = 0.10790033\n",
      "Iteration 1898, loss = 0.10630420\n",
      "Iteration 1899, loss = 0.18897579\n",
      "Iteration 1900, loss = 0.12610046\n",
      "Iteration 1901, loss = 0.14404805\n",
      "Iteration 1902, loss = 0.23446660\n",
      "Iteration 1903, loss = 0.13352815\n",
      "Iteration 1904, loss = 0.10518219\n",
      "Iteration 1905, loss = 0.12167434\n",
      "Iteration 1906, loss = 0.17049392\n",
      "Iteration 1907, loss = 0.18957459\n",
      "Iteration 1908, loss = 0.39041512\n",
      "Iteration 1909, loss = 0.12813563\n",
      "Iteration 1910, loss = 0.11404328\n",
      "Iteration 1911, loss = 0.10377472\n",
      "Iteration 1912, loss = 0.10535882\n",
      "Iteration 1913, loss = 0.10417712\n",
      "Iteration 1914, loss = 0.13578373\n",
      "Iteration 1915, loss = 0.10710563\n",
      "Iteration 1916, loss = 0.14051802\n",
      "Iteration 1917, loss = 0.32800379\n",
      "Iteration 1918, loss = 0.10358659\n",
      "Iteration 1919, loss = 0.12622347\n",
      "Iteration 1920, loss = 0.22562674\n",
      "Iteration 1921, loss = 0.10694820\n",
      "Iteration 1922, loss = 0.23058918\n",
      "Iteration 1923, loss = 0.26401509\n",
      "Iteration 1924, loss = 0.13430718\n",
      "Iteration 1925, loss = 0.12998789\n",
      "Iteration 1926, loss = 0.11716520\n",
      "Iteration 1927, loss = 0.12465188\n",
      "Iteration 1928, loss = 0.12005064\n",
      "Iteration 1929, loss = 0.11257189\n",
      "Iteration 1930, loss = 0.10997693\n",
      "Iteration 1931, loss = 0.13179937\n",
      "Iteration 1932, loss = 0.10752848\n",
      "Iteration 1933, loss = 0.10612661\n",
      "Iteration 1934, loss = 0.09972243\n",
      "Iteration 1935, loss = 0.19416714\n",
      "Iteration 1936, loss = 0.40422060\n",
      "Iteration 1937, loss = 0.14227429\n",
      "Iteration 1938, loss = 0.11550133\n",
      "Iteration 1939, loss = 0.11181042\n",
      "Iteration 1940, loss = 0.13316338\n",
      "Iteration 1941, loss = 0.39469767\n",
      "Iteration 1942, loss = 0.14430702\n",
      "Iteration 1943, loss = 0.12812921\n",
      "Iteration 1944, loss = 0.12492190\n",
      "Iteration 1945, loss = 0.15052995\n",
      "Iteration 1946, loss = 0.15074295\n",
      "Iteration 1947, loss = 0.13574022\n",
      "Iteration 1948, loss = 0.21345081\n",
      "Iteration 1949, loss = 0.32764742\n",
      "Iteration 1950, loss = 0.17013662\n",
      "Iteration 1951, loss = 0.12804974\n",
      "Iteration 1952, loss = 0.13110141\n",
      "Iteration 1953, loss = 0.25309858\n",
      "Iteration 1954, loss = 0.18105614\n",
      "Iteration 1955, loss = 0.19040503\n",
      "Iteration 1956, loss = 0.20905723\n",
      "Iteration 1957, loss = 0.13525521\n",
      "Iteration 1958, loss = 0.20729785\n",
      "Iteration 1959, loss = 0.11681025\n",
      "Iteration 1960, loss = 0.12666546\n",
      "Iteration 1961, loss = 0.26086154\n",
      "Iteration 1962, loss = 0.19332032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1963, loss = 0.12536405\n",
      "Iteration 1964, loss = 0.19586497\n",
      "Iteration 1965, loss = 0.34438982\n",
      "Iteration 1966, loss = 0.20883058\n",
      "Iteration 1967, loss = 0.14034071\n",
      "Iteration 1968, loss = 0.13961161\n",
      "Iteration 1969, loss = 0.13023046\n",
      "Iteration 1970, loss = 0.13030948\n",
      "Iteration 1971, loss = 0.16910676\n",
      "Iteration 1972, loss = 0.16186554\n",
      "Iteration 1973, loss = 0.22001344\n",
      "Iteration 1974, loss = 0.23523209\n",
      "Iteration 1975, loss = 0.11449831\n",
      "Iteration 1976, loss = 0.12386678\n",
      "Iteration 1977, loss = 0.14180781\n",
      "Iteration 1978, loss = 0.12850783\n",
      "Iteration 1979, loss = 0.11604460\n",
      "Iteration 1980, loss = 0.11183936\n",
      "Iteration 1981, loss = 0.13053369\n",
      "Iteration 1982, loss = 0.10312403\n",
      "Iteration 1983, loss = 0.13821784\n",
      "Iteration 1984, loss = 0.13012794\n",
      "Iteration 1985, loss = 0.16099847\n",
      "Iteration 1986, loss = 0.25458369\n",
      "Iteration 1987, loss = 0.31852987\n",
      "Iteration 1988, loss = 0.26547061\n",
      "Iteration 1989, loss = 0.13886900\n",
      "Iteration 1990, loss = 0.19362197\n",
      "Iteration 1991, loss = 0.25654269\n",
      "Iteration 1992, loss = 0.22204992\n",
      "Iteration 1993, loss = 0.19292875\n",
      "Iteration 1994, loss = 0.16177574\n",
      "Iteration 1995, loss = 0.15816338\n",
      "Iteration 1996, loss = 0.12870435\n",
      "Iteration 1997, loss = 0.13114618\n",
      "Iteration 1998, loss = 0.11685183\n",
      "Iteration 1999, loss = 0.13051673\n",
      "Iteration 2000, loss = 0.11881611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:617: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9478260869565217"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import learning_curve\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# mlps = []\n",
    "# mlp = MLPClassifier(hidden_layer_sizes=(1000,200), max_iter=2000, alpha=1e-4,\n",
    "#                     solver='sgd', verbose=100,\n",
    "#                     learning_rate_init=0.01, n_iter_no_change=1000)\n",
    "# #x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2)\n",
    "# mlp.fit(x_train, y_train)\n",
    "# mlp.score(x_test, y_test)\n",
    "# #train_sizes, train_scores, valid_scores = learning_curve(mlp, data, label, train_sizes=np.linspace(0.1, 1.0, 1), cv=2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8983050847457628"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52531646, 0.51582278]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 505 samples, validate on 127 samples\n",
      "Epoch 1/2000\n",
      "505/505 [==============================] - 1s 1ms/step - loss: 0.6981 - accuracy: 0.4653 - val_loss: 0.7009 - val_accuracy: 0.4646\n",
      "Epoch 2/2000\n",
      "505/505 [==============================] - 0s 234us/step - loss: 0.6932 - accuracy: 0.5168 - val_loss: 0.7048 - val_accuracy: 0.4173\n",
      "Epoch 3/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6889 - accuracy: 0.5149 - val_loss: 0.7057 - val_accuracy: 0.4646\n",
      "Epoch 4/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6913 - accuracy: 0.5030 - val_loss: 0.7069 - val_accuracy: 0.4724\n",
      "Epoch 5/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6887 - accuracy: 0.5149 - val_loss: 0.7084 - val_accuracy: 0.4646\n",
      "Epoch 6/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6856 - accuracy: 0.5604 - val_loss: 0.7101 - val_accuracy: 0.4488\n",
      "Epoch 7/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6878 - accuracy: 0.5347 - val_loss: 0.7122 - val_accuracy: 0.4488\n",
      "Epoch 8/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6877 - accuracy: 0.5465 - val_loss: 0.7125 - val_accuracy: 0.4488\n",
      "Epoch 9/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6886 - accuracy: 0.5545 - val_loss: 0.7139 - val_accuracy: 0.4567\n",
      "Epoch 10/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6864 - accuracy: 0.5525 - val_loss: 0.7139 - val_accuracy: 0.4567\n",
      "Epoch 11/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6905 - accuracy: 0.5545 - val_loss: 0.7135 - val_accuracy: 0.4567\n",
      "Epoch 12/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6891 - accuracy: 0.5386 - val_loss: 0.7136 - val_accuracy: 0.4567\n",
      "Epoch 13/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6838 - accuracy: 0.5505 - val_loss: 0.7149 - val_accuracy: 0.4567\n",
      "Epoch 14/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6889 - accuracy: 0.5584 - val_loss: 0.7164 - val_accuracy: 0.4567\n",
      "Epoch 15/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6879 - accuracy: 0.5545 - val_loss: 0.7173 - val_accuracy: 0.4567\n",
      "Epoch 16/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6838 - accuracy: 0.5683 - val_loss: 0.7167 - val_accuracy: 0.4567\n",
      "Epoch 17/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6858 - accuracy: 0.5485 - val_loss: 0.7144 - val_accuracy: 0.4567\n",
      "Epoch 18/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6875 - accuracy: 0.5366 - val_loss: 0.7158 - val_accuracy: 0.4567\n",
      "Epoch 19/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6831 - accuracy: 0.5485 - val_loss: 0.7149 - val_accuracy: 0.4567\n",
      "Epoch 20/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6851 - accuracy: 0.5386 - val_loss: 0.7144 - val_accuracy: 0.4724\n",
      "Epoch 21/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6847 - accuracy: 0.5663 - val_loss: 0.7151 - val_accuracy: 0.4646\n",
      "Epoch 22/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6835 - accuracy: 0.5545 - val_loss: 0.7178 - val_accuracy: 0.4567\n",
      "Epoch 23/2000\n",
      "505/505 [==============================] - 0s 224us/step - loss: 0.6804 - accuracy: 0.5703 - val_loss: 0.7190 - val_accuracy: 0.4567\n",
      "Epoch 24/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6842 - accuracy: 0.5624 - val_loss: 0.7195 - val_accuracy: 0.4567\n",
      "Epoch 25/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6831 - accuracy: 0.5663 - val_loss: 0.7188 - val_accuracy: 0.4646\n",
      "Epoch 26/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6811 - accuracy: 0.5644 - val_loss: 0.7189 - val_accuracy: 0.4803\n",
      "Epoch 27/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6838 - accuracy: 0.5485 - val_loss: 0.7205 - val_accuracy: 0.4724\n",
      "Epoch 28/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6818 - accuracy: 0.5545 - val_loss: 0.7194 - val_accuracy: 0.4724\n",
      "Epoch 29/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6831 - accuracy: 0.5505 - val_loss: 0.7202 - val_accuracy: 0.4724\n",
      "Epoch 30/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6838 - accuracy: 0.5485 - val_loss: 0.7198 - val_accuracy: 0.4724\n",
      "Epoch 31/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6829 - accuracy: 0.5584 - val_loss: 0.7181 - val_accuracy: 0.4724\n",
      "Epoch 32/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6833 - accuracy: 0.5564 - val_loss: 0.7191 - val_accuracy: 0.4724\n",
      "Epoch 33/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6860 - accuracy: 0.5564 - val_loss: 0.7182 - val_accuracy: 0.4724\n",
      "Epoch 34/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6824 - accuracy: 0.5525 - val_loss: 0.7202 - val_accuracy: 0.4724\n",
      "Epoch 35/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6833 - accuracy: 0.5525 - val_loss: 0.7192 - val_accuracy: 0.4724\n",
      "Epoch 36/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6836 - accuracy: 0.5564 - val_loss: 0.7211 - val_accuracy: 0.4724\n",
      "Epoch 37/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6847 - accuracy: 0.5564 - val_loss: 0.7204 - val_accuracy: 0.4724\n",
      "Epoch 38/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6810 - accuracy: 0.5564 - val_loss: 0.7199 - val_accuracy: 0.4724\n",
      "Epoch 39/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6803 - accuracy: 0.5644 - val_loss: 0.7200 - val_accuracy: 0.4724\n",
      "Epoch 40/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6801 - accuracy: 0.5584 - val_loss: 0.7191 - val_accuracy: 0.4724\n",
      "Epoch 41/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6798 - accuracy: 0.5683 - val_loss: 0.7190 - val_accuracy: 0.4724\n",
      "Epoch 42/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6852 - accuracy: 0.5584 - val_loss: 0.7206 - val_accuracy: 0.4724\n",
      "Epoch 43/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6831 - accuracy: 0.5485 - val_loss: 0.7196 - val_accuracy: 0.4724\n",
      "Epoch 44/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6859 - accuracy: 0.5624 - val_loss: 0.7199 - val_accuracy: 0.4724\n",
      "Epoch 45/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6830 - accuracy: 0.5644 - val_loss: 0.7222 - val_accuracy: 0.4724\n",
      "Epoch 46/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6786 - accuracy: 0.5624 - val_loss: 0.7200 - val_accuracy: 0.4724\n",
      "Epoch 47/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6829 - accuracy: 0.5564 - val_loss: 0.7204 - val_accuracy: 0.4724\n",
      "Epoch 48/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6810 - accuracy: 0.5644 - val_loss: 0.7221 - val_accuracy: 0.4724\n",
      "Epoch 49/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6749 - accuracy: 0.5644 - val_loss: 0.7221 - val_accuracy: 0.4724\n",
      "Epoch 50/2000\n",
      "505/505 [==============================] - 0s 224us/step - loss: 0.6833 - accuracy: 0.5584 - val_loss: 0.7231 - val_accuracy: 0.4724\n",
      "Epoch 51/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6793 - accuracy: 0.5624 - val_loss: 0.7228 - val_accuracy: 0.4724\n",
      "Epoch 52/2000\n",
      "505/505 [==============================] - 0s 228us/step - loss: 0.6784 - accuracy: 0.5624 - val_loss: 0.7213 - val_accuracy: 0.4724\n",
      "Epoch 53/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6824 - accuracy: 0.5525 - val_loss: 0.7214 - val_accuracy: 0.4724\n",
      "Epoch 54/2000\n",
      "505/505 [==============================] - 0s 231us/step - loss: 0.6800 - accuracy: 0.5644 - val_loss: 0.7214 - val_accuracy: 0.4724\n",
      "Epoch 55/2000\n",
      "505/505 [==============================] - 0s 231us/step - loss: 0.6809 - accuracy: 0.5564 - val_loss: 0.7222 - val_accuracy: 0.4724\n",
      "Epoch 56/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6824 - accuracy: 0.5604 - val_loss: 0.7218 - val_accuracy: 0.4724\n",
      "Epoch 57/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6796 - accuracy: 0.5644 - val_loss: 0.7210 - val_accuracy: 0.4724\n",
      "Epoch 58/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6786 - accuracy: 0.5644 - val_loss: 0.7228 - val_accuracy: 0.4724\n",
      "Epoch 59/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6801 - accuracy: 0.5604 - val_loss: 0.7232 - val_accuracy: 0.4724\n",
      "Epoch 60/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6819 - accuracy: 0.5644 - val_loss: 0.7225 - val_accuracy: 0.4724\n",
      "Epoch 61/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6869 - accuracy: 0.5446 - val_loss: 0.7241 - val_accuracy: 0.4724\n",
      "Epoch 62/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6800 - accuracy: 0.5446 - val_loss: 0.7248 - val_accuracy: 0.4724\n",
      "Epoch 63/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6791 - accuracy: 0.5624 - val_loss: 0.7254 - val_accuracy: 0.4724\n",
      "Epoch 64/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6772 - accuracy: 0.5604 - val_loss: 0.7238 - val_accuracy: 0.4724\n",
      "Epoch 65/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6760 - accuracy: 0.5703 - val_loss: 0.7248 - val_accuracy: 0.4724\n",
      "Epoch 66/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6769 - accuracy: 0.5683 - val_loss: 0.7244 - val_accuracy: 0.4724\n",
      "Epoch 67/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6774 - accuracy: 0.5644 - val_loss: 0.7218 - val_accuracy: 0.4724\n",
      "Epoch 68/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6823 - accuracy: 0.5604 - val_loss: 0.7196 - val_accuracy: 0.4724\n",
      "Epoch 69/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6777 - accuracy: 0.5663 - val_loss: 0.7223 - val_accuracy: 0.4724\n",
      "Epoch 70/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6746 - accuracy: 0.5663 - val_loss: 0.7238 - val_accuracy: 0.4724\n",
      "Epoch 71/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6755 - accuracy: 0.5743 - val_loss: 0.7251 - val_accuracy: 0.4724\n",
      "Epoch 72/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6788 - accuracy: 0.5505 - val_loss: 0.7257 - val_accuracy: 0.4724\n",
      "Epoch 73/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6779 - accuracy: 0.5584 - val_loss: 0.7245 - val_accuracy: 0.4724\n",
      "Epoch 74/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6799 - accuracy: 0.5762 - val_loss: 0.7229 - val_accuracy: 0.4724\n",
      "Epoch 75/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6757 - accuracy: 0.5624 - val_loss: 0.7238 - val_accuracy: 0.4724\n",
      "Epoch 76/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6780 - accuracy: 0.5683 - val_loss: 0.7233 - val_accuracy: 0.4724\n",
      "Epoch 77/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6746 - accuracy: 0.5762 - val_loss: 0.7238 - val_accuracy: 0.4724\n",
      "Epoch 78/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6844 - accuracy: 0.5426 - val_loss: 0.7224 - val_accuracy: 0.4724\n",
      "Epoch 79/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6798 - accuracy: 0.5584 - val_loss: 0.7212 - val_accuracy: 0.4724\n",
      "Epoch 80/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6796 - accuracy: 0.5663 - val_loss: 0.7206 - val_accuracy: 0.4724\n",
      "Epoch 81/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6785 - accuracy: 0.5584 - val_loss: 0.7217 - val_accuracy: 0.4724\n",
      "Epoch 82/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.6772 - accuracy: 0.5644 - val_loss: 0.7211 - val_accuracy: 0.4724\n",
      "Epoch 83/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6791 - accuracy: 0.5604 - val_loss: 0.7220 - val_accuracy: 0.4724\n",
      "Epoch 84/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6746 - accuracy: 0.5703 - val_loss: 0.7227 - val_accuracy: 0.4724\n",
      "Epoch 85/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6762 - accuracy: 0.5743 - val_loss: 0.7224 - val_accuracy: 0.4724\n",
      "Epoch 86/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6789 - accuracy: 0.5545 - val_loss: 0.7222 - val_accuracy: 0.4724\n",
      "Epoch 87/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6782 - accuracy: 0.5723 - val_loss: 0.7240 - val_accuracy: 0.4724\n",
      "Epoch 88/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6773 - accuracy: 0.5663 - val_loss: 0.7250 - val_accuracy: 0.4724\n",
      "Epoch 89/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6785 - accuracy: 0.5644 - val_loss: 0.7238 - val_accuracy: 0.4724\n",
      "Epoch 90/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6773 - accuracy: 0.5545 - val_loss: 0.7237 - val_accuracy: 0.4724\n",
      "Epoch 91/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6807 - accuracy: 0.5564 - val_loss: 0.7266 - val_accuracy: 0.4724\n",
      "Epoch 92/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6754 - accuracy: 0.5604 - val_loss: 0.7244 - val_accuracy: 0.4724\n",
      "Epoch 93/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6813 - accuracy: 0.5604 - val_loss: 0.7246 - val_accuracy: 0.4724\n",
      "Epoch 94/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.6753 - accuracy: 0.5703 - val_loss: 0.7265 - val_accuracy: 0.4724\n",
      "Epoch 95/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6789 - accuracy: 0.5663 - val_loss: 0.7260 - val_accuracy: 0.4724\n",
      "Epoch 96/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6757 - accuracy: 0.5743 - val_loss: 0.7246 - val_accuracy: 0.4724\n",
      "Epoch 97/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6796 - accuracy: 0.5644 - val_loss: 0.7237 - val_accuracy: 0.4724\n",
      "Epoch 98/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6779 - accuracy: 0.5644 - val_loss: 0.7243 - val_accuracy: 0.4724\n",
      "Epoch 99/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6761 - accuracy: 0.5644 - val_loss: 0.7251 - val_accuracy: 0.4724\n",
      "Epoch 100/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6796 - accuracy: 0.5604 - val_loss: 0.7245 - val_accuracy: 0.4724\n",
      "Epoch 101/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6702 - accuracy: 0.5743 - val_loss: 0.7241 - val_accuracy: 0.4724\n",
      "Epoch 102/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6702 - accuracy: 0.5743 - val_loss: 0.7251 - val_accuracy: 0.4724\n",
      "Epoch 103/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6717 - accuracy: 0.5782 - val_loss: 0.7241 - val_accuracy: 0.4724\n",
      "Epoch 104/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6786 - accuracy: 0.5683 - val_loss: 0.7253 - val_accuracy: 0.4724\n",
      "Epoch 105/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6696 - accuracy: 0.5743 - val_loss: 0.7256 - val_accuracy: 0.4724\n",
      "Epoch 106/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6841 - accuracy: 0.5564 - val_loss: 0.7264 - val_accuracy: 0.4724\n",
      "Epoch 107/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6773 - accuracy: 0.5663 - val_loss: 0.7253 - val_accuracy: 0.4724\n",
      "Epoch 108/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6807 - accuracy: 0.5624 - val_loss: 0.7264 - val_accuracy: 0.4724\n",
      "Epoch 109/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6782 - accuracy: 0.5505 - val_loss: 0.7253 - val_accuracy: 0.4724\n",
      "Epoch 110/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6759 - accuracy: 0.5663 - val_loss: 0.7239 - val_accuracy: 0.4724\n",
      "Epoch 111/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6794 - accuracy: 0.5545 - val_loss: 0.7241 - val_accuracy: 0.4724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6789 - accuracy: 0.5545 - val_loss: 0.7250 - val_accuracy: 0.4724\n",
      "Epoch 113/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6785 - accuracy: 0.5604 - val_loss: 0.7259 - val_accuracy: 0.4724\n",
      "Epoch 114/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6777 - accuracy: 0.5564 - val_loss: 0.7240 - val_accuracy: 0.4724\n",
      "Epoch 115/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6743 - accuracy: 0.5762 - val_loss: 0.7250 - val_accuracy: 0.4724\n",
      "Epoch 116/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6780 - accuracy: 0.5663 - val_loss: 0.7231 - val_accuracy: 0.4724\n",
      "Epoch 117/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6762 - accuracy: 0.5584 - val_loss: 0.7235 - val_accuracy: 0.4724\n",
      "Epoch 118/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6748 - accuracy: 0.5564 - val_loss: 0.7210 - val_accuracy: 0.4724\n",
      "Epoch 119/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6759 - accuracy: 0.5663 - val_loss: 0.7232 - val_accuracy: 0.4724\n",
      "Epoch 120/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6782 - accuracy: 0.5624 - val_loss: 0.7242 - val_accuracy: 0.4724\n",
      "Epoch 121/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6753 - accuracy: 0.5743 - val_loss: 0.7246 - val_accuracy: 0.4724\n",
      "Epoch 122/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.6752 - accuracy: 0.5604 - val_loss: 0.7238 - val_accuracy: 0.4724\n",
      "Epoch 123/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6801 - accuracy: 0.5584 - val_loss: 0.7255 - val_accuracy: 0.4724\n",
      "Epoch 124/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6753 - accuracy: 0.5564 - val_loss: 0.7250 - val_accuracy: 0.4724\n",
      "Epoch 125/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6734 - accuracy: 0.5683 - val_loss: 0.7245 - val_accuracy: 0.4724\n",
      "Epoch 126/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6785 - accuracy: 0.5703 - val_loss: 0.7234 - val_accuracy: 0.4724\n",
      "Epoch 127/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6782 - accuracy: 0.5505 - val_loss: 0.7245 - val_accuracy: 0.4724\n",
      "Epoch 128/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6745 - accuracy: 0.5683 - val_loss: 0.7252 - val_accuracy: 0.4724\n",
      "Epoch 129/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6777 - accuracy: 0.5743 - val_loss: 0.7263 - val_accuracy: 0.4724\n",
      "Epoch 130/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6753 - accuracy: 0.5604 - val_loss: 0.7251 - val_accuracy: 0.4724\n",
      "Epoch 131/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6759 - accuracy: 0.5564 - val_loss: 0.7241 - val_accuracy: 0.4724\n",
      "Epoch 132/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6771 - accuracy: 0.5584 - val_loss: 0.7244 - val_accuracy: 0.4724\n",
      "Epoch 133/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6813 - accuracy: 0.5505 - val_loss: 0.7256 - val_accuracy: 0.4724\n",
      "Epoch 134/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6778 - accuracy: 0.5584 - val_loss: 0.7287 - val_accuracy: 0.4724\n",
      "Epoch 135/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6761 - accuracy: 0.5604 - val_loss: 0.7288 - val_accuracy: 0.4724\n",
      "Epoch 136/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6838 - accuracy: 0.5564 - val_loss: 0.7289 - val_accuracy: 0.4724\n",
      "Epoch 137/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6713 - accuracy: 0.5644 - val_loss: 0.7250 - val_accuracy: 0.4724\n",
      "Epoch 138/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6767 - accuracy: 0.5604 - val_loss: 0.7238 - val_accuracy: 0.4724\n",
      "Epoch 139/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6808 - accuracy: 0.5485 - val_loss: 0.7240 - val_accuracy: 0.4724\n",
      "Epoch 140/2000\n",
      "505/505 [==============================] - 0s 208us/step - loss: 0.6755 - accuracy: 0.5564 - val_loss: 0.7240 - val_accuracy: 0.4724\n",
      "Epoch 141/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6752 - accuracy: 0.5683 - val_loss: 0.7249 - val_accuracy: 0.4724\n",
      "Epoch 142/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6750 - accuracy: 0.5584 - val_loss: 0.7243 - val_accuracy: 0.4724\n",
      "Epoch 143/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6723 - accuracy: 0.5762 - val_loss: 0.7241 - val_accuracy: 0.4724\n",
      "Epoch 144/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6760 - accuracy: 0.5644 - val_loss: 0.7243 - val_accuracy: 0.4724\n",
      "Epoch 145/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6780 - accuracy: 0.5644 - val_loss: 0.7245 - val_accuracy: 0.4724\n",
      "Epoch 146/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6735 - accuracy: 0.5842 - val_loss: 0.7248 - val_accuracy: 0.4724\n",
      "Epoch 147/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6761 - accuracy: 0.5485 - val_loss: 0.7293 - val_accuracy: 0.4724\n",
      "Epoch 148/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6707 - accuracy: 0.5782 - val_loss: 0.7304 - val_accuracy: 0.4724\n",
      "Epoch 149/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6704 - accuracy: 0.5703 - val_loss: 0.7287 - val_accuracy: 0.4724\n",
      "Epoch 150/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6764 - accuracy: 0.5644 - val_loss: 0.7268 - val_accuracy: 0.4724\n",
      "Epoch 151/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6754 - accuracy: 0.5663 - val_loss: 0.7286 - val_accuracy: 0.4724\n",
      "Epoch 152/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6755 - accuracy: 0.5683 - val_loss: 0.7275 - val_accuracy: 0.4724\n",
      "Epoch 153/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6752 - accuracy: 0.5644 - val_loss: 0.7273 - val_accuracy: 0.4724\n",
      "Epoch 154/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6758 - accuracy: 0.5644 - val_loss: 0.7261 - val_accuracy: 0.4724\n",
      "Epoch 155/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6765 - accuracy: 0.5683 - val_loss: 0.7242 - val_accuracy: 0.4724\n",
      "Epoch 156/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6722 - accuracy: 0.5762 - val_loss: 0.7259 - val_accuracy: 0.4724\n",
      "Epoch 157/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6725 - accuracy: 0.5663 - val_loss: 0.7267 - val_accuracy: 0.4724\n",
      "Epoch 158/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6748 - accuracy: 0.5723 - val_loss: 0.7287 - val_accuracy: 0.4724\n",
      "Epoch 159/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6774 - accuracy: 0.5525 - val_loss: 0.7285 - val_accuracy: 0.4724\n",
      "Epoch 160/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6745 - accuracy: 0.5505 - val_loss: 0.7281 - val_accuracy: 0.4724\n",
      "Epoch 161/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6790 - accuracy: 0.5624 - val_loss: 0.7263 - val_accuracy: 0.4724\n",
      "Epoch 162/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6746 - accuracy: 0.5545 - val_loss: 0.7250 - val_accuracy: 0.4724\n",
      "Epoch 163/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6718 - accuracy: 0.5644 - val_loss: 0.7259 - val_accuracy: 0.4724\n",
      "Epoch 164/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6733 - accuracy: 0.5545 - val_loss: 0.7267 - val_accuracy: 0.4724\n",
      "Epoch 165/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6742 - accuracy: 0.5723 - val_loss: 0.7278 - val_accuracy: 0.4724\n",
      "Epoch 166/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6699 - accuracy: 0.5663 - val_loss: 0.7266 - val_accuracy: 0.4724\n",
      "Epoch 167/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6746 - accuracy: 0.5624 - val_loss: 0.7268 - val_accuracy: 0.4724\n",
      "Epoch 168/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6696 - accuracy: 0.5604 - val_loss: 0.7284 - val_accuracy: 0.4724\n",
      "Epoch 169/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6738 - accuracy: 0.5782 - val_loss: 0.7286 - val_accuracy: 0.4724\n",
      "Epoch 170/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6726 - accuracy: 0.5604 - val_loss: 0.7281 - val_accuracy: 0.4724\n",
      "Epoch 171/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6749 - accuracy: 0.5564 - val_loss: 0.7292 - val_accuracy: 0.4724\n",
      "Epoch 172/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6754 - accuracy: 0.5525 - val_loss: 0.7275 - val_accuracy: 0.4724\n",
      "Epoch 173/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6722 - accuracy: 0.5683 - val_loss: 0.7287 - val_accuracy: 0.4724\n",
      "Epoch 174/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6720 - accuracy: 0.5802 - val_loss: 0.7292 - val_accuracy: 0.4724\n",
      "Epoch 175/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6735 - accuracy: 0.5604 - val_loss: 0.7247 - val_accuracy: 0.4724\n",
      "Epoch 176/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6727 - accuracy: 0.5525 - val_loss: 0.7253 - val_accuracy: 0.4724\n",
      "Epoch 177/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6772 - accuracy: 0.5525 - val_loss: 0.7276 - val_accuracy: 0.4724\n",
      "Epoch 178/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6723 - accuracy: 0.5644 - val_loss: 0.7276 - val_accuracy: 0.4724\n",
      "Epoch 179/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6781 - accuracy: 0.5505 - val_loss: 0.7286 - val_accuracy: 0.4724\n",
      "Epoch 180/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6735 - accuracy: 0.5545 - val_loss: 0.7274 - val_accuracy: 0.4724\n",
      "Epoch 181/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6732 - accuracy: 0.5604 - val_loss: 0.7262 - val_accuracy: 0.4724\n",
      "Epoch 182/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6754 - accuracy: 0.5663 - val_loss: 0.7280 - val_accuracy: 0.4724\n",
      "Epoch 183/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6768 - accuracy: 0.5624 - val_loss: 0.7236 - val_accuracy: 0.4724\n",
      "Epoch 184/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6748 - accuracy: 0.5584 - val_loss: 0.7260 - val_accuracy: 0.4724\n",
      "Epoch 185/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6739 - accuracy: 0.5604 - val_loss: 0.7275 - val_accuracy: 0.4724\n",
      "Epoch 186/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6720 - accuracy: 0.5624 - val_loss: 0.7283 - val_accuracy: 0.4724\n",
      "Epoch 187/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6691 - accuracy: 0.5743 - val_loss: 0.7291 - val_accuracy: 0.4724\n",
      "Epoch 188/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6734 - accuracy: 0.5604 - val_loss: 0.7271 - val_accuracy: 0.4724\n",
      "Epoch 189/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6717 - accuracy: 0.5663 - val_loss: 0.7298 - val_accuracy: 0.4724\n",
      "Epoch 190/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6733 - accuracy: 0.5525 - val_loss: 0.7297 - val_accuracy: 0.4724\n",
      "Epoch 191/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6695 - accuracy: 0.5644 - val_loss: 0.7264 - val_accuracy: 0.4724\n",
      "Epoch 192/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6736 - accuracy: 0.5604 - val_loss: 0.7258 - val_accuracy: 0.4724\n",
      "Epoch 193/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6712 - accuracy: 0.5663 - val_loss: 0.7281 - val_accuracy: 0.4724\n",
      "Epoch 194/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6708 - accuracy: 0.5723 - val_loss: 0.7263 - val_accuracy: 0.4724\n",
      "Epoch 195/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6761 - accuracy: 0.5584 - val_loss: 0.7286 - val_accuracy: 0.4724\n",
      "Epoch 196/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6783 - accuracy: 0.5525 - val_loss: 0.7311 - val_accuracy: 0.4724\n",
      "Epoch 197/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.6717 - accuracy: 0.5683 - val_loss: 0.7319 - val_accuracy: 0.4724\n",
      "Epoch 198/2000\n",
      "505/505 [==============================] - 0s 233us/step - loss: 0.6713 - accuracy: 0.5723 - val_loss: 0.7316 - val_accuracy: 0.4724\n",
      "Epoch 199/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6706 - accuracy: 0.5644 - val_loss: 0.7301 - val_accuracy: 0.4724\n",
      "Epoch 200/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6748 - accuracy: 0.5644 - val_loss: 0.7295 - val_accuracy: 0.4724\n",
      "Epoch 201/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6699 - accuracy: 0.5743 - val_loss: 0.7276 - val_accuracy: 0.4724\n",
      "Epoch 202/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6702 - accuracy: 0.5584 - val_loss: 0.7291 - val_accuracy: 0.4724\n",
      "Epoch 203/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6704 - accuracy: 0.5604 - val_loss: 0.7278 - val_accuracy: 0.4724\n",
      "Epoch 204/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6727 - accuracy: 0.5663 - val_loss: 0.7279 - val_accuracy: 0.4724\n",
      "Epoch 205/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6722 - accuracy: 0.5525 - val_loss: 0.7279 - val_accuracy: 0.4724\n",
      "Epoch 206/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6756 - accuracy: 0.5723 - val_loss: 0.7287 - val_accuracy: 0.4724\n",
      "Epoch 207/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6696 - accuracy: 0.5525 - val_loss: 0.7261 - val_accuracy: 0.4724\n",
      "Epoch 208/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6721 - accuracy: 0.5663 - val_loss: 0.7266 - val_accuracy: 0.4724\n",
      "Epoch 209/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6717 - accuracy: 0.5743 - val_loss: 0.7276 - val_accuracy: 0.4724\n",
      "Epoch 210/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6734 - accuracy: 0.5465 - val_loss: 0.7308 - val_accuracy: 0.4724\n",
      "Epoch 211/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6698 - accuracy: 0.5762 - val_loss: 0.7294 - val_accuracy: 0.4724\n",
      "Epoch 212/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6726 - accuracy: 0.5624 - val_loss: 0.7275 - val_accuracy: 0.4724\n",
      "Epoch 213/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6715 - accuracy: 0.5743 - val_loss: 0.7287 - val_accuracy: 0.4724\n",
      "Epoch 214/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6740 - accuracy: 0.5604 - val_loss: 0.7270 - val_accuracy: 0.4724\n",
      "Epoch 215/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6688 - accuracy: 0.5723 - val_loss: 0.7297 - val_accuracy: 0.4724\n",
      "Epoch 216/2000\n",
      "505/505 [==============================] - 0s 281us/step - loss: 0.6681 - accuracy: 0.5743 - val_loss: 0.7307 - val_accuracy: 0.4724\n",
      "Epoch 217/2000\n",
      "505/505 [==============================] - 0s 265us/step - loss: 0.6688 - accuracy: 0.5762 - val_loss: 0.7274 - val_accuracy: 0.4724\n",
      "Epoch 218/2000\n",
      "505/505 [==============================] - 0s 267us/step - loss: 0.6742 - accuracy: 0.5762 - val_loss: 0.7290 - val_accuracy: 0.4724\n",
      "Epoch 219/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6717 - accuracy: 0.5743 - val_loss: 0.7310 - val_accuracy: 0.4724\n",
      "Epoch 220/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6729 - accuracy: 0.5743 - val_loss: 0.7307 - val_accuracy: 0.4724\n",
      "Epoch 221/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6763 - accuracy: 0.5604 - val_loss: 0.7301 - val_accuracy: 0.4724\n",
      "Epoch 222/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 217us/step - loss: 0.6703 - accuracy: 0.5683 - val_loss: 0.7276 - val_accuracy: 0.4724\n",
      "Epoch 223/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6686 - accuracy: 0.5743 - val_loss: 0.7303 - val_accuracy: 0.4724\n",
      "Epoch 224/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6735 - accuracy: 0.5683 - val_loss: 0.7306 - val_accuracy: 0.4724\n",
      "Epoch 225/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6718 - accuracy: 0.5703 - val_loss: 0.7279 - val_accuracy: 0.4724\n",
      "Epoch 226/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6751 - accuracy: 0.5505 - val_loss: 0.7295 - val_accuracy: 0.4724\n",
      "Epoch 227/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6702 - accuracy: 0.5624 - val_loss: 0.7310 - val_accuracy: 0.4724\n",
      "Epoch 228/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6680 - accuracy: 0.5604 - val_loss: 0.7270 - val_accuracy: 0.4724\n",
      "Epoch 229/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6714 - accuracy: 0.5663 - val_loss: 0.7277 - val_accuracy: 0.4724\n",
      "Epoch 230/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6735 - accuracy: 0.5644 - val_loss: 0.7265 - val_accuracy: 0.4724\n",
      "Epoch 231/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6695 - accuracy: 0.5762 - val_loss: 0.7257 - val_accuracy: 0.4724\n",
      "Epoch 232/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6713 - accuracy: 0.5762 - val_loss: 0.7282 - val_accuracy: 0.4724\n",
      "Epoch 233/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6724 - accuracy: 0.5485 - val_loss: 0.7273 - val_accuracy: 0.4724\n",
      "Epoch 234/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6746 - accuracy: 0.5545 - val_loss: 0.7272 - val_accuracy: 0.4724\n",
      "Epoch 235/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6732 - accuracy: 0.5663 - val_loss: 0.7254 - val_accuracy: 0.4882\n",
      "Epoch 236/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6803 - accuracy: 0.5505 - val_loss: 0.7266 - val_accuracy: 0.4724\n",
      "Epoch 237/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6709 - accuracy: 0.5564 - val_loss: 0.7261 - val_accuracy: 0.4882\n",
      "Epoch 238/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6721 - accuracy: 0.5604 - val_loss: 0.7290 - val_accuracy: 0.4724\n",
      "Epoch 239/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6699 - accuracy: 0.5663 - val_loss: 0.7291 - val_accuracy: 0.4724\n",
      "Epoch 240/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6709 - accuracy: 0.5584 - val_loss: 0.7315 - val_accuracy: 0.4724\n",
      "Epoch 241/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6752 - accuracy: 0.5485 - val_loss: 0.7311 - val_accuracy: 0.4724\n",
      "Epoch 242/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6687 - accuracy: 0.5822 - val_loss: 0.7309 - val_accuracy: 0.4724\n",
      "Epoch 243/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6679 - accuracy: 0.5663 - val_loss: 0.7304 - val_accuracy: 0.4724\n",
      "Epoch 244/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6745 - accuracy: 0.5624 - val_loss: 0.7260 - val_accuracy: 0.5197\n",
      "Epoch 245/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6707 - accuracy: 0.5683 - val_loss: 0.7276 - val_accuracy: 0.4724\n",
      "Epoch 246/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6740 - accuracy: 0.5505 - val_loss: 0.7287 - val_accuracy: 0.4724\n",
      "Epoch 247/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6716 - accuracy: 0.5723 - val_loss: 0.7305 - val_accuracy: 0.4724\n",
      "Epoch 248/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6746 - accuracy: 0.5465 - val_loss: 0.7312 - val_accuracy: 0.4724\n",
      "Epoch 249/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6682 - accuracy: 0.5644 - val_loss: 0.7302 - val_accuracy: 0.4724\n",
      "Epoch 250/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6748 - accuracy: 0.5505 - val_loss: 0.7303 - val_accuracy: 0.4724\n",
      "Epoch 251/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6713 - accuracy: 0.5624 - val_loss: 0.7328 - val_accuracy: 0.4724\n",
      "Epoch 252/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6670 - accuracy: 0.5683 - val_loss: 0.7300 - val_accuracy: 0.4724\n",
      "Epoch 253/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6713 - accuracy: 0.5683 - val_loss: 0.7290 - val_accuracy: 0.4724\n",
      "Epoch 254/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6712 - accuracy: 0.5663 - val_loss: 0.7306 - val_accuracy: 0.4724\n",
      "Epoch 255/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6668 - accuracy: 0.5861 - val_loss: 0.7333 - val_accuracy: 0.4724\n",
      "Epoch 256/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6682 - accuracy: 0.5663 - val_loss: 0.7332 - val_accuracy: 0.4724\n",
      "Epoch 257/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6744 - accuracy: 0.5485 - val_loss: 0.7335 - val_accuracy: 0.4724\n",
      "Epoch 258/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6698 - accuracy: 0.5644 - val_loss: 0.7300 - val_accuracy: 0.4724\n",
      "Epoch 259/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6686 - accuracy: 0.5584 - val_loss: 0.7303 - val_accuracy: 0.4724\n",
      "Epoch 260/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6677 - accuracy: 0.5525 - val_loss: 0.7312 - val_accuracy: 0.4724\n",
      "Epoch 261/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6659 - accuracy: 0.5663 - val_loss: 0.7333 - val_accuracy: 0.4724\n",
      "Epoch 262/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6696 - accuracy: 0.5663 - val_loss: 0.7338 - val_accuracy: 0.4724\n",
      "Epoch 263/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6673 - accuracy: 0.5723 - val_loss: 0.7321 - val_accuracy: 0.4724\n",
      "Epoch 264/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6683 - accuracy: 0.5644 - val_loss: 0.7302 - val_accuracy: 0.4724\n",
      "Epoch 265/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6665 - accuracy: 0.5802 - val_loss: 0.7297 - val_accuracy: 0.4724\n",
      "Epoch 266/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6699 - accuracy: 0.5663 - val_loss: 0.7290 - val_accuracy: 0.4882\n",
      "Epoch 267/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6651 - accuracy: 0.5604 - val_loss: 0.7293 - val_accuracy: 0.4803\n",
      "Epoch 268/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6676 - accuracy: 0.5683 - val_loss: 0.7265 - val_accuracy: 0.4961\n",
      "Epoch 269/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6667 - accuracy: 0.5663 - val_loss: 0.7290 - val_accuracy: 0.5118\n",
      "Epoch 270/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6708 - accuracy: 0.5525 - val_loss: 0.7344 - val_accuracy: 0.4724\n",
      "Epoch 271/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6660 - accuracy: 0.5703 - val_loss: 0.7378 - val_accuracy: 0.4724\n",
      "Epoch 272/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6668 - accuracy: 0.5723 - val_loss: 0.7364 - val_accuracy: 0.4724\n",
      "Epoch 273/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6710 - accuracy: 0.5663 - val_loss: 0.7330 - val_accuracy: 0.4724\n",
      "Epoch 274/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6736 - accuracy: 0.5564 - val_loss: 0.7307 - val_accuracy: 0.4724\n",
      "Epoch 275/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6696 - accuracy: 0.5822 - val_loss: 0.7309 - val_accuracy: 0.4882\n",
      "Epoch 276/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6680 - accuracy: 0.5703 - val_loss: 0.7281 - val_accuracy: 0.4882\n",
      "Epoch 277/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6678 - accuracy: 0.5762 - val_loss: 0.7315 - val_accuracy: 0.4724\n",
      "Epoch 278/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6716 - accuracy: 0.5564 - val_loss: 0.7287 - val_accuracy: 0.4882\n",
      "Epoch 279/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6723 - accuracy: 0.5663 - val_loss: 0.7296 - val_accuracy: 0.4961\n",
      "Epoch 280/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6703 - accuracy: 0.5723 - val_loss: 0.7269 - val_accuracy: 0.4882\n",
      "Epoch 281/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6717 - accuracy: 0.5505 - val_loss: 0.7299 - val_accuracy: 0.4961\n",
      "Epoch 282/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6676 - accuracy: 0.5762 - val_loss: 0.7335 - val_accuracy: 0.4724\n",
      "Epoch 283/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6720 - accuracy: 0.5644 - val_loss: 0.7314 - val_accuracy: 0.5039\n",
      "Epoch 284/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6701 - accuracy: 0.5564 - val_loss: 0.7334 - val_accuracy: 0.4724\n",
      "Epoch 285/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6719 - accuracy: 0.5782 - val_loss: 0.7326 - val_accuracy: 0.4724\n",
      "Epoch 286/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6751 - accuracy: 0.5683 - val_loss: 0.7299 - val_accuracy: 0.4961\n",
      "Epoch 287/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6754 - accuracy: 0.5545 - val_loss: 0.7299 - val_accuracy: 0.4961\n",
      "Epoch 288/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6709 - accuracy: 0.5644 - val_loss: 0.7333 - val_accuracy: 0.4724\n",
      "Epoch 289/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6704 - accuracy: 0.5703 - val_loss: 0.7369 - val_accuracy: 0.4724\n",
      "Epoch 290/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6723 - accuracy: 0.5663 - val_loss: 0.7367 - val_accuracy: 0.4724\n",
      "Epoch 291/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6699 - accuracy: 0.5683 - val_loss: 0.7367 - val_accuracy: 0.4724\n",
      "Epoch 292/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6698 - accuracy: 0.5683 - val_loss: 0.7330 - val_accuracy: 0.5039\n",
      "Epoch 293/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6666 - accuracy: 0.5762 - val_loss: 0.7342 - val_accuracy: 0.4724\n",
      "Epoch 294/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6651 - accuracy: 0.5683 - val_loss: 0.7336 - val_accuracy: 0.5118\n",
      "Epoch 295/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6714 - accuracy: 0.5545 - val_loss: 0.7325 - val_accuracy: 0.5118\n",
      "Epoch 296/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6710 - accuracy: 0.5624 - val_loss: 0.7339 - val_accuracy: 0.5118\n",
      "Epoch 297/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6678 - accuracy: 0.5584 - val_loss: 0.7324 - val_accuracy: 0.4961\n",
      "Epoch 298/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6645 - accuracy: 0.5723 - val_loss: 0.7348 - val_accuracy: 0.4724\n",
      "Epoch 299/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6693 - accuracy: 0.5624 - val_loss: 0.7332 - val_accuracy: 0.5197\n",
      "Epoch 300/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6661 - accuracy: 0.5822 - val_loss: 0.7316 - val_accuracy: 0.4961\n",
      "Epoch 301/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6701 - accuracy: 0.5644 - val_loss: 0.7290 - val_accuracy: 0.4882\n",
      "Epoch 302/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6624 - accuracy: 0.5842 - val_loss: 0.7319 - val_accuracy: 0.4882\n",
      "Epoch 303/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6683 - accuracy: 0.5921 - val_loss: 0.7283 - val_accuracy: 0.4882\n",
      "Epoch 304/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6658 - accuracy: 0.5802 - val_loss: 0.7315 - val_accuracy: 0.4882\n",
      "Epoch 305/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6707 - accuracy: 0.5564 - val_loss: 0.7330 - val_accuracy: 0.4961\n",
      "Epoch 306/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6688 - accuracy: 0.5644 - val_loss: 0.7305 - val_accuracy: 0.4882\n",
      "Epoch 307/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6661 - accuracy: 0.5802 - val_loss: 0.7319 - val_accuracy: 0.4882\n",
      "Epoch 308/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6710 - accuracy: 0.5683 - val_loss: 0.7368 - val_accuracy: 0.4724\n",
      "Epoch 309/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6674 - accuracy: 0.5762 - val_loss: 0.7386 - val_accuracy: 0.4724\n",
      "Epoch 310/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6672 - accuracy: 0.5683 - val_loss: 0.7364 - val_accuracy: 0.4724\n",
      "Epoch 311/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6718 - accuracy: 0.5584 - val_loss: 0.7362 - val_accuracy: 0.4724\n",
      "Epoch 312/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6694 - accuracy: 0.5743 - val_loss: 0.7312 - val_accuracy: 0.4882\n",
      "Epoch 313/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6690 - accuracy: 0.5564 - val_loss: 0.7289 - val_accuracy: 0.4882\n",
      "Epoch 314/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6633 - accuracy: 0.5822 - val_loss: 0.7359 - val_accuracy: 0.5118\n",
      "Epoch 315/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6705 - accuracy: 0.5723 - val_loss: 0.7330 - val_accuracy: 0.4882\n",
      "Epoch 316/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6655 - accuracy: 0.5624 - val_loss: 0.7388 - val_accuracy: 0.4724\n",
      "Epoch 317/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6708 - accuracy: 0.5663 - val_loss: 0.7361 - val_accuracy: 0.4961\n",
      "Epoch 318/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6675 - accuracy: 0.5723 - val_loss: 0.7340 - val_accuracy: 0.5118\n",
      "Epoch 319/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6671 - accuracy: 0.5624 - val_loss: 0.7315 - val_accuracy: 0.4882\n",
      "Epoch 320/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6702 - accuracy: 0.5624 - val_loss: 0.7313 - val_accuracy: 0.4882\n",
      "Epoch 321/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6643 - accuracy: 0.5723 - val_loss: 0.7341 - val_accuracy: 0.5118\n",
      "Epoch 322/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6665 - accuracy: 0.5644 - val_loss: 0.7359 - val_accuracy: 0.5118\n",
      "Epoch 323/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6702 - accuracy: 0.5624 - val_loss: 0.7311 - val_accuracy: 0.4882\n",
      "Epoch 324/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6669 - accuracy: 0.5723 - val_loss: 0.7330 - val_accuracy: 0.4882\n",
      "Epoch 325/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6684 - accuracy: 0.5683 - val_loss: 0.7343 - val_accuracy: 0.5039\n",
      "Epoch 326/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6650 - accuracy: 0.5802 - val_loss: 0.7304 - val_accuracy: 0.4882\n",
      "Epoch 327/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6671 - accuracy: 0.5584 - val_loss: 0.7330 - val_accuracy: 0.4882\n",
      "Epoch 328/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6672 - accuracy: 0.5703 - val_loss: 0.7329 - val_accuracy: 0.4882\n",
      "Epoch 329/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6716 - accuracy: 0.5525 - val_loss: 0.7265 - val_accuracy: 0.4882\n",
      "Epoch 330/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6710 - accuracy: 0.5663 - val_loss: 0.7255 - val_accuracy: 0.4882\n",
      "Epoch 331/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6676 - accuracy: 0.5663 - val_loss: 0.7279 - val_accuracy: 0.4882\n",
      "Epoch 332/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 213us/step - loss: 0.6706 - accuracy: 0.5683 - val_loss: 0.7297 - val_accuracy: 0.4882\n",
      "Epoch 333/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6647 - accuracy: 0.5802 - val_loss: 0.7343 - val_accuracy: 0.4882\n",
      "Epoch 334/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6681 - accuracy: 0.5505 - val_loss: 0.7304 - val_accuracy: 0.4882\n",
      "Epoch 335/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6697 - accuracy: 0.5644 - val_loss: 0.7302 - val_accuracy: 0.4882\n",
      "Epoch 336/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6633 - accuracy: 0.5802 - val_loss: 0.7312 - val_accuracy: 0.4882\n",
      "Epoch 337/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6723 - accuracy: 0.5505 - val_loss: 0.7321 - val_accuracy: 0.4882\n",
      "Epoch 338/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6665 - accuracy: 0.5723 - val_loss: 0.7335 - val_accuracy: 0.4882\n",
      "Epoch 339/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6674 - accuracy: 0.5584 - val_loss: 0.7324 - val_accuracy: 0.4882\n",
      "Epoch 340/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6669 - accuracy: 0.5683 - val_loss: 0.7330 - val_accuracy: 0.4882\n",
      "Epoch 341/2000\n",
      "505/505 [==============================] - 0s 208us/step - loss: 0.6678 - accuracy: 0.5723 - val_loss: 0.7294 - val_accuracy: 0.4882\n",
      "Epoch 342/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6654 - accuracy: 0.5624 - val_loss: 0.7291 - val_accuracy: 0.4882\n",
      "Epoch 343/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6698 - accuracy: 0.5683 - val_loss: 0.7283 - val_accuracy: 0.4882\n",
      "Epoch 344/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6650 - accuracy: 0.5723 - val_loss: 0.7334 - val_accuracy: 0.4882\n",
      "Epoch 345/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6688 - accuracy: 0.5762 - val_loss: 0.7295 - val_accuracy: 0.4882\n",
      "Epoch 346/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6629 - accuracy: 0.5782 - val_loss: 0.7289 - val_accuracy: 0.4882\n",
      "Epoch 347/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6655 - accuracy: 0.5683 - val_loss: 0.7359 - val_accuracy: 0.4882\n",
      "Epoch 348/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6666 - accuracy: 0.5782 - val_loss: 0.7303 - val_accuracy: 0.4882\n",
      "Epoch 349/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6635 - accuracy: 0.5703 - val_loss: 0.7299 - val_accuracy: 0.4882\n",
      "Epoch 350/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6648 - accuracy: 0.5703 - val_loss: 0.7287 - val_accuracy: 0.4882\n",
      "Epoch 351/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6691 - accuracy: 0.5644 - val_loss: 0.7314 - val_accuracy: 0.4882\n",
      "Epoch 352/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6659 - accuracy: 0.5545 - val_loss: 0.7336 - val_accuracy: 0.4882\n",
      "Epoch 353/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6659 - accuracy: 0.5743 - val_loss: 0.7315 - val_accuracy: 0.4882\n",
      "Epoch 354/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6679 - accuracy: 0.5624 - val_loss: 0.7295 - val_accuracy: 0.4882\n",
      "Epoch 355/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6643 - accuracy: 0.5782 - val_loss: 0.7319 - val_accuracy: 0.4882\n",
      "Epoch 356/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6683 - accuracy: 0.5624 - val_loss: 0.7355 - val_accuracy: 0.4882\n",
      "Epoch 357/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6649 - accuracy: 0.5762 - val_loss: 0.7346 - val_accuracy: 0.4882\n",
      "Epoch 358/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6664 - accuracy: 0.5743 - val_loss: 0.7286 - val_accuracy: 0.4882\n",
      "Epoch 359/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6659 - accuracy: 0.5584 - val_loss: 0.7345 - val_accuracy: 0.4882\n",
      "Epoch 360/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6683 - accuracy: 0.5584 - val_loss: 0.7344 - val_accuracy: 0.4882\n",
      "Epoch 361/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6647 - accuracy: 0.5723 - val_loss: 0.7347 - val_accuracy: 0.4882\n",
      "Epoch 362/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6694 - accuracy: 0.5584 - val_loss: 0.7315 - val_accuracy: 0.4882\n",
      "Epoch 363/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6634 - accuracy: 0.5683 - val_loss: 0.7345 - val_accuracy: 0.4882\n",
      "Epoch 364/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6672 - accuracy: 0.5564 - val_loss: 0.7339 - val_accuracy: 0.4882\n",
      "Epoch 365/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6628 - accuracy: 0.5644 - val_loss: 0.7342 - val_accuracy: 0.4882\n",
      "Epoch 366/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6620 - accuracy: 0.5762 - val_loss: 0.7318 - val_accuracy: 0.4882\n",
      "Epoch 367/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6673 - accuracy: 0.5663 - val_loss: 0.7361 - val_accuracy: 0.4882\n",
      "Epoch 368/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6640 - accuracy: 0.5842 - val_loss: 0.7318 - val_accuracy: 0.4882\n",
      "Epoch 369/2000\n",
      "505/505 [==============================] - 0s 231us/step - loss: 0.6649 - accuracy: 0.5762 - val_loss: 0.7366 - val_accuracy: 0.4882\n",
      "Epoch 370/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6670 - accuracy: 0.5683 - val_loss: 0.7355 - val_accuracy: 0.4882\n",
      "Epoch 371/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6656 - accuracy: 0.5663 - val_loss: 0.7344 - val_accuracy: 0.4882\n",
      "Epoch 372/2000\n",
      "505/505 [==============================] - 0s 237us/step - loss: 0.6683 - accuracy: 0.5663 - val_loss: 0.7347 - val_accuracy: 0.4882\n",
      "Epoch 373/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6641 - accuracy: 0.5703 - val_loss: 0.7372 - val_accuracy: 0.4882\n",
      "Epoch 374/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6680 - accuracy: 0.5802 - val_loss: 0.7358 - val_accuracy: 0.4882\n",
      "Epoch 375/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6668 - accuracy: 0.5683 - val_loss: 0.7379 - val_accuracy: 0.4882\n",
      "Epoch 376/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6675 - accuracy: 0.5663 - val_loss: 0.7384 - val_accuracy: 0.4961\n",
      "Epoch 377/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6668 - accuracy: 0.5604 - val_loss: 0.7316 - val_accuracy: 0.4882\n",
      "Epoch 378/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6655 - accuracy: 0.5723 - val_loss: 0.7342 - val_accuracy: 0.4882\n",
      "Epoch 379/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6583 - accuracy: 0.5762 - val_loss: 0.7299 - val_accuracy: 0.4882\n",
      "Epoch 380/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6648 - accuracy: 0.5762 - val_loss: 0.7295 - val_accuracy: 0.4882\n",
      "Epoch 381/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6633 - accuracy: 0.5683 - val_loss: 0.7354 - val_accuracy: 0.4882\n",
      "Epoch 382/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6627 - accuracy: 0.5723 - val_loss: 0.7336 - val_accuracy: 0.4882\n",
      "Epoch 383/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6610 - accuracy: 0.5663 - val_loss: 0.7339 - val_accuracy: 0.4882\n",
      "Epoch 384/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6691 - accuracy: 0.5604 - val_loss: 0.7317 - val_accuracy: 0.4882\n",
      "Epoch 385/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6653 - accuracy: 0.5743 - val_loss: 0.7373 - val_accuracy: 0.4882\n",
      "Epoch 386/2000\n",
      "505/505 [==============================] - 0s 295us/step - loss: 0.6632 - accuracy: 0.5703 - val_loss: 0.7360 - val_accuracy: 0.4882\n",
      "Epoch 387/2000\n",
      "505/505 [==============================] - 0s 246us/step - loss: 0.6656 - accuracy: 0.5624 - val_loss: 0.7412 - val_accuracy: 0.5118\n",
      "Epoch 388/2000\n",
      "505/505 [==============================] - 0s 239us/step - loss: 0.6653 - accuracy: 0.5723 - val_loss: 0.7427 - val_accuracy: 0.5197\n",
      "Epoch 389/2000\n",
      "505/505 [==============================] - 0s 247us/step - loss: 0.6657 - accuracy: 0.5525 - val_loss: 0.7347 - val_accuracy: 0.4882\n",
      "Epoch 390/2000\n",
      "505/505 [==============================] - 0s 237us/step - loss: 0.6653 - accuracy: 0.5802 - val_loss: 0.7314 - val_accuracy: 0.4882\n",
      "Epoch 391/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6689 - accuracy: 0.5683 - val_loss: 0.7290 - val_accuracy: 0.4882\n",
      "Epoch 392/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6632 - accuracy: 0.5723 - val_loss: 0.7302 - val_accuracy: 0.4882\n",
      "Epoch 393/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6616 - accuracy: 0.5604 - val_loss: 0.7296 - val_accuracy: 0.4882\n",
      "Epoch 394/2000\n",
      "505/505 [==============================] - 0s 224us/step - loss: 0.6597 - accuracy: 0.5723 - val_loss: 0.7330 - val_accuracy: 0.4882\n",
      "Epoch 395/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6647 - accuracy: 0.5644 - val_loss: 0.7337 - val_accuracy: 0.4882\n",
      "Epoch 396/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6672 - accuracy: 0.5663 - val_loss: 0.7356 - val_accuracy: 0.4882\n",
      "Epoch 397/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6681 - accuracy: 0.5743 - val_loss: 0.7362 - val_accuracy: 0.4882\n",
      "Epoch 398/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6687 - accuracy: 0.5465 - val_loss: 0.7317 - val_accuracy: 0.4882\n",
      "Epoch 399/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6623 - accuracy: 0.5743 - val_loss: 0.7315 - val_accuracy: 0.4882\n",
      "Epoch 400/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6631 - accuracy: 0.5683 - val_loss: 0.7350 - val_accuracy: 0.4882\n",
      "Epoch 401/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6636 - accuracy: 0.5802 - val_loss: 0.7358 - val_accuracy: 0.4882\n",
      "Epoch 402/2000\n",
      "505/505 [==============================] - 0s 249us/step - loss: 0.6630 - accuracy: 0.5782 - val_loss: 0.7349 - val_accuracy: 0.4882\n",
      "Epoch 403/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6669 - accuracy: 0.5604 - val_loss: 0.7308 - val_accuracy: 0.4882\n",
      "Epoch 404/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6668 - accuracy: 0.5683 - val_loss: 0.7324 - val_accuracy: 0.4882\n",
      "Epoch 405/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6643 - accuracy: 0.5743 - val_loss: 0.7319 - val_accuracy: 0.4882\n",
      "Epoch 406/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6612 - accuracy: 0.5743 - val_loss: 0.7344 - val_accuracy: 0.4882\n",
      "Epoch 407/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6642 - accuracy: 0.5683 - val_loss: 0.7282 - val_accuracy: 0.4882\n",
      "Epoch 408/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6622 - accuracy: 0.5802 - val_loss: 0.7339 - val_accuracy: 0.4882\n",
      "Epoch 409/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6670 - accuracy: 0.5525 - val_loss: 0.7323 - val_accuracy: 0.4882\n",
      "Epoch 410/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6650 - accuracy: 0.5703 - val_loss: 0.7338 - val_accuracy: 0.4882\n",
      "Epoch 411/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6659 - accuracy: 0.5683 - val_loss: 0.7323 - val_accuracy: 0.4882\n",
      "Epoch 412/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6626 - accuracy: 0.5683 - val_loss: 0.7294 - val_accuracy: 0.4882\n",
      "Epoch 413/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6643 - accuracy: 0.5465 - val_loss: 0.7332 - val_accuracy: 0.4882\n",
      "Epoch 414/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6638 - accuracy: 0.5723 - val_loss: 0.7307 - val_accuracy: 0.4882\n",
      "Epoch 415/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6642 - accuracy: 0.5683 - val_loss: 0.7312 - val_accuracy: 0.4882\n",
      "Epoch 416/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6664 - accuracy: 0.5584 - val_loss: 0.7352 - val_accuracy: 0.4961\n",
      "Epoch 417/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6615 - accuracy: 0.5545 - val_loss: 0.7343 - val_accuracy: 0.4961\n",
      "Epoch 418/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6640 - accuracy: 0.5762 - val_loss: 0.7361 - val_accuracy: 0.4961\n",
      "Epoch 419/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6600 - accuracy: 0.5842 - val_loss: 0.7357 - val_accuracy: 0.4961\n",
      "Epoch 420/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6632 - accuracy: 0.5683 - val_loss: 0.7297 - val_accuracy: 0.4882\n",
      "Epoch 421/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6568 - accuracy: 0.5861 - val_loss: 0.7279 - val_accuracy: 0.4882\n",
      "Epoch 422/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6645 - accuracy: 0.5762 - val_loss: 0.7327 - val_accuracy: 0.4882\n",
      "Epoch 423/2000\n",
      "505/505 [==============================] - 0s 226us/step - loss: 0.6624 - accuracy: 0.5723 - val_loss: 0.7327 - val_accuracy: 0.4882\n",
      "Epoch 424/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6632 - accuracy: 0.5743 - val_loss: 0.7349 - val_accuracy: 0.4882\n",
      "Epoch 425/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6611 - accuracy: 0.5683 - val_loss: 0.7322 - val_accuracy: 0.4882\n",
      "Epoch 426/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6639 - accuracy: 0.5782 - val_loss: 0.7339 - val_accuracy: 0.4882\n",
      "Epoch 427/2000\n",
      "505/505 [==============================] - 0s 239us/step - loss: 0.6632 - accuracy: 0.5663 - val_loss: 0.7339 - val_accuracy: 0.4882\n",
      "Epoch 428/2000\n",
      "505/505 [==============================] - 0s 237us/step - loss: 0.6602 - accuracy: 0.5663 - val_loss: 0.7343 - val_accuracy: 0.4882\n",
      "Epoch 429/2000\n",
      "505/505 [==============================] - 0s 233us/step - loss: 0.6636 - accuracy: 0.5802 - val_loss: 0.7327 - val_accuracy: 0.4882\n",
      "Epoch 430/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6642 - accuracy: 0.5644 - val_loss: 0.7302 - val_accuracy: 0.4882\n",
      "Epoch 431/2000\n",
      "505/505 [==============================] - 0s 233us/step - loss: 0.6616 - accuracy: 0.5703 - val_loss: 0.7243 - val_accuracy: 0.5197\n",
      "Epoch 432/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6637 - accuracy: 0.5782 - val_loss: 0.7355 - val_accuracy: 0.4882\n",
      "Epoch 433/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6678 - accuracy: 0.5545 - val_loss: 0.7395 - val_accuracy: 0.4961\n",
      "Epoch 434/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.6626 - accuracy: 0.5723 - val_loss: 0.7362 - val_accuracy: 0.4882\n",
      "Epoch 435/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6593 - accuracy: 0.5743 - val_loss: 0.7287 - val_accuracy: 0.4882\n",
      "Epoch 436/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6606 - accuracy: 0.5861 - val_loss: 0.7319 - val_accuracy: 0.4882\n",
      "Epoch 437/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6584 - accuracy: 0.5921 - val_loss: 0.7372 - val_accuracy: 0.4961\n",
      "Epoch 438/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6596 - accuracy: 0.5644 - val_loss: 0.7360 - val_accuracy: 0.4961\n",
      "Epoch 439/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6632 - accuracy: 0.5644 - val_loss: 0.7336 - val_accuracy: 0.4961\n",
      "Epoch 440/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6597 - accuracy: 0.5762 - val_loss: 0.7347 - val_accuracy: 0.4961\n",
      "Epoch 441/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6651 - accuracy: 0.5683 - val_loss: 0.7353 - val_accuracy: 0.4961\n",
      "Epoch 442/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 221us/step - loss: 0.6613 - accuracy: 0.5525 - val_loss: 0.7338 - val_accuracy: 0.4882\n",
      "Epoch 443/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6645 - accuracy: 0.5584 - val_loss: 0.7356 - val_accuracy: 0.4961\n",
      "Epoch 444/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6610 - accuracy: 0.5861 - val_loss: 0.7308 - val_accuracy: 0.4882\n",
      "Epoch 445/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6609 - accuracy: 0.5663 - val_loss: 0.7300 - val_accuracy: 0.4882\n",
      "Epoch 446/2000\n",
      "505/505 [==============================] - 0s 224us/step - loss: 0.6639 - accuracy: 0.5743 - val_loss: 0.7339 - val_accuracy: 0.4961\n",
      "Epoch 447/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6628 - accuracy: 0.5683 - val_loss: 0.7344 - val_accuracy: 0.4961\n",
      "Epoch 448/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6564 - accuracy: 0.5881 - val_loss: 0.7395 - val_accuracy: 0.4961\n",
      "Epoch 449/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6597 - accuracy: 0.5564 - val_loss: 0.7386 - val_accuracy: 0.4961\n",
      "Epoch 450/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6561 - accuracy: 0.5743 - val_loss: 0.7341 - val_accuracy: 0.4961\n",
      "Epoch 451/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6626 - accuracy: 0.5683 - val_loss: 0.7315 - val_accuracy: 0.4882\n",
      "Epoch 452/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6598 - accuracy: 0.5822 - val_loss: 0.7311 - val_accuracy: 0.4882\n",
      "Epoch 453/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6644 - accuracy: 0.5743 - val_loss: 0.7304 - val_accuracy: 0.4882\n",
      "Epoch 454/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6628 - accuracy: 0.5663 - val_loss: 0.7240 - val_accuracy: 0.4882\n",
      "Epoch 455/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6607 - accuracy: 0.5703 - val_loss: 0.7341 - val_accuracy: 0.4961\n",
      "Epoch 456/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6643 - accuracy: 0.5703 - val_loss: 0.7324 - val_accuracy: 0.4961\n",
      "Epoch 457/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6573 - accuracy: 0.5743 - val_loss: 0.7335 - val_accuracy: 0.4961\n",
      "Epoch 458/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6640 - accuracy: 0.5822 - val_loss: 0.7324 - val_accuracy: 0.4882\n",
      "Epoch 459/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6671 - accuracy: 0.5604 - val_loss: 0.7369 - val_accuracy: 0.4961\n",
      "Epoch 460/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6623 - accuracy: 0.5723 - val_loss: 0.7320 - val_accuracy: 0.4882\n",
      "Epoch 461/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6610 - accuracy: 0.5743 - val_loss: 0.7300 - val_accuracy: 0.4882\n",
      "Epoch 462/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6619 - accuracy: 0.5663 - val_loss: 0.7344 - val_accuracy: 0.4961\n",
      "Epoch 463/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6581 - accuracy: 0.5762 - val_loss: 0.7356 - val_accuracy: 0.4961\n",
      "Epoch 464/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6646 - accuracy: 0.5723 - val_loss: 0.7340 - val_accuracy: 0.4961\n",
      "Epoch 465/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6591 - accuracy: 0.5842 - val_loss: 0.7388 - val_accuracy: 0.4961\n",
      "Epoch 466/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6583 - accuracy: 0.5644 - val_loss: 0.7347 - val_accuracy: 0.4961\n",
      "Epoch 467/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6643 - accuracy: 0.5545 - val_loss: 0.7312 - val_accuracy: 0.4882\n",
      "Epoch 468/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6634 - accuracy: 0.5644 - val_loss: 0.7319 - val_accuracy: 0.4882\n",
      "Epoch 469/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6600 - accuracy: 0.5762 - val_loss: 0.7377 - val_accuracy: 0.4961\n",
      "Epoch 470/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6596 - accuracy: 0.5782 - val_loss: 0.7324 - val_accuracy: 0.4882\n",
      "Epoch 471/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6637 - accuracy: 0.5683 - val_loss: 0.7271 - val_accuracy: 0.4882\n",
      "Epoch 472/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.6594 - accuracy: 0.5723 - val_loss: 0.7283 - val_accuracy: 0.4882\n",
      "Epoch 473/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6621 - accuracy: 0.5683 - val_loss: 0.7286 - val_accuracy: 0.4882\n",
      "Epoch 474/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6586 - accuracy: 0.5644 - val_loss: 0.7276 - val_accuracy: 0.4882\n",
      "Epoch 475/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6571 - accuracy: 0.5743 - val_loss: 0.7269 - val_accuracy: 0.4882\n",
      "Epoch 476/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6635 - accuracy: 0.5604 - val_loss: 0.7301 - val_accuracy: 0.4882\n",
      "Epoch 477/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6583 - accuracy: 0.5782 - val_loss: 0.7350 - val_accuracy: 0.4961\n",
      "Epoch 478/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6620 - accuracy: 0.5802 - val_loss: 0.7364 - val_accuracy: 0.4961\n",
      "Epoch 479/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6630 - accuracy: 0.5663 - val_loss: 0.7341 - val_accuracy: 0.4961\n",
      "Epoch 480/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6612 - accuracy: 0.5663 - val_loss: 0.7296 - val_accuracy: 0.4882\n",
      "Epoch 481/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6601 - accuracy: 0.5703 - val_loss: 0.7303 - val_accuracy: 0.4961\n",
      "Epoch 482/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6592 - accuracy: 0.5743 - val_loss: 0.7296 - val_accuracy: 0.4882\n",
      "Epoch 483/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6612 - accuracy: 0.5743 - val_loss: 0.7321 - val_accuracy: 0.4961\n",
      "Epoch 484/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6594 - accuracy: 0.5921 - val_loss: 0.7281 - val_accuracy: 0.4882\n",
      "Epoch 485/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6586 - accuracy: 0.5723 - val_loss: 0.7312 - val_accuracy: 0.4961\n",
      "Epoch 486/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6580 - accuracy: 0.5723 - val_loss: 0.7318 - val_accuracy: 0.4961\n",
      "Epoch 487/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6616 - accuracy: 0.5564 - val_loss: 0.7342 - val_accuracy: 0.4961\n",
      "Epoch 488/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6602 - accuracy: 0.5723 - val_loss: 0.7318 - val_accuracy: 0.4961\n",
      "Epoch 489/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6611 - accuracy: 0.5762 - val_loss: 0.7251 - val_accuracy: 0.4882\n",
      "Epoch 490/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6609 - accuracy: 0.5683 - val_loss: 0.7275 - val_accuracy: 0.4882\n",
      "Epoch 491/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6590 - accuracy: 0.5802 - val_loss: 0.7315 - val_accuracy: 0.4961\n",
      "Epoch 492/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6574 - accuracy: 0.5762 - val_loss: 0.7360 - val_accuracy: 0.5039\n",
      "Epoch 493/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6605 - accuracy: 0.5525 - val_loss: 0.7279 - val_accuracy: 0.4882\n",
      "Epoch 494/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6569 - accuracy: 0.5762 - val_loss: 0.7268 - val_accuracy: 0.4882\n",
      "Epoch 495/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6587 - accuracy: 0.5545 - val_loss: 0.7264 - val_accuracy: 0.4882\n",
      "Epoch 496/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6576 - accuracy: 0.5762 - val_loss: 0.7240 - val_accuracy: 0.4882\n",
      "Epoch 497/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6600 - accuracy: 0.5782 - val_loss: 0.7309 - val_accuracy: 0.4882\n",
      "Epoch 498/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6597 - accuracy: 0.5663 - val_loss: 0.7369 - val_accuracy: 0.4961\n",
      "Epoch 499/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6577 - accuracy: 0.5743 - val_loss: 0.7351 - val_accuracy: 0.4961\n",
      "Epoch 500/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6576 - accuracy: 0.5762 - val_loss: 0.7328 - val_accuracy: 0.4882\n",
      "Epoch 501/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6615 - accuracy: 0.5663 - val_loss: 0.7308 - val_accuracy: 0.4882\n",
      "Epoch 502/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6610 - accuracy: 0.5663 - val_loss: 0.7303 - val_accuracy: 0.4882\n",
      "Epoch 503/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6553 - accuracy: 0.5743 - val_loss: 0.7297 - val_accuracy: 0.4882\n",
      "Epoch 504/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6607 - accuracy: 0.5703 - val_loss: 0.7351 - val_accuracy: 0.4961\n",
      "Epoch 505/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6568 - accuracy: 0.5723 - val_loss: 0.7350 - val_accuracy: 0.4961\n",
      "Epoch 506/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6628 - accuracy: 0.5683 - val_loss: 0.7320 - val_accuracy: 0.4882\n",
      "Epoch 507/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6612 - accuracy: 0.5624 - val_loss: 0.7289 - val_accuracy: 0.4882\n",
      "Epoch 508/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6560 - accuracy: 0.5921 - val_loss: 0.7265 - val_accuracy: 0.4882\n",
      "Epoch 509/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6580 - accuracy: 0.5723 - val_loss: 0.7394 - val_accuracy: 0.5039\n",
      "Epoch 510/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6611 - accuracy: 0.5624 - val_loss: 0.7366 - val_accuracy: 0.4961\n",
      "Epoch 511/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6569 - accuracy: 0.5861 - val_loss: 0.7340 - val_accuracy: 0.4882\n",
      "Epoch 512/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6556 - accuracy: 0.5842 - val_loss: 0.7295 - val_accuracy: 0.4882\n",
      "Epoch 513/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6601 - accuracy: 0.5545 - val_loss: 0.7282 - val_accuracy: 0.4882\n",
      "Epoch 514/2000\n",
      "505/505 [==============================] - 0s 208us/step - loss: 0.6593 - accuracy: 0.5644 - val_loss: 0.7344 - val_accuracy: 0.4882\n",
      "Epoch 515/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6569 - accuracy: 0.5683 - val_loss: 0.7307 - val_accuracy: 0.4882\n",
      "Epoch 516/2000\n",
      "505/505 [==============================] - 0s 205us/step - loss: 0.6585 - accuracy: 0.5802 - val_loss: 0.7277 - val_accuracy: 0.4882\n",
      "Epoch 517/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6574 - accuracy: 0.5762 - val_loss: 0.7306 - val_accuracy: 0.4882\n",
      "Epoch 518/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6544 - accuracy: 0.5782 - val_loss: 0.7283 - val_accuracy: 0.4882\n",
      "Epoch 519/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6616 - accuracy: 0.5604 - val_loss: 0.7294 - val_accuracy: 0.4882\n",
      "Epoch 520/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6601 - accuracy: 0.5663 - val_loss: 0.7259 - val_accuracy: 0.4882\n",
      "Epoch 521/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6574 - accuracy: 0.5743 - val_loss: 0.7251 - val_accuracy: 0.4882\n",
      "Epoch 522/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6602 - accuracy: 0.5762 - val_loss: 0.7289 - val_accuracy: 0.4882\n",
      "Epoch 523/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6573 - accuracy: 0.5663 - val_loss: 0.7299 - val_accuracy: 0.4961\n",
      "Epoch 524/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6568 - accuracy: 0.5624 - val_loss: 0.7287 - val_accuracy: 0.4882\n",
      "Epoch 525/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6552 - accuracy: 0.5802 - val_loss: 0.7312 - val_accuracy: 0.4882\n",
      "Epoch 526/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6552 - accuracy: 0.5822 - val_loss: 0.7321 - val_accuracy: 0.4882\n",
      "Epoch 527/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6593 - accuracy: 0.5663 - val_loss: 0.7337 - val_accuracy: 0.4961\n",
      "Epoch 528/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6551 - accuracy: 0.5762 - val_loss: 0.7365 - val_accuracy: 0.4961\n",
      "Epoch 529/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6544 - accuracy: 0.5762 - val_loss: 0.7329 - val_accuracy: 0.4882\n",
      "Epoch 530/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6577 - accuracy: 0.5861 - val_loss: 0.7271 - val_accuracy: 0.4882\n",
      "Epoch 531/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6586 - accuracy: 0.5802 - val_loss: 0.7317 - val_accuracy: 0.4882\n",
      "Epoch 532/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6592 - accuracy: 0.5822 - val_loss: 0.7261 - val_accuracy: 0.4882\n",
      "Epoch 533/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6619 - accuracy: 0.5644 - val_loss: 0.7253 - val_accuracy: 0.4882\n",
      "Epoch 534/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6556 - accuracy: 0.5921 - val_loss: 0.7333 - val_accuracy: 0.4882\n",
      "Epoch 535/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6586 - accuracy: 0.5624 - val_loss: 0.7277 - val_accuracy: 0.4882\n",
      "Epoch 536/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6536 - accuracy: 0.5881 - val_loss: 0.7302 - val_accuracy: 0.4882\n",
      "Epoch 537/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6568 - accuracy: 0.5723 - val_loss: 0.7254 - val_accuracy: 0.4882\n",
      "Epoch 538/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6573 - accuracy: 0.5703 - val_loss: 0.7271 - val_accuracy: 0.4882\n",
      "Epoch 539/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6575 - accuracy: 0.5802 - val_loss: 0.7273 - val_accuracy: 0.4882\n",
      "Epoch 540/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6576 - accuracy: 0.5683 - val_loss: 0.7225 - val_accuracy: 0.5118\n",
      "Epoch 541/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6527 - accuracy: 0.5782 - val_loss: 0.7277 - val_accuracy: 0.4882\n",
      "Epoch 542/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6571 - accuracy: 0.5644 - val_loss: 0.7301 - val_accuracy: 0.4882\n",
      "Epoch 543/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6551 - accuracy: 0.5624 - val_loss: 0.7287 - val_accuracy: 0.4882\n",
      "Epoch 544/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6546 - accuracy: 0.5703 - val_loss: 0.7262 - val_accuracy: 0.4882\n",
      "Epoch 545/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6578 - accuracy: 0.5703 - val_loss: 0.7335 - val_accuracy: 0.4882\n",
      "Epoch 546/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6537 - accuracy: 0.5822 - val_loss: 0.7314 - val_accuracy: 0.4882\n",
      "Epoch 547/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6498 - accuracy: 0.5683 - val_loss: 0.7322 - val_accuracy: 0.4882\n",
      "Epoch 548/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6599 - accuracy: 0.5743 - val_loss: 0.7289 - val_accuracy: 0.4882\n",
      "Epoch 549/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6518 - accuracy: 0.5901 - val_loss: 0.7279 - val_accuracy: 0.4882\n",
      "Epoch 550/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6553 - accuracy: 0.5584 - val_loss: 0.7273 - val_accuracy: 0.4882\n",
      "Epoch 551/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6558 - accuracy: 0.5644 - val_loss: 0.7267 - val_accuracy: 0.4882\n",
      "Epoch 552/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 209us/step - loss: 0.6513 - accuracy: 0.5901 - val_loss: 0.7304 - val_accuracy: 0.4882\n",
      "Epoch 553/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6578 - accuracy: 0.5703 - val_loss: 0.7299 - val_accuracy: 0.4882\n",
      "Epoch 554/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6626 - accuracy: 0.5644 - val_loss: 0.7347 - val_accuracy: 0.4961\n",
      "Epoch 555/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6534 - accuracy: 0.5941 - val_loss: 0.7294 - val_accuracy: 0.4882\n",
      "Epoch 556/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6567 - accuracy: 0.5644 - val_loss: 0.7283 - val_accuracy: 0.4882\n",
      "Epoch 557/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6588 - accuracy: 0.5644 - val_loss: 0.7301 - val_accuracy: 0.4882\n",
      "Epoch 558/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6557 - accuracy: 0.5703 - val_loss: 0.7347 - val_accuracy: 0.4961\n",
      "Epoch 559/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6610 - accuracy: 0.5663 - val_loss: 0.7296 - val_accuracy: 0.4882\n",
      "Epoch 560/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6606 - accuracy: 0.5703 - val_loss: 0.7279 - val_accuracy: 0.4882\n",
      "Epoch 561/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6558 - accuracy: 0.5802 - val_loss: 0.7259 - val_accuracy: 0.4882\n",
      "Epoch 562/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6543 - accuracy: 0.5743 - val_loss: 0.7239 - val_accuracy: 0.4882\n",
      "Epoch 563/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6576 - accuracy: 0.5723 - val_loss: 0.7356 - val_accuracy: 0.4961\n",
      "Epoch 564/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6599 - accuracy: 0.5723 - val_loss: 0.7291 - val_accuracy: 0.4882\n",
      "Epoch 565/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6539 - accuracy: 0.5842 - val_loss: 0.7284 - val_accuracy: 0.4882\n",
      "Epoch 566/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6571 - accuracy: 0.5842 - val_loss: 0.7250 - val_accuracy: 0.4882\n",
      "Epoch 567/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6543 - accuracy: 0.5743 - val_loss: 0.7326 - val_accuracy: 0.4961\n",
      "Epoch 568/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6574 - accuracy: 0.5723 - val_loss: 0.7225 - val_accuracy: 0.5197\n",
      "Epoch 569/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6553 - accuracy: 0.5624 - val_loss: 0.7257 - val_accuracy: 0.4882\n",
      "Epoch 570/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6540 - accuracy: 0.5822 - val_loss: 0.7284 - val_accuracy: 0.4882\n",
      "Epoch 571/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6559 - accuracy: 0.5683 - val_loss: 0.7306 - val_accuracy: 0.4961\n",
      "Epoch 572/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6591 - accuracy: 0.5663 - val_loss: 0.7269 - val_accuracy: 0.4882\n",
      "Epoch 573/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6529 - accuracy: 0.5564 - val_loss: 0.7322 - val_accuracy: 0.4961\n",
      "Epoch 574/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6539 - accuracy: 0.5663 - val_loss: 0.7306 - val_accuracy: 0.4961\n",
      "Epoch 575/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6551 - accuracy: 0.5861 - val_loss: 0.7254 - val_accuracy: 0.4882\n",
      "Epoch 576/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6549 - accuracy: 0.5624 - val_loss: 0.7255 - val_accuracy: 0.4882\n",
      "Epoch 577/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6507 - accuracy: 0.5842 - val_loss: 0.7255 - val_accuracy: 0.4961\n",
      "Epoch 578/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6499 - accuracy: 0.5960 - val_loss: 0.7251 - val_accuracy: 0.4961\n",
      "Epoch 579/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6540 - accuracy: 0.5802 - val_loss: 0.7239 - val_accuracy: 0.4961\n",
      "Epoch 580/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6587 - accuracy: 0.5901 - val_loss: 0.7241 - val_accuracy: 0.4961\n",
      "Epoch 581/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6492 - accuracy: 0.5743 - val_loss: 0.7352 - val_accuracy: 0.4961\n",
      "Epoch 582/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6519 - accuracy: 0.5802 - val_loss: 0.7333 - val_accuracy: 0.4961\n",
      "Epoch 583/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6564 - accuracy: 0.5921 - val_loss: 0.7277 - val_accuracy: 0.4961\n",
      "Epoch 584/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.6535 - accuracy: 0.5921 - val_loss: 0.7211 - val_accuracy: 0.4882\n",
      "Epoch 585/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6537 - accuracy: 0.5663 - val_loss: 0.7291 - val_accuracy: 0.4961\n",
      "Epoch 586/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.6568 - accuracy: 0.5743 - val_loss: 0.7275 - val_accuracy: 0.4961\n",
      "Epoch 587/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6567 - accuracy: 0.5663 - val_loss: 0.7298 - val_accuracy: 0.4961\n",
      "Epoch 588/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6538 - accuracy: 0.5782 - val_loss: 0.7310 - val_accuracy: 0.4961\n",
      "Epoch 589/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6511 - accuracy: 0.5683 - val_loss: 0.7262 - val_accuracy: 0.4882\n",
      "Epoch 590/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6539 - accuracy: 0.5782 - val_loss: 0.7267 - val_accuracy: 0.4961\n",
      "Epoch 591/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6531 - accuracy: 0.5822 - val_loss: 0.7348 - val_accuracy: 0.4961\n",
      "Epoch 592/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6487 - accuracy: 0.5782 - val_loss: 0.7313 - val_accuracy: 0.4961\n",
      "Epoch 593/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6486 - accuracy: 0.6079 - val_loss: 0.7263 - val_accuracy: 0.4961\n",
      "Epoch 594/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6517 - accuracy: 0.5822 - val_loss: 0.7249 - val_accuracy: 0.4961\n",
      "Epoch 595/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6499 - accuracy: 0.5921 - val_loss: 0.7273 - val_accuracy: 0.4961\n",
      "Epoch 596/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6487 - accuracy: 0.5861 - val_loss: 0.7334 - val_accuracy: 0.4961\n",
      "Epoch 597/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6558 - accuracy: 0.5703 - val_loss: 0.7199 - val_accuracy: 0.5197\n",
      "Epoch 598/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6564 - accuracy: 0.5762 - val_loss: 0.7224 - val_accuracy: 0.4961\n",
      "Epoch 599/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6517 - accuracy: 0.5782 - val_loss: 0.7255 - val_accuracy: 0.4961\n",
      "Epoch 600/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6491 - accuracy: 0.5901 - val_loss: 0.7296 - val_accuracy: 0.4961\n",
      "Epoch 601/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6491 - accuracy: 0.5980 - val_loss: 0.7308 - val_accuracy: 0.4961\n",
      "Epoch 602/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6529 - accuracy: 0.5802 - val_loss: 0.7312 - val_accuracy: 0.4961\n",
      "Epoch 603/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6490 - accuracy: 0.5802 - val_loss: 0.7328 - val_accuracy: 0.4961\n",
      "Epoch 604/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6534 - accuracy: 0.5842 - val_loss: 0.7400 - val_accuracy: 0.5039\n",
      "Epoch 605/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6504 - accuracy: 0.5960 - val_loss: 0.7311 - val_accuracy: 0.4961\n",
      "Epoch 606/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6527 - accuracy: 0.5782 - val_loss: 0.7266 - val_accuracy: 0.4961\n",
      "Epoch 607/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6511 - accuracy: 0.5703 - val_loss: 0.7256 - val_accuracy: 0.4961\n",
      "Epoch 608/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6521 - accuracy: 0.5782 - val_loss: 0.7292 - val_accuracy: 0.4961\n",
      "Epoch 609/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6499 - accuracy: 0.6000 - val_loss: 0.7303 - val_accuracy: 0.4961\n",
      "Epoch 610/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6515 - accuracy: 0.5782 - val_loss: 0.7190 - val_accuracy: 0.5354\n",
      "Epoch 611/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6514 - accuracy: 0.5842 - val_loss: 0.7280 - val_accuracy: 0.4961\n",
      "Epoch 612/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6574 - accuracy: 0.5782 - val_loss: 0.7232 - val_accuracy: 0.4961\n",
      "Epoch 613/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6534 - accuracy: 0.5822 - val_loss: 0.7241 - val_accuracy: 0.4961\n",
      "Epoch 614/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6490 - accuracy: 0.5802 - val_loss: 0.7276 - val_accuracy: 0.4961\n",
      "Epoch 615/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6567 - accuracy: 0.5762 - val_loss: 0.7269 - val_accuracy: 0.4961\n",
      "Epoch 616/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6485 - accuracy: 0.5901 - val_loss: 0.7206 - val_accuracy: 0.4882\n",
      "Epoch 617/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6499 - accuracy: 0.5782 - val_loss: 0.7322 - val_accuracy: 0.4961\n",
      "Epoch 618/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6519 - accuracy: 0.5743 - val_loss: 0.7299 - val_accuracy: 0.4961\n",
      "Epoch 619/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6566 - accuracy: 0.5723 - val_loss: 0.7274 - val_accuracy: 0.4961\n",
      "Epoch 620/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6523 - accuracy: 0.5921 - val_loss: 0.7313 - val_accuracy: 0.4961\n",
      "Epoch 621/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6514 - accuracy: 0.5901 - val_loss: 0.7252 - val_accuracy: 0.4961\n",
      "Epoch 622/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6518 - accuracy: 0.5743 - val_loss: 0.7233 - val_accuracy: 0.4961\n",
      "Epoch 623/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6545 - accuracy: 0.5822 - val_loss: 0.7288 - val_accuracy: 0.4961\n",
      "Epoch 624/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6489 - accuracy: 0.5842 - val_loss: 0.7301 - val_accuracy: 0.4961\n",
      "Epoch 625/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6473 - accuracy: 0.5921 - val_loss: 0.7270 - val_accuracy: 0.4961\n",
      "Epoch 626/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6495 - accuracy: 0.5861 - val_loss: 0.7350 - val_accuracy: 0.4961\n",
      "Epoch 627/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6477 - accuracy: 0.5921 - val_loss: 0.7269 - val_accuracy: 0.4961\n",
      "Epoch 628/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6473 - accuracy: 0.6000 - val_loss: 0.7302 - val_accuracy: 0.4961\n",
      "Epoch 629/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6512 - accuracy: 0.5881 - val_loss: 0.7299 - val_accuracy: 0.4961\n",
      "Epoch 630/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6519 - accuracy: 0.5822 - val_loss: 0.7224 - val_accuracy: 0.5039\n",
      "Epoch 631/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6483 - accuracy: 0.6059 - val_loss: 0.7250 - val_accuracy: 0.4961\n",
      "Epoch 632/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6494 - accuracy: 0.5881 - val_loss: 0.7316 - val_accuracy: 0.4961\n",
      "Epoch 633/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6522 - accuracy: 0.5822 - val_loss: 0.7295 - val_accuracy: 0.4961\n",
      "Epoch 634/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6481 - accuracy: 0.5842 - val_loss: 0.7319 - val_accuracy: 0.4961\n",
      "Epoch 635/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6503 - accuracy: 0.5881 - val_loss: 0.7320 - val_accuracy: 0.4961\n",
      "Epoch 636/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6436 - accuracy: 0.6040 - val_loss: 0.7227 - val_accuracy: 0.4961\n",
      "Epoch 637/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6489 - accuracy: 0.5842 - val_loss: 0.7288 - val_accuracy: 0.4961\n",
      "Epoch 638/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6538 - accuracy: 0.5683 - val_loss: 0.7286 - val_accuracy: 0.4961\n",
      "Epoch 639/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6504 - accuracy: 0.5861 - val_loss: 0.7253 - val_accuracy: 0.4961\n",
      "Epoch 640/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6534 - accuracy: 0.5861 - val_loss: 0.7265 - val_accuracy: 0.4961\n",
      "Epoch 641/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6500 - accuracy: 0.5782 - val_loss: 0.7345 - val_accuracy: 0.4961\n",
      "Epoch 642/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6488 - accuracy: 0.6040 - val_loss: 0.7261 - val_accuracy: 0.4961\n",
      "Epoch 643/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6526 - accuracy: 0.5822 - val_loss: 0.7300 - val_accuracy: 0.4961\n",
      "Epoch 644/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6505 - accuracy: 0.5762 - val_loss: 0.7325 - val_accuracy: 0.4961\n",
      "Epoch 645/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6553 - accuracy: 0.5683 - val_loss: 0.7350 - val_accuracy: 0.4961\n",
      "Epoch 646/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6453 - accuracy: 0.5921 - val_loss: 0.7285 - val_accuracy: 0.4961\n",
      "Epoch 647/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6467 - accuracy: 0.5921 - val_loss: 0.7265 - val_accuracy: 0.4961\n",
      "Epoch 648/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6490 - accuracy: 0.6000 - val_loss: 0.7218 - val_accuracy: 0.5039\n",
      "Epoch 649/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6478 - accuracy: 0.5881 - val_loss: 0.7231 - val_accuracy: 0.4882\n",
      "Epoch 650/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6477 - accuracy: 0.5941 - val_loss: 0.7222 - val_accuracy: 0.4882\n",
      "Epoch 651/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6474 - accuracy: 0.6000 - val_loss: 0.7260 - val_accuracy: 0.4961\n",
      "Epoch 652/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6475 - accuracy: 0.5921 - val_loss: 0.7230 - val_accuracy: 0.4961\n",
      "Epoch 653/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6438 - accuracy: 0.6119 - val_loss: 0.7249 - val_accuracy: 0.4961\n",
      "Epoch 654/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6513 - accuracy: 0.5941 - val_loss: 0.7184 - val_accuracy: 0.5354\n",
      "Epoch 655/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6485 - accuracy: 0.5861 - val_loss: 0.7264 - val_accuracy: 0.4961\n",
      "Epoch 656/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6514 - accuracy: 0.5980 - val_loss: 0.7236 - val_accuracy: 0.4961\n",
      "Epoch 657/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.6489 - accuracy: 0.5921 - val_loss: 0.7223 - val_accuracy: 0.4961\n",
      "Epoch 658/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6462 - accuracy: 0.6000 - val_loss: 0.7209 - val_accuracy: 0.4961\n",
      "Epoch 659/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6506 - accuracy: 0.5861 - val_loss: 0.7214 - val_accuracy: 0.4961\n",
      "Epoch 660/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6490 - accuracy: 0.6059 - val_loss: 0.7165 - val_accuracy: 0.5197\n",
      "Epoch 661/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6469 - accuracy: 0.5941 - val_loss: 0.7190 - val_accuracy: 0.4961\n",
      "Epoch 662/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 211us/step - loss: 0.6484 - accuracy: 0.5960 - val_loss: 0.7234 - val_accuracy: 0.4961\n",
      "Epoch 663/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6501 - accuracy: 0.5901 - val_loss: 0.7266 - val_accuracy: 0.4961\n",
      "Epoch 664/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6532 - accuracy: 0.5941 - val_loss: 0.7227 - val_accuracy: 0.4961\n",
      "Epoch 665/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6512 - accuracy: 0.5762 - val_loss: 0.7252 - val_accuracy: 0.4961\n",
      "Epoch 666/2000\n",
      "505/505 [==============================] - 0s 208us/step - loss: 0.6448 - accuracy: 0.6000 - val_loss: 0.7235 - val_accuracy: 0.4961\n",
      "Epoch 667/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6482 - accuracy: 0.5842 - val_loss: 0.7206 - val_accuracy: 0.4961\n",
      "Epoch 668/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6504 - accuracy: 0.5743 - val_loss: 0.7277 - val_accuracy: 0.4961\n",
      "Epoch 669/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6476 - accuracy: 0.5861 - val_loss: 0.7348 - val_accuracy: 0.5039\n",
      "Epoch 670/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6452 - accuracy: 0.6079 - val_loss: 0.7266 - val_accuracy: 0.4961\n",
      "Epoch 671/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6482 - accuracy: 0.5921 - val_loss: 0.7201 - val_accuracy: 0.4961\n",
      "Epoch 672/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6464 - accuracy: 0.6000 - val_loss: 0.7257 - val_accuracy: 0.4961\n",
      "Epoch 673/2000\n",
      "505/505 [==============================] - 0s 208us/step - loss: 0.6527 - accuracy: 0.5762 - val_loss: 0.7247 - val_accuracy: 0.4961\n",
      "Epoch 674/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6477 - accuracy: 0.6020 - val_loss: 0.7260 - val_accuracy: 0.4961\n",
      "Epoch 675/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6414 - accuracy: 0.6000 - val_loss: 0.7241 - val_accuracy: 0.4961\n",
      "Epoch 676/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6509 - accuracy: 0.5842 - val_loss: 0.7220 - val_accuracy: 0.4961\n",
      "Epoch 677/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6427 - accuracy: 0.6257 - val_loss: 0.7246 - val_accuracy: 0.4961\n",
      "Epoch 678/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6453 - accuracy: 0.5842 - val_loss: 0.7195 - val_accuracy: 0.4961\n",
      "Epoch 679/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6473 - accuracy: 0.6000 - val_loss: 0.7225 - val_accuracy: 0.4961\n",
      "Epoch 680/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6475 - accuracy: 0.6079 - val_loss: 0.7112 - val_accuracy: 0.5197\n",
      "Epoch 681/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6442 - accuracy: 0.5901 - val_loss: 0.7197 - val_accuracy: 0.4961\n",
      "Epoch 682/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6440 - accuracy: 0.6119 - val_loss: 0.7246 - val_accuracy: 0.4961\n",
      "Epoch 683/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6410 - accuracy: 0.6040 - val_loss: 0.7332 - val_accuracy: 0.4961\n",
      "Epoch 684/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6485 - accuracy: 0.6040 - val_loss: 0.7266 - val_accuracy: 0.4961\n",
      "Epoch 685/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6453 - accuracy: 0.5941 - val_loss: 0.7272 - val_accuracy: 0.4961\n",
      "Epoch 686/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6437 - accuracy: 0.6059 - val_loss: 0.7242 - val_accuracy: 0.4961\n",
      "Epoch 687/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6452 - accuracy: 0.6059 - val_loss: 0.7200 - val_accuracy: 0.4961\n",
      "Epoch 688/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6451 - accuracy: 0.5941 - val_loss: 0.7228 - val_accuracy: 0.4961\n",
      "Epoch 689/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6438 - accuracy: 0.6218 - val_loss: 0.7265 - val_accuracy: 0.4961\n",
      "Epoch 690/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6428 - accuracy: 0.6198 - val_loss: 0.7257 - val_accuracy: 0.4961\n",
      "Epoch 691/2000\n",
      "505/505 [==============================] - 0s 231us/step - loss: 0.6447 - accuracy: 0.6119 - val_loss: 0.7258 - val_accuracy: 0.4961\n",
      "Epoch 692/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6472 - accuracy: 0.5941 - val_loss: 0.7266 - val_accuracy: 0.5039\n",
      "Epoch 693/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6417 - accuracy: 0.6099 - val_loss: 0.7285 - val_accuracy: 0.5039\n",
      "Epoch 694/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6406 - accuracy: 0.6317 - val_loss: 0.7252 - val_accuracy: 0.4961\n",
      "Epoch 695/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6405 - accuracy: 0.6198 - val_loss: 0.7197 - val_accuracy: 0.4961\n",
      "Epoch 696/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6453 - accuracy: 0.6040 - val_loss: 0.7273 - val_accuracy: 0.5039\n",
      "Epoch 697/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6468 - accuracy: 0.6020 - val_loss: 0.7231 - val_accuracy: 0.5039\n",
      "Epoch 698/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6428 - accuracy: 0.6000 - val_loss: 0.7300 - val_accuracy: 0.5039\n",
      "Epoch 699/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6465 - accuracy: 0.6139 - val_loss: 0.7161 - val_accuracy: 0.4882\n",
      "Epoch 700/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6447 - accuracy: 0.5980 - val_loss: 0.7104 - val_accuracy: 0.5276\n",
      "Epoch 701/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.6456 - accuracy: 0.6079 - val_loss: 0.7210 - val_accuracy: 0.5039\n",
      "Epoch 702/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6452 - accuracy: 0.6040 - val_loss: 0.7159 - val_accuracy: 0.4961\n",
      "Epoch 703/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6381 - accuracy: 0.6297 - val_loss: 0.7108 - val_accuracy: 0.5354\n",
      "Epoch 704/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6418 - accuracy: 0.6158 - val_loss: 0.7159 - val_accuracy: 0.4961\n",
      "Epoch 705/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6429 - accuracy: 0.6040 - val_loss: 0.7174 - val_accuracy: 0.4961\n",
      "Epoch 706/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6430 - accuracy: 0.5980 - val_loss: 0.7216 - val_accuracy: 0.5039\n",
      "Epoch 707/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6467 - accuracy: 0.5960 - val_loss: 0.7244 - val_accuracy: 0.5039\n",
      "Epoch 708/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6388 - accuracy: 0.6158 - val_loss: 0.7187 - val_accuracy: 0.5039\n",
      "Epoch 709/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6434 - accuracy: 0.6020 - val_loss: 0.7141 - val_accuracy: 0.5118\n",
      "Epoch 710/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6474 - accuracy: 0.6059 - val_loss: 0.7263 - val_accuracy: 0.5039\n",
      "Epoch 711/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6406 - accuracy: 0.6099 - val_loss: 0.7283 - val_accuracy: 0.5039\n",
      "Epoch 712/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6445 - accuracy: 0.6158 - val_loss: 0.7315 - val_accuracy: 0.5039\n",
      "Epoch 713/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6382 - accuracy: 0.6356 - val_loss: 0.7219 - val_accuracy: 0.5039\n",
      "Epoch 714/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6470 - accuracy: 0.6376 - val_loss: 0.7205 - val_accuracy: 0.5039\n",
      "Epoch 715/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6430 - accuracy: 0.6158 - val_loss: 0.7350 - val_accuracy: 0.5197\n",
      "Epoch 716/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6410 - accuracy: 0.6356 - val_loss: 0.7342 - val_accuracy: 0.5118\n",
      "Epoch 717/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6430 - accuracy: 0.6218 - val_loss: 0.7238 - val_accuracy: 0.5039\n",
      "Epoch 718/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6487 - accuracy: 0.6158 - val_loss: 0.7281 - val_accuracy: 0.5039\n",
      "Epoch 719/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6411 - accuracy: 0.6277 - val_loss: 0.7252 - val_accuracy: 0.5039\n",
      "Epoch 720/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6456 - accuracy: 0.6396 - val_loss: 0.7154 - val_accuracy: 0.4961\n",
      "Epoch 721/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6358 - accuracy: 0.6317 - val_loss: 0.7171 - val_accuracy: 0.4961\n",
      "Epoch 722/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6395 - accuracy: 0.6515 - val_loss: 0.7114 - val_accuracy: 0.5276\n",
      "Epoch 723/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6372 - accuracy: 0.6257 - val_loss: 0.7183 - val_accuracy: 0.5039\n",
      "Epoch 724/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6420 - accuracy: 0.6158 - val_loss: 0.7298 - val_accuracy: 0.5039\n",
      "Epoch 725/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6403 - accuracy: 0.6238 - val_loss: 0.7196 - val_accuracy: 0.4961\n",
      "Epoch 726/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6411 - accuracy: 0.6178 - val_loss: 0.7103 - val_accuracy: 0.5354\n",
      "Epoch 727/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6404 - accuracy: 0.6297 - val_loss: 0.7203 - val_accuracy: 0.4961\n",
      "Epoch 728/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6442 - accuracy: 0.6119 - val_loss: 0.7263 - val_accuracy: 0.5039\n",
      "Epoch 729/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6387 - accuracy: 0.6218 - val_loss: 0.7174 - val_accuracy: 0.4961\n",
      "Epoch 730/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6418 - accuracy: 0.6475 - val_loss: 0.7111 - val_accuracy: 0.5197\n",
      "Epoch 731/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6443 - accuracy: 0.6139 - val_loss: 0.7176 - val_accuracy: 0.5039\n",
      "Epoch 732/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6442 - accuracy: 0.5782 - val_loss: 0.7280 - val_accuracy: 0.5118\n",
      "Epoch 733/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6339 - accuracy: 0.6416 - val_loss: 0.7235 - val_accuracy: 0.5118\n",
      "Epoch 734/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6444 - accuracy: 0.6277 - val_loss: 0.7099 - val_accuracy: 0.5433\n",
      "Epoch 735/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6439 - accuracy: 0.6356 - val_loss: 0.7202 - val_accuracy: 0.4961\n",
      "Epoch 736/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6405 - accuracy: 0.6574 - val_loss: 0.7231 - val_accuracy: 0.5039\n",
      "Epoch 737/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6388 - accuracy: 0.6277 - val_loss: 0.7229 - val_accuracy: 0.5039\n",
      "Epoch 738/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6405 - accuracy: 0.6396 - val_loss: 0.7245 - val_accuracy: 0.5039\n",
      "Epoch 739/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6376 - accuracy: 0.6535 - val_loss: 0.7207 - val_accuracy: 0.5039\n",
      "Epoch 740/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.6392 - accuracy: 0.6436 - val_loss: 0.7294 - val_accuracy: 0.5039\n",
      "Epoch 741/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6384 - accuracy: 0.6158 - val_loss: 0.7335 - val_accuracy: 0.5118\n",
      "Epoch 742/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6349 - accuracy: 0.6396 - val_loss: 0.7346 - val_accuracy: 0.5197\n",
      "Epoch 743/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6494 - accuracy: 0.6099 - val_loss: 0.7259 - val_accuracy: 0.5118\n",
      "Epoch 744/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6426 - accuracy: 0.6455 - val_loss: 0.7207 - val_accuracy: 0.5118\n",
      "Epoch 745/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6357 - accuracy: 0.6554 - val_loss: 0.7228 - val_accuracy: 0.5118\n",
      "Epoch 746/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6440 - accuracy: 0.6139 - val_loss: 0.7182 - val_accuracy: 0.5039\n",
      "Epoch 747/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6358 - accuracy: 0.6337 - val_loss: 0.7400 - val_accuracy: 0.5354\n",
      "Epoch 748/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6408 - accuracy: 0.6376 - val_loss: 0.7276 - val_accuracy: 0.5118\n",
      "Epoch 749/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6416 - accuracy: 0.6337 - val_loss: 0.7210 - val_accuracy: 0.5118\n",
      "Epoch 750/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6470 - accuracy: 0.6495 - val_loss: 0.7124 - val_accuracy: 0.5039\n",
      "Epoch 751/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6338 - accuracy: 0.6416 - val_loss: 0.7187 - val_accuracy: 0.5118\n",
      "Epoch 752/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6338 - accuracy: 0.6515 - val_loss: 0.7112 - val_accuracy: 0.5039\n",
      "Epoch 753/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.6376 - accuracy: 0.6436 - val_loss: 0.7223 - val_accuracy: 0.5118\n",
      "Epoch 754/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6368 - accuracy: 0.6554 - val_loss: 0.7158 - val_accuracy: 0.5118\n",
      "Epoch 755/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6389 - accuracy: 0.6653 - val_loss: 0.7262 - val_accuracy: 0.5118\n",
      "Epoch 756/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6337 - accuracy: 0.6475 - val_loss: 0.7271 - val_accuracy: 0.5118\n",
      "Epoch 757/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6369 - accuracy: 0.6436 - val_loss: 0.7209 - val_accuracy: 0.5118\n",
      "Epoch 758/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6342 - accuracy: 0.6515 - val_loss: 0.7171 - val_accuracy: 0.5039\n",
      "Epoch 759/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6450 - accuracy: 0.6356 - val_loss: 0.7177 - val_accuracy: 0.5118\n",
      "Epoch 760/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6420 - accuracy: 0.6396 - val_loss: 0.7202 - val_accuracy: 0.5118\n",
      "Epoch 761/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6377 - accuracy: 0.6178 - val_loss: 0.7199 - val_accuracy: 0.5118\n",
      "Epoch 762/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6362 - accuracy: 0.6455 - val_loss: 0.7288 - val_accuracy: 0.5118\n",
      "Epoch 763/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6392 - accuracy: 0.6515 - val_loss: 0.7141 - val_accuracy: 0.5039\n",
      "Epoch 764/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6405 - accuracy: 0.6475 - val_loss: 0.7295 - val_accuracy: 0.5276\n",
      "Epoch 765/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6428 - accuracy: 0.6337 - val_loss: 0.7192 - val_accuracy: 0.5118\n",
      "Epoch 766/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6328 - accuracy: 0.6614 - val_loss: 0.7223 - val_accuracy: 0.5197\n",
      "Epoch 767/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6400 - accuracy: 0.6535 - val_loss: 0.7265 - val_accuracy: 0.5276\n",
      "Epoch 768/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6391 - accuracy: 0.6376 - val_loss: 0.7266 - val_accuracy: 0.5276\n",
      "Epoch 769/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6405 - accuracy: 0.6416 - val_loss: 0.7157 - val_accuracy: 0.5276\n",
      "Epoch 770/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6359 - accuracy: 0.6317 - val_loss: 0.7219 - val_accuracy: 0.5276\n",
      "Epoch 771/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6375 - accuracy: 0.6535 - val_loss: 0.7105 - val_accuracy: 0.5197\n",
      "Epoch 772/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 223us/step - loss: 0.6382 - accuracy: 0.6614 - val_loss: 0.7232 - val_accuracy: 0.5276\n",
      "Epoch 773/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6304 - accuracy: 0.6851 - val_loss: 0.7197 - val_accuracy: 0.5276\n",
      "Epoch 774/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6397 - accuracy: 0.6554 - val_loss: 0.7198 - val_accuracy: 0.5276\n",
      "Epoch 775/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6337 - accuracy: 0.6594 - val_loss: 0.7188 - val_accuracy: 0.5276\n",
      "Epoch 776/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6401 - accuracy: 0.6297 - val_loss: 0.7316 - val_accuracy: 0.5591\n",
      "Epoch 777/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6332 - accuracy: 0.6772 - val_loss: 0.7179 - val_accuracy: 0.5276\n",
      "Epoch 778/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6262 - accuracy: 0.6713 - val_loss: 0.7141 - val_accuracy: 0.5276\n",
      "Epoch 779/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6347 - accuracy: 0.6673 - val_loss: 0.7065 - val_accuracy: 0.5591\n",
      "Epoch 780/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6394 - accuracy: 0.6713 - val_loss: 0.7142 - val_accuracy: 0.5276\n",
      "Epoch 781/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6323 - accuracy: 0.6792 - val_loss: 0.7115 - val_accuracy: 0.5276\n",
      "Epoch 782/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6327 - accuracy: 0.6614 - val_loss: 0.7186 - val_accuracy: 0.5276\n",
      "Epoch 783/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6306 - accuracy: 0.6634 - val_loss: 0.7186 - val_accuracy: 0.5276\n",
      "Epoch 784/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6293 - accuracy: 0.6713 - val_loss: 0.7164 - val_accuracy: 0.5276\n",
      "Epoch 785/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6359 - accuracy: 0.6574 - val_loss: 0.7190 - val_accuracy: 0.5276\n",
      "Epoch 786/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6339 - accuracy: 0.6752 - val_loss: 0.7171 - val_accuracy: 0.5276\n",
      "Epoch 787/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6362 - accuracy: 0.6733 - val_loss: 0.7109 - val_accuracy: 0.5276\n",
      "Epoch 788/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.6319 - accuracy: 0.6713 - val_loss: 0.7239 - val_accuracy: 0.5354\n",
      "Epoch 789/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6419 - accuracy: 0.6317 - val_loss: 0.7090 - val_accuracy: 0.5276\n",
      "Epoch 790/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6375 - accuracy: 0.6574 - val_loss: 0.7109 - val_accuracy: 0.5276\n",
      "Epoch 791/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.6326 - accuracy: 0.6713 - val_loss: 0.7132 - val_accuracy: 0.5276\n",
      "Epoch 792/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6389 - accuracy: 0.6752 - val_loss: 0.7076 - val_accuracy: 0.5276\n",
      "Epoch 793/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6333 - accuracy: 0.6614 - val_loss: 0.7336 - val_accuracy: 0.5276\n",
      "Epoch 794/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6325 - accuracy: 0.6950 - val_loss: 0.7046 - val_accuracy: 0.5512\n",
      "Epoch 795/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6302 - accuracy: 0.6752 - val_loss: 0.6986 - val_accuracy: 0.5276\n",
      "Epoch 796/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6314 - accuracy: 0.6515 - val_loss: 0.7069 - val_accuracy: 0.5433\n",
      "Epoch 797/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6301 - accuracy: 0.6931 - val_loss: 0.7063 - val_accuracy: 0.5669\n",
      "Epoch 798/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6301 - accuracy: 0.6614 - val_loss: 0.7144 - val_accuracy: 0.5276\n",
      "Epoch 799/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6336 - accuracy: 0.6673 - val_loss: 0.7097 - val_accuracy: 0.5276\n",
      "Epoch 800/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6326 - accuracy: 0.6713 - val_loss: 0.7064 - val_accuracy: 0.5276\n",
      "Epoch 801/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6346 - accuracy: 0.6871 - val_loss: 0.7018 - val_accuracy: 0.5669\n",
      "Epoch 802/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6277 - accuracy: 0.6673 - val_loss: 0.7033 - val_accuracy: 0.5512\n",
      "Epoch 803/2000\n",
      "505/505 [==============================] - 0s 237us/step - loss: 0.6370 - accuracy: 0.6832 - val_loss: 0.7163 - val_accuracy: 0.5433\n",
      "Epoch 804/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6276 - accuracy: 0.7089 - val_loss: 0.7106 - val_accuracy: 0.5433\n",
      "Epoch 805/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6341 - accuracy: 0.6871 - val_loss: 0.7076 - val_accuracy: 0.5433\n",
      "Epoch 806/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6283 - accuracy: 0.6792 - val_loss: 0.7255 - val_accuracy: 0.5827\n",
      "Epoch 807/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6236 - accuracy: 0.7149 - val_loss: 0.7098 - val_accuracy: 0.5433\n",
      "Epoch 808/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6271 - accuracy: 0.6891 - val_loss: 0.7176 - val_accuracy: 0.5433\n",
      "Epoch 809/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6285 - accuracy: 0.6752 - val_loss: 0.7239 - val_accuracy: 0.5748\n",
      "Epoch 810/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6375 - accuracy: 0.6792 - val_loss: 0.7064 - val_accuracy: 0.5433\n",
      "Epoch 811/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6241 - accuracy: 0.6891 - val_loss: 0.7181 - val_accuracy: 0.5512\n",
      "Epoch 812/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6248 - accuracy: 0.6931 - val_loss: 0.7302 - val_accuracy: 0.5591\n",
      "Epoch 813/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6302 - accuracy: 0.7149 - val_loss: 0.7108 - val_accuracy: 0.5433\n",
      "Epoch 814/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6281 - accuracy: 0.6812 - val_loss: 0.7211 - val_accuracy: 0.5748\n",
      "Epoch 815/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6282 - accuracy: 0.6832 - val_loss: 0.7262 - val_accuracy: 0.5984\n",
      "Epoch 816/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6301 - accuracy: 0.6990 - val_loss: 0.7147 - val_accuracy: 0.5669\n",
      "Epoch 817/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6300 - accuracy: 0.6990 - val_loss: 0.7243 - val_accuracy: 0.5984\n",
      "Epoch 818/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6268 - accuracy: 0.6594 - val_loss: 0.7162 - val_accuracy: 0.5669\n",
      "Epoch 819/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6291 - accuracy: 0.7030 - val_loss: 0.7104 - val_accuracy: 0.5669\n",
      "Epoch 820/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6227 - accuracy: 0.7208 - val_loss: 0.7045 - val_accuracy: 0.5669\n",
      "Epoch 821/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6249 - accuracy: 0.7109 - val_loss: 0.7247 - val_accuracy: 0.5984\n",
      "Epoch 822/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6217 - accuracy: 0.7089 - val_loss: 0.7062 - val_accuracy: 0.5591\n",
      "Epoch 823/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6330 - accuracy: 0.6891 - val_loss: 0.7228 - val_accuracy: 0.5984\n",
      "Epoch 824/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6319 - accuracy: 0.6832 - val_loss: 0.7171 - val_accuracy: 0.5669\n",
      "Epoch 825/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6322 - accuracy: 0.7188 - val_loss: 0.7082 - val_accuracy: 0.5669\n",
      "Epoch 826/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6267 - accuracy: 0.7050 - val_loss: 0.7043 - val_accuracy: 0.5906\n",
      "Epoch 827/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6247 - accuracy: 0.7188 - val_loss: 0.7032 - val_accuracy: 0.5906\n",
      "Epoch 828/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6317 - accuracy: 0.6733 - val_loss: 0.7128 - val_accuracy: 0.5669\n",
      "Epoch 829/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6237 - accuracy: 0.7069 - val_loss: 0.7095 - val_accuracy: 0.5669\n",
      "Epoch 830/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6226 - accuracy: 0.7030 - val_loss: 0.6964 - val_accuracy: 0.5669\n",
      "Epoch 831/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6243 - accuracy: 0.6950 - val_loss: 0.7125 - val_accuracy: 0.5669\n",
      "Epoch 832/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6253 - accuracy: 0.6931 - val_loss: 0.7123 - val_accuracy: 0.5669\n",
      "Epoch 833/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6204 - accuracy: 0.6891 - val_loss: 0.7121 - val_accuracy: 0.5669\n",
      "Epoch 834/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6198 - accuracy: 0.7188 - val_loss: 0.6999 - val_accuracy: 0.6142\n",
      "Epoch 835/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6196 - accuracy: 0.7109 - val_loss: 0.7156 - val_accuracy: 0.5669\n",
      "Epoch 836/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6213 - accuracy: 0.6970 - val_loss: 0.7126 - val_accuracy: 0.5669\n",
      "Epoch 837/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6279 - accuracy: 0.7109 - val_loss: 0.6978 - val_accuracy: 0.5827\n",
      "Epoch 838/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6227 - accuracy: 0.7168 - val_loss: 0.7074 - val_accuracy: 0.5669\n",
      "Epoch 839/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6292 - accuracy: 0.7129 - val_loss: 0.7010 - val_accuracy: 0.5669\n",
      "Epoch 840/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6242 - accuracy: 0.7149 - val_loss: 0.7051 - val_accuracy: 0.5669\n",
      "Epoch 841/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6219 - accuracy: 0.7030 - val_loss: 0.6980 - val_accuracy: 0.6063\n",
      "Epoch 842/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6268 - accuracy: 0.6931 - val_loss: 0.7058 - val_accuracy: 0.5591\n",
      "Epoch 843/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6222 - accuracy: 0.7267 - val_loss: 0.7121 - val_accuracy: 0.5748\n",
      "Epoch 844/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6243 - accuracy: 0.7030 - val_loss: 0.7141 - val_accuracy: 0.5748\n",
      "Epoch 845/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6228 - accuracy: 0.7168 - val_loss: 0.7025 - val_accuracy: 0.5748\n",
      "Epoch 846/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6219 - accuracy: 0.7327 - val_loss: 0.7098 - val_accuracy: 0.5748\n",
      "Epoch 847/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6256 - accuracy: 0.7030 - val_loss: 0.6990 - val_accuracy: 0.6063\n",
      "Epoch 848/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6283 - accuracy: 0.7010 - val_loss: 0.7054 - val_accuracy: 0.5827\n",
      "Epoch 849/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6238 - accuracy: 0.7228 - val_loss: 0.7103 - val_accuracy: 0.5827\n",
      "Epoch 850/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6213 - accuracy: 0.7228 - val_loss: 0.7139 - val_accuracy: 0.5827\n",
      "Epoch 851/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6238 - accuracy: 0.7248 - val_loss: 0.6926 - val_accuracy: 0.5669\n",
      "Epoch 852/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6222 - accuracy: 0.7347 - val_loss: 0.7012 - val_accuracy: 0.6142\n",
      "Epoch 853/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6197 - accuracy: 0.6970 - val_loss: 0.7177 - val_accuracy: 0.6063\n",
      "Epoch 854/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6175 - accuracy: 0.7366 - val_loss: 0.7212 - val_accuracy: 0.6220\n",
      "Epoch 855/2000\n",
      "505/505 [==============================] - 0s 233us/step - loss: 0.6286 - accuracy: 0.7267 - val_loss: 0.7014 - val_accuracy: 0.5906\n",
      "Epoch 856/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6254 - accuracy: 0.7327 - val_loss: 0.6947 - val_accuracy: 0.6063\n",
      "Epoch 857/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6247 - accuracy: 0.7208 - val_loss: 0.7115 - val_accuracy: 0.5827\n",
      "Epoch 858/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6165 - accuracy: 0.7228 - val_loss: 0.7118 - val_accuracy: 0.5906\n",
      "Epoch 859/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6158 - accuracy: 0.7545 - val_loss: 0.7114 - val_accuracy: 0.5906\n",
      "Epoch 860/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6200 - accuracy: 0.7267 - val_loss: 0.7027 - val_accuracy: 0.6063\n",
      "Epoch 861/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6145 - accuracy: 0.7604 - val_loss: 0.7053 - val_accuracy: 0.5984\n",
      "Epoch 862/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6206 - accuracy: 0.7168 - val_loss: 0.7012 - val_accuracy: 0.6142\n",
      "Epoch 863/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6189 - accuracy: 0.7327 - val_loss: 0.7071 - val_accuracy: 0.5827\n",
      "Epoch 864/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6091 - accuracy: 0.7505 - val_loss: 0.7038 - val_accuracy: 0.5827\n",
      "Epoch 865/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6201 - accuracy: 0.7248 - val_loss: 0.6988 - val_accuracy: 0.6220\n",
      "Epoch 866/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6196 - accuracy: 0.7287 - val_loss: 0.6987 - val_accuracy: 0.6142\n",
      "Epoch 867/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6174 - accuracy: 0.7307 - val_loss: 0.7003 - val_accuracy: 0.5906\n",
      "Epoch 868/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6216 - accuracy: 0.7366 - val_loss: 0.7140 - val_accuracy: 0.6220\n",
      "Epoch 869/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6230 - accuracy: 0.7208 - val_loss: 0.7167 - val_accuracy: 0.6457\n",
      "Epoch 870/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6176 - accuracy: 0.7426 - val_loss: 0.7063 - val_accuracy: 0.6063\n",
      "Epoch 871/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6175 - accuracy: 0.7248 - val_loss: 0.7015 - val_accuracy: 0.5984\n",
      "Epoch 872/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6152 - accuracy: 0.7366 - val_loss: 0.7221 - val_accuracy: 0.6457\n",
      "Epoch 873/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6205 - accuracy: 0.7248 - val_loss: 0.7082 - val_accuracy: 0.6142\n",
      "Epoch 874/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6155 - accuracy: 0.7228 - val_loss: 0.6997 - val_accuracy: 0.5984\n",
      "Epoch 875/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6148 - accuracy: 0.7267 - val_loss: 0.7264 - val_accuracy: 0.6457\n",
      "Epoch 876/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6217 - accuracy: 0.7168 - val_loss: 0.7050 - val_accuracy: 0.6142\n",
      "Epoch 877/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6118 - accuracy: 0.7366 - val_loss: 0.7027 - val_accuracy: 0.6063\n",
      "Epoch 878/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6216 - accuracy: 0.7307 - val_loss: 0.7061 - val_accuracy: 0.6142\n",
      "Epoch 879/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6124 - accuracy: 0.7584 - val_loss: 0.7008 - val_accuracy: 0.6142\n",
      "Epoch 880/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6200 - accuracy: 0.7327 - val_loss: 0.6988 - val_accuracy: 0.6063\n",
      "Epoch 881/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6211 - accuracy: 0.7327 - val_loss: 0.6853 - val_accuracy: 0.6220\n",
      "Epoch 882/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 219us/step - loss: 0.6213 - accuracy: 0.7287 - val_loss: 0.6982 - val_accuracy: 0.6142\n",
      "Epoch 883/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6100 - accuracy: 0.7525 - val_loss: 0.6976 - val_accuracy: 0.6142\n",
      "Epoch 884/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6081 - accuracy: 0.7644 - val_loss: 0.6972 - val_accuracy: 0.6063\n",
      "Epoch 885/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6161 - accuracy: 0.7069 - val_loss: 0.7242 - val_accuracy: 0.6063\n",
      "Epoch 886/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6113 - accuracy: 0.7663 - val_loss: 0.6948 - val_accuracy: 0.6378\n",
      "Epoch 887/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6126 - accuracy: 0.7347 - val_loss: 0.7017 - val_accuracy: 0.6142\n",
      "Epoch 888/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6118 - accuracy: 0.7703 - val_loss: 0.6960 - val_accuracy: 0.6063\n",
      "Epoch 889/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6072 - accuracy: 0.7584 - val_loss: 0.7046 - val_accuracy: 0.6142\n",
      "Epoch 890/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.6159 - accuracy: 0.7366 - val_loss: 0.7148 - val_accuracy: 0.6457\n",
      "Epoch 891/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6066 - accuracy: 0.7782 - val_loss: 0.7228 - val_accuracy: 0.6063\n",
      "Epoch 892/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.6145 - accuracy: 0.7267 - val_loss: 0.7105 - val_accuracy: 0.6142\n",
      "Epoch 893/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6081 - accuracy: 0.7782 - val_loss: 0.7016 - val_accuracy: 0.6142\n",
      "Epoch 894/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6087 - accuracy: 0.7584 - val_loss: 0.6958 - val_accuracy: 0.6142\n",
      "Epoch 895/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6160 - accuracy: 0.7366 - val_loss: 0.6928 - val_accuracy: 0.6220\n",
      "Epoch 896/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6096 - accuracy: 0.7564 - val_loss: 0.7139 - val_accuracy: 0.6378\n",
      "Epoch 897/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6139 - accuracy: 0.7505 - val_loss: 0.6964 - val_accuracy: 0.6142\n",
      "Epoch 898/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6063 - accuracy: 0.7406 - val_loss: 0.6997 - val_accuracy: 0.6142\n",
      "Epoch 899/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6140 - accuracy: 0.7267 - val_loss: 0.7071 - val_accuracy: 0.6220\n",
      "Epoch 900/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6140 - accuracy: 0.7525 - val_loss: 0.7041 - val_accuracy: 0.6142\n",
      "Epoch 901/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6132 - accuracy: 0.7525 - val_loss: 0.6943 - val_accuracy: 0.6142\n",
      "Epoch 902/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6083 - accuracy: 0.7723 - val_loss: 0.6963 - val_accuracy: 0.6142\n",
      "Epoch 903/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6107 - accuracy: 0.7485 - val_loss: 0.6911 - val_accuracy: 0.6378\n",
      "Epoch 904/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6049 - accuracy: 0.7703 - val_loss: 0.7096 - val_accuracy: 0.6299\n",
      "Epoch 905/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6132 - accuracy: 0.7366 - val_loss: 0.6879 - val_accuracy: 0.6535\n",
      "Epoch 906/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6130 - accuracy: 0.7545 - val_loss: 0.6939 - val_accuracy: 0.6220\n",
      "Epoch 907/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6112 - accuracy: 0.7663 - val_loss: 0.6847 - val_accuracy: 0.6457\n",
      "Epoch 908/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6155 - accuracy: 0.7406 - val_loss: 0.6992 - val_accuracy: 0.6220\n",
      "Epoch 909/2000\n",
      "505/505 [==============================] - 0s 224us/step - loss: 0.6048 - accuracy: 0.7663 - val_loss: 0.6750 - val_accuracy: 0.6142\n",
      "Epoch 910/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.6164 - accuracy: 0.7446 - val_loss: 0.6802 - val_accuracy: 0.6457\n",
      "Epoch 911/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6052 - accuracy: 0.7644 - val_loss: 0.6938 - val_accuracy: 0.6299\n",
      "Epoch 912/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6034 - accuracy: 0.7743 - val_loss: 0.6965 - val_accuracy: 0.6299\n",
      "Epoch 913/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6053 - accuracy: 0.7921 - val_loss: 0.6745 - val_accuracy: 0.6142\n",
      "Epoch 914/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6046 - accuracy: 0.7762 - val_loss: 0.7141 - val_accuracy: 0.6772\n",
      "Epoch 915/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6034 - accuracy: 0.7901 - val_loss: 0.6744 - val_accuracy: 0.6220\n",
      "Epoch 916/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6109 - accuracy: 0.7505 - val_loss: 0.6855 - val_accuracy: 0.6614\n",
      "Epoch 917/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5992 - accuracy: 0.7683 - val_loss: 0.6894 - val_accuracy: 0.6693\n",
      "Epoch 918/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.5998 - accuracy: 0.7624 - val_loss: 0.6817 - val_accuracy: 0.6220\n",
      "Epoch 919/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6083 - accuracy: 0.7525 - val_loss: 0.6857 - val_accuracy: 0.6693\n",
      "Epoch 920/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.6094 - accuracy: 0.7762 - val_loss: 0.7087 - val_accuracy: 0.6772\n",
      "Epoch 921/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5991 - accuracy: 0.7921 - val_loss: 0.7092 - val_accuracy: 0.6614\n",
      "Epoch 922/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6021 - accuracy: 0.7861 - val_loss: 0.7113 - val_accuracy: 0.6772\n",
      "Epoch 923/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.6066 - accuracy: 0.7703 - val_loss: 0.6829 - val_accuracy: 0.6693\n",
      "Epoch 924/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6043 - accuracy: 0.7604 - val_loss: 0.6804 - val_accuracy: 0.6614\n",
      "Epoch 925/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6068 - accuracy: 0.7703 - val_loss: 0.6854 - val_accuracy: 0.6378\n",
      "Epoch 926/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6063 - accuracy: 0.7861 - val_loss: 0.6896 - val_accuracy: 0.6299\n",
      "Epoch 927/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6069 - accuracy: 0.7861 - val_loss: 0.6953 - val_accuracy: 0.6299\n",
      "Epoch 928/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5965 - accuracy: 0.7941 - val_loss: 0.7022 - val_accuracy: 0.6378\n",
      "Epoch 929/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6047 - accuracy: 0.7802 - val_loss: 0.6699 - val_accuracy: 0.6220\n",
      "Epoch 930/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6026 - accuracy: 0.7545 - val_loss: 0.7222 - val_accuracy: 0.6378\n",
      "Epoch 931/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6027 - accuracy: 0.7842 - val_loss: 0.7113 - val_accuracy: 0.6614\n",
      "Epoch 932/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5995 - accuracy: 0.7683 - val_loss: 0.7103 - val_accuracy: 0.6850\n",
      "Epoch 933/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6026 - accuracy: 0.7743 - val_loss: 0.6914 - val_accuracy: 0.6457\n",
      "Epoch 934/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6013 - accuracy: 0.7782 - val_loss: 0.6948 - val_accuracy: 0.6457\n",
      "Epoch 935/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5988 - accuracy: 0.7881 - val_loss: 0.6815 - val_accuracy: 0.6850\n",
      "Epoch 936/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5987 - accuracy: 0.7802 - val_loss: 0.6940 - val_accuracy: 0.6693\n",
      "Epoch 937/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.5998 - accuracy: 0.7921 - val_loss: 0.6840 - val_accuracy: 0.6929\n",
      "Epoch 938/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5998 - accuracy: 0.8020 - val_loss: 0.6738 - val_accuracy: 0.6299\n",
      "Epoch 939/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5974 - accuracy: 0.7703 - val_loss: 0.7050 - val_accuracy: 0.6850\n",
      "Epoch 940/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6062 - accuracy: 0.7822 - val_loss: 0.6800 - val_accuracy: 0.6772\n",
      "Epoch 941/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.6007 - accuracy: 0.7743 - val_loss: 0.7267 - val_accuracy: 0.6614\n",
      "Epoch 942/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5972 - accuracy: 0.7842 - val_loss: 0.7011 - val_accuracy: 0.6850\n",
      "Epoch 943/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5941 - accuracy: 0.8059 - val_loss: 0.6941 - val_accuracy: 0.6535\n",
      "Epoch 944/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5960 - accuracy: 0.8059 - val_loss: 0.7258 - val_accuracy: 0.6614\n",
      "Epoch 945/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.6080 - accuracy: 0.7703 - val_loss: 0.6869 - val_accuracy: 0.6535\n",
      "Epoch 946/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.6051 - accuracy: 0.7644 - val_loss: 0.7042 - val_accuracy: 0.6850\n",
      "Epoch 947/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6060 - accuracy: 0.7584 - val_loss: 0.6776 - val_accuracy: 0.6457\n",
      "Epoch 948/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.6010 - accuracy: 0.7782 - val_loss: 0.6835 - val_accuracy: 0.6929\n",
      "Epoch 949/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5969 - accuracy: 0.7980 - val_loss: 0.7161 - val_accuracy: 0.6614\n",
      "Epoch 950/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.6022 - accuracy: 0.7980 - val_loss: 0.6943 - val_accuracy: 0.6614\n",
      "Epoch 951/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5961 - accuracy: 0.7921 - val_loss: 0.6942 - val_accuracy: 0.6693\n",
      "Epoch 952/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5990 - accuracy: 0.7822 - val_loss: 0.7071 - val_accuracy: 0.7087\n",
      "Epoch 953/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5938 - accuracy: 0.8020 - val_loss: 0.7008 - val_accuracy: 0.7008\n",
      "Epoch 954/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5995 - accuracy: 0.7960 - val_loss: 0.7001 - val_accuracy: 0.6772\n",
      "Epoch 955/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.6003 - accuracy: 0.7723 - val_loss: 0.7327 - val_accuracy: 0.6378\n",
      "Epoch 956/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5969 - accuracy: 0.7921 - val_loss: 0.6796 - val_accuracy: 0.6614\n",
      "Epoch 957/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.6029 - accuracy: 0.7743 - val_loss: 0.6885 - val_accuracy: 0.6535\n",
      "Epoch 958/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5978 - accuracy: 0.7921 - val_loss: 0.6849 - val_accuracy: 0.6535\n",
      "Epoch 959/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.6019 - accuracy: 0.7842 - val_loss: 0.6888 - val_accuracy: 0.6693\n",
      "Epoch 960/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5975 - accuracy: 0.7802 - val_loss: 0.6806 - val_accuracy: 0.6850\n",
      "Epoch 961/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5942 - accuracy: 0.7960 - val_loss: 0.6887 - val_accuracy: 0.6693\n",
      "Epoch 962/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.5898 - accuracy: 0.8059 - val_loss: 0.7192 - val_accuracy: 0.6772\n",
      "Epoch 963/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.5950 - accuracy: 0.8079 - val_loss: 0.6787 - val_accuracy: 0.7087\n",
      "Epoch 964/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5903 - accuracy: 0.7921 - val_loss: 0.7189 - val_accuracy: 0.6850\n",
      "Epoch 965/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5943 - accuracy: 0.8020 - val_loss: 0.6949 - val_accuracy: 0.6772\n",
      "Epoch 966/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5917 - accuracy: 0.8119 - val_loss: 0.7020 - val_accuracy: 0.7165\n",
      "Epoch 967/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5921 - accuracy: 0.8040 - val_loss: 0.6800 - val_accuracy: 0.6772\n",
      "Epoch 968/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5896 - accuracy: 0.7842 - val_loss: 0.6966 - val_accuracy: 0.7244\n",
      "Epoch 969/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.5893 - accuracy: 0.8059 - val_loss: 0.6937 - val_accuracy: 0.7087\n",
      "Epoch 970/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5894 - accuracy: 0.8079 - val_loss: 0.6711 - val_accuracy: 0.7165\n",
      "Epoch 971/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5880 - accuracy: 0.8020 - val_loss: 0.6696 - val_accuracy: 0.7008\n",
      "Epoch 972/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5935 - accuracy: 0.7921 - val_loss: 0.6700 - val_accuracy: 0.7165\n",
      "Epoch 973/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5960 - accuracy: 0.7901 - val_loss: 0.6735 - val_accuracy: 0.6693\n",
      "Epoch 974/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5882 - accuracy: 0.8079 - val_loss: 0.6711 - val_accuracy: 0.6772\n",
      "Epoch 975/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5879 - accuracy: 0.8000 - val_loss: 0.6619 - val_accuracy: 0.6929\n",
      "Epoch 976/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5933 - accuracy: 0.7663 - val_loss: 0.6658 - val_accuracy: 0.7165\n",
      "Epoch 977/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.5872 - accuracy: 0.8059 - val_loss: 0.6798 - val_accuracy: 0.6929\n",
      "Epoch 978/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5859 - accuracy: 0.8198 - val_loss: 0.6934 - val_accuracy: 0.7323\n",
      "Epoch 979/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.5910 - accuracy: 0.8158 - val_loss: 0.6812 - val_accuracy: 0.6850\n",
      "Epoch 980/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5928 - accuracy: 0.8000 - val_loss: 0.6624 - val_accuracy: 0.6772\n",
      "Epoch 981/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5968 - accuracy: 0.7881 - val_loss: 0.6637 - val_accuracy: 0.7244\n",
      "Epoch 982/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5747 - accuracy: 0.8416 - val_loss: 0.6965 - val_accuracy: 0.7244\n",
      "Epoch 983/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5799 - accuracy: 0.8020 - val_loss: 0.6531 - val_accuracy: 0.7008\n",
      "Epoch 984/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5934 - accuracy: 0.8040 - val_loss: 0.6723 - val_accuracy: 0.6929\n",
      "Epoch 985/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5871 - accuracy: 0.7921 - val_loss: 0.6683 - val_accuracy: 0.7008\n",
      "Epoch 986/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5858 - accuracy: 0.8079 - val_loss: 0.6642 - val_accuracy: 0.7480\n",
      "Epoch 987/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5776 - accuracy: 0.8079 - val_loss: 0.6907 - val_accuracy: 0.7244\n",
      "Epoch 988/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5903 - accuracy: 0.8020 - val_loss: 0.6696 - val_accuracy: 0.7165\n",
      "Epoch 989/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5824 - accuracy: 0.8337 - val_loss: 0.6576 - val_accuracy: 0.7402\n",
      "Epoch 990/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5859 - accuracy: 0.8040 - val_loss: 0.6873 - val_accuracy: 0.7323\n",
      "Epoch 991/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5892 - accuracy: 0.8079 - val_loss: 0.6827 - val_accuracy: 0.7087\n",
      "Epoch 992/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 214us/step - loss: 0.5795 - accuracy: 0.8178 - val_loss: 0.6985 - val_accuracy: 0.7165\n",
      "Epoch 993/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5864 - accuracy: 0.8139 - val_loss: 0.6720 - val_accuracy: 0.7165\n",
      "Epoch 994/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5842 - accuracy: 0.7921 - val_loss: 0.6850 - val_accuracy: 0.7244\n",
      "Epoch 995/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5843 - accuracy: 0.7822 - val_loss: 0.6708 - val_accuracy: 0.7165\n",
      "Epoch 996/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.5818 - accuracy: 0.7723 - val_loss: 0.7029 - val_accuracy: 0.7087\n",
      "Epoch 997/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5805 - accuracy: 0.8178 - val_loss: 0.6961 - val_accuracy: 0.7244\n",
      "Epoch 998/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5831 - accuracy: 0.8277 - val_loss: 0.6539 - val_accuracy: 0.7323\n",
      "Epoch 999/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5764 - accuracy: 0.8218 - val_loss: 0.6620 - val_accuracy: 0.7323\n",
      "Epoch 1000/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5770 - accuracy: 0.8238 - val_loss: 0.6711 - val_accuracy: 0.7008\n",
      "Epoch 1001/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5771 - accuracy: 0.8158 - val_loss: 0.6752 - val_accuracy: 0.6929\n",
      "Epoch 1002/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5770 - accuracy: 0.8079 - val_loss: 0.6666 - val_accuracy: 0.7008\n",
      "Epoch 1003/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5815 - accuracy: 0.8059 - val_loss: 0.6546 - val_accuracy: 0.7323\n",
      "Epoch 1004/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5770 - accuracy: 0.8178 - val_loss: 0.6894 - val_accuracy: 0.7638\n",
      "Epoch 1005/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5798 - accuracy: 0.8218 - val_loss: 0.6826 - val_accuracy: 0.7323\n",
      "Epoch 1006/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5748 - accuracy: 0.8099 - val_loss: 0.6600 - val_accuracy: 0.7244\n",
      "Epoch 1007/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5733 - accuracy: 0.8297 - val_loss: 0.6953 - val_accuracy: 0.7323\n",
      "Epoch 1008/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5848 - accuracy: 0.7921 - val_loss: 0.6900 - val_accuracy: 0.7559\n",
      "Epoch 1009/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5711 - accuracy: 0.8297 - val_loss: 0.6500 - val_accuracy: 0.7165\n",
      "Epoch 1010/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5758 - accuracy: 0.8178 - val_loss: 0.6807 - val_accuracy: 0.7559\n",
      "Epoch 1011/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5846 - accuracy: 0.8020 - val_loss: 0.6851 - val_accuracy: 0.7559\n",
      "Epoch 1012/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5702 - accuracy: 0.8376 - val_loss: 0.6716 - val_accuracy: 0.7323\n",
      "Epoch 1013/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5793 - accuracy: 0.8000 - val_loss: 0.6615 - val_accuracy: 0.7717\n",
      "Epoch 1014/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5748 - accuracy: 0.8079 - val_loss: 0.6703 - val_accuracy: 0.7165\n",
      "Epoch 1015/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5767 - accuracy: 0.8238 - val_loss: 0.6607 - val_accuracy: 0.7402\n",
      "Epoch 1016/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5777 - accuracy: 0.8119 - val_loss: 0.6707 - val_accuracy: 0.7165\n",
      "Epoch 1017/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5762 - accuracy: 0.8059 - val_loss: 0.6909 - val_accuracy: 0.7480\n",
      "Epoch 1018/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5811 - accuracy: 0.8257 - val_loss: 0.6782 - val_accuracy: 0.7480\n",
      "Epoch 1019/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5760 - accuracy: 0.8099 - val_loss: 0.6687 - val_accuracy: 0.7402\n",
      "Epoch 1020/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5701 - accuracy: 0.8257 - val_loss: 0.6832 - val_accuracy: 0.7559\n",
      "Epoch 1021/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5737 - accuracy: 0.8475 - val_loss: 0.6866 - val_accuracy: 0.7559\n",
      "Epoch 1022/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5642 - accuracy: 0.8257 - val_loss: 0.6469 - val_accuracy: 0.7244\n",
      "Epoch 1023/2000\n",
      "505/505 [==============================] - 0s 228us/step - loss: 0.5679 - accuracy: 0.8396 - val_loss: 0.6528 - val_accuracy: 0.7559\n",
      "Epoch 1024/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5651 - accuracy: 0.8297 - val_loss: 0.6843 - val_accuracy: 0.7559\n",
      "Epoch 1025/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.5800 - accuracy: 0.8040 - val_loss: 0.6581 - val_accuracy: 0.7480\n",
      "Epoch 1026/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5619 - accuracy: 0.8337 - val_loss: 0.6582 - val_accuracy: 0.7402\n",
      "Epoch 1027/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.5743 - accuracy: 0.8277 - val_loss: 0.6711 - val_accuracy: 0.7244\n",
      "Epoch 1028/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.5669 - accuracy: 0.8178 - val_loss: 0.6676 - val_accuracy: 0.7165\n",
      "Epoch 1029/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5622 - accuracy: 0.8396 - val_loss: 0.6440 - val_accuracy: 0.7323\n",
      "Epoch 1030/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5594 - accuracy: 0.8376 - val_loss: 0.6680 - val_accuracy: 0.7402\n",
      "Epoch 1031/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5713 - accuracy: 0.8317 - val_loss: 0.6721 - val_accuracy: 0.7165\n",
      "Epoch 1032/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5688 - accuracy: 0.8356 - val_loss: 0.6606 - val_accuracy: 0.7559\n",
      "Epoch 1033/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5691 - accuracy: 0.8059 - val_loss: 0.6553 - val_accuracy: 0.7559\n",
      "Epoch 1034/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5760 - accuracy: 0.8139 - val_loss: 0.7012 - val_accuracy: 0.6929\n",
      "Epoch 1035/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5757 - accuracy: 0.7980 - val_loss: 0.6808 - val_accuracy: 0.7559\n",
      "Epoch 1036/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.5600 - accuracy: 0.8317 - val_loss: 0.6692 - val_accuracy: 0.7323\n",
      "Epoch 1037/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5679 - accuracy: 0.8198 - val_loss: 0.6799 - val_accuracy: 0.7559\n",
      "Epoch 1038/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5668 - accuracy: 0.8356 - val_loss: 0.6881 - val_accuracy: 0.7480\n",
      "Epoch 1039/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5640 - accuracy: 0.8455 - val_loss: 0.6432 - val_accuracy: 0.7165\n",
      "Epoch 1040/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5677 - accuracy: 0.8218 - val_loss: 0.6566 - val_accuracy: 0.7402\n",
      "Epoch 1041/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5557 - accuracy: 0.8495 - val_loss: 0.6359 - val_accuracy: 0.7244\n",
      "Epoch 1042/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5603 - accuracy: 0.8198 - val_loss: 0.6632 - val_accuracy: 0.7165\n",
      "Epoch 1043/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5523 - accuracy: 0.8356 - val_loss: 0.6764 - val_accuracy: 0.7323\n",
      "Epoch 1044/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5547 - accuracy: 0.8535 - val_loss: 0.6738 - val_accuracy: 0.7402\n",
      "Epoch 1045/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5639 - accuracy: 0.8317 - val_loss: 0.6587 - val_accuracy: 0.7402\n",
      "Epoch 1046/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5586 - accuracy: 0.8337 - val_loss: 0.6812 - val_accuracy: 0.7559\n",
      "Epoch 1047/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5611 - accuracy: 0.8277 - val_loss: 0.6493 - val_accuracy: 0.7638\n",
      "Epoch 1048/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5516 - accuracy: 0.8614 - val_loss: 0.6447 - val_accuracy: 0.7638\n",
      "Epoch 1049/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5607 - accuracy: 0.8515 - val_loss: 0.6507 - val_accuracy: 0.7402\n",
      "Epoch 1050/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5534 - accuracy: 0.8416 - val_loss: 0.6761 - val_accuracy: 0.7638\n",
      "Epoch 1051/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5629 - accuracy: 0.8218 - val_loss: 0.6527 - val_accuracy: 0.7244\n",
      "Epoch 1052/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5598 - accuracy: 0.8158 - val_loss: 0.6391 - val_accuracy: 0.7480\n",
      "Epoch 1053/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5639 - accuracy: 0.8158 - val_loss: 0.6520 - val_accuracy: 0.7323\n",
      "Epoch 1054/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5502 - accuracy: 0.8475 - val_loss: 0.7118 - val_accuracy: 0.6850\n",
      "Epoch 1055/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5658 - accuracy: 0.8158 - val_loss: 0.6666 - val_accuracy: 0.7480\n",
      "Epoch 1056/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5575 - accuracy: 0.8297 - val_loss: 0.6407 - val_accuracy: 0.7480\n",
      "Epoch 1057/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5600 - accuracy: 0.8337 - val_loss: 0.6580 - val_accuracy: 0.7559\n",
      "Epoch 1058/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5569 - accuracy: 0.8495 - val_loss: 0.6297 - val_accuracy: 0.7480\n",
      "Epoch 1059/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5465 - accuracy: 0.8416 - val_loss: 0.6480 - val_accuracy: 0.7717\n",
      "Epoch 1060/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5613 - accuracy: 0.8317 - val_loss: 0.6621 - val_accuracy: 0.7323\n",
      "Epoch 1061/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5556 - accuracy: 0.8396 - val_loss: 0.6331 - val_accuracy: 0.7402\n",
      "Epoch 1062/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5594 - accuracy: 0.8356 - val_loss: 0.6399 - val_accuracy: 0.7638\n",
      "Epoch 1063/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5547 - accuracy: 0.8337 - val_loss: 0.6455 - val_accuracy: 0.7480\n",
      "Epoch 1064/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5554 - accuracy: 0.8297 - val_loss: 0.6399 - val_accuracy: 0.7559\n",
      "Epoch 1065/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.5542 - accuracy: 0.8257 - val_loss: 0.6313 - val_accuracy: 0.7244\n",
      "Epoch 1066/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5506 - accuracy: 0.8436 - val_loss: 0.6279 - val_accuracy: 0.7480\n",
      "Epoch 1067/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.5503 - accuracy: 0.8337 - val_loss: 0.6714 - val_accuracy: 0.7638\n",
      "Epoch 1068/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.5464 - accuracy: 0.8436 - val_loss: 0.6808 - val_accuracy: 0.7323\n",
      "Epoch 1069/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5573 - accuracy: 0.8277 - val_loss: 0.6433 - val_accuracy: 0.7638\n",
      "Epoch 1070/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5498 - accuracy: 0.8436 - val_loss: 0.6834 - val_accuracy: 0.7953\n",
      "Epoch 1071/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5559 - accuracy: 0.8218 - val_loss: 0.6462 - val_accuracy: 0.7244\n",
      "Epoch 1072/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5431 - accuracy: 0.8436 - val_loss: 0.6814 - val_accuracy: 0.7323\n",
      "Epoch 1073/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5527 - accuracy: 0.8317 - val_loss: 0.6551 - val_accuracy: 0.7323\n",
      "Epoch 1074/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5559 - accuracy: 0.8396 - val_loss: 0.6411 - val_accuracy: 0.7874\n",
      "Epoch 1075/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5574 - accuracy: 0.8178 - val_loss: 0.6168 - val_accuracy: 0.7323\n",
      "Epoch 1076/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5432 - accuracy: 0.8297 - val_loss: 0.6958 - val_accuracy: 0.7008\n",
      "Epoch 1077/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5556 - accuracy: 0.8337 - val_loss: 0.6743 - val_accuracy: 0.7244\n",
      "Epoch 1078/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.5457 - accuracy: 0.8416 - val_loss: 0.6469 - val_accuracy: 0.7559\n",
      "Epoch 1079/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5503 - accuracy: 0.8396 - val_loss: 0.6418 - val_accuracy: 0.7638\n",
      "Epoch 1080/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5464 - accuracy: 0.8436 - val_loss: 0.6330 - val_accuracy: 0.7717\n",
      "Epoch 1081/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5483 - accuracy: 0.8436 - val_loss: 0.6265 - val_accuracy: 0.7638\n",
      "Epoch 1082/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5415 - accuracy: 0.8594 - val_loss: 0.6158 - val_accuracy: 0.7559\n",
      "Epoch 1083/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5423 - accuracy: 0.8475 - val_loss: 0.6270 - val_accuracy: 0.7480\n",
      "Epoch 1084/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5474 - accuracy: 0.8436 - val_loss: 0.6413 - val_accuracy: 0.7953\n",
      "Epoch 1085/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5507 - accuracy: 0.8257 - val_loss: 0.6149 - val_accuracy: 0.7717\n",
      "Epoch 1086/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5437 - accuracy: 0.8535 - val_loss: 0.6045 - val_accuracy: 0.7165\n",
      "Epoch 1087/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5462 - accuracy: 0.8396 - val_loss: 0.6484 - val_accuracy: 0.7638\n",
      "Epoch 1088/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5425 - accuracy: 0.8436 - val_loss: 0.6368 - val_accuracy: 0.7480\n",
      "Epoch 1089/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5428 - accuracy: 0.8416 - val_loss: 0.6295 - val_accuracy: 0.7953\n",
      "Epoch 1090/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5375 - accuracy: 0.8713 - val_loss: 0.6704 - val_accuracy: 0.7165\n",
      "Epoch 1091/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5425 - accuracy: 0.8238 - val_loss: 0.6562 - val_accuracy: 0.7874\n",
      "Epoch 1092/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5378 - accuracy: 0.8594 - val_loss: 0.6386 - val_accuracy: 0.7402\n",
      "Epoch 1093/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5332 - accuracy: 0.8673 - val_loss: 0.6442 - val_accuracy: 0.7559\n",
      "Epoch 1094/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5367 - accuracy: 0.8594 - val_loss: 0.6312 - val_accuracy: 0.8031\n",
      "Epoch 1095/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5321 - accuracy: 0.8614 - val_loss: 0.6420 - val_accuracy: 0.7323\n",
      "Epoch 1096/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.5363 - accuracy: 0.8475 - val_loss: 0.6112 - val_accuracy: 0.7717\n",
      "Epoch 1097/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5307 - accuracy: 0.8733 - val_loss: 0.7103 - val_accuracy: 0.6929\n",
      "Epoch 1098/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.5475 - accuracy: 0.8356 - val_loss: 0.6718 - val_accuracy: 0.7402\n",
      "Epoch 1099/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5319 - accuracy: 0.8614 - val_loss: 0.6389 - val_accuracy: 0.7559\n",
      "Epoch 1100/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5407 - accuracy: 0.8475 - val_loss: 0.6220 - val_accuracy: 0.7795\n",
      "Epoch 1101/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5359 - accuracy: 0.8475 - val_loss: 0.6162 - val_accuracy: 0.7638\n",
      "Epoch 1102/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 221us/step - loss: 0.5294 - accuracy: 0.8554 - val_loss: 0.6568 - val_accuracy: 0.7244\n",
      "Epoch 1103/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5342 - accuracy: 0.8455 - val_loss: 0.6460 - val_accuracy: 0.7717\n",
      "Epoch 1104/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5279 - accuracy: 0.8515 - val_loss: 0.6038 - val_accuracy: 0.7638\n",
      "Epoch 1105/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5441 - accuracy: 0.8455 - val_loss: 0.6236 - val_accuracy: 0.7480\n",
      "Epoch 1106/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5248 - accuracy: 0.8495 - val_loss: 0.6389 - val_accuracy: 0.7402\n",
      "Epoch 1107/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5249 - accuracy: 0.8416 - val_loss: 0.6265 - val_accuracy: 0.7795\n",
      "Epoch 1108/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5305 - accuracy: 0.8554 - val_loss: 0.6295 - val_accuracy: 0.7559\n",
      "Epoch 1109/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5300 - accuracy: 0.8574 - val_loss: 0.6590 - val_accuracy: 0.7480\n",
      "Epoch 1110/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.5391 - accuracy: 0.8475 - val_loss: 0.5990 - val_accuracy: 0.7480\n",
      "Epoch 1111/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.5271 - accuracy: 0.8772 - val_loss: 0.6048 - val_accuracy: 0.7795\n",
      "Epoch 1112/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5199 - accuracy: 0.8436 - val_loss: 0.5995 - val_accuracy: 0.7717\n",
      "Epoch 1113/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.5245 - accuracy: 0.8416 - val_loss: 0.6650 - val_accuracy: 0.7244\n",
      "Epoch 1114/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5286 - accuracy: 0.8416 - val_loss: 0.6262 - val_accuracy: 0.7795\n",
      "Epoch 1115/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5191 - accuracy: 0.8574 - val_loss: 0.6301 - val_accuracy: 0.7717\n",
      "Epoch 1116/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5306 - accuracy: 0.8396 - val_loss: 0.6102 - val_accuracy: 0.7638\n",
      "Epoch 1117/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.5245 - accuracy: 0.8594 - val_loss: 0.6538 - val_accuracy: 0.7717\n",
      "Epoch 1118/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5290 - accuracy: 0.8554 - val_loss: 0.6028 - val_accuracy: 0.7874\n",
      "Epoch 1119/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5172 - accuracy: 0.8455 - val_loss: 0.6436 - val_accuracy: 0.7953\n",
      "Epoch 1120/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.5348 - accuracy: 0.8376 - val_loss: 0.6249 - val_accuracy: 0.7795\n",
      "Epoch 1121/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5142 - accuracy: 0.8693 - val_loss: 0.6051 - val_accuracy: 0.7559\n",
      "Epoch 1122/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.5309 - accuracy: 0.8337 - val_loss: 0.5941 - val_accuracy: 0.7795\n",
      "Epoch 1123/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5206 - accuracy: 0.8535 - val_loss: 0.6413 - val_accuracy: 0.7717\n",
      "Epoch 1124/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.5267 - accuracy: 0.8495 - val_loss: 0.6509 - val_accuracy: 0.7638\n",
      "Epoch 1125/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5216 - accuracy: 0.8455 - val_loss: 0.6519 - val_accuracy: 0.7717\n",
      "Epoch 1126/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.5186 - accuracy: 0.8515 - val_loss: 0.6025 - val_accuracy: 0.7953\n",
      "Epoch 1127/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5180 - accuracy: 0.8614 - val_loss: 0.6278 - val_accuracy: 0.7559\n",
      "Epoch 1128/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5157 - accuracy: 0.8634 - val_loss: 0.6470 - val_accuracy: 0.7244\n",
      "Epoch 1129/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5171 - accuracy: 0.8515 - val_loss: 0.6544 - val_accuracy: 0.7402\n",
      "Epoch 1130/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.5142 - accuracy: 0.8673 - val_loss: 0.6192 - val_accuracy: 0.7717\n",
      "Epoch 1131/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5265 - accuracy: 0.8396 - val_loss: 0.6262 - val_accuracy: 0.7717\n",
      "Epoch 1132/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5213 - accuracy: 0.8673 - val_loss: 0.6135 - val_accuracy: 0.7953\n",
      "Epoch 1133/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5061 - accuracy: 0.8653 - val_loss: 0.5927 - val_accuracy: 0.7480\n",
      "Epoch 1134/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5133 - accuracy: 0.8515 - val_loss: 0.5930 - val_accuracy: 0.7244\n",
      "Epoch 1135/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.5196 - accuracy: 0.8515 - val_loss: 0.6374 - val_accuracy: 0.7717\n",
      "Epoch 1136/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5115 - accuracy: 0.8634 - val_loss: 0.6077 - val_accuracy: 0.7874\n",
      "Epoch 1137/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.5157 - accuracy: 0.8495 - val_loss: 0.6150 - val_accuracy: 0.7638\n",
      "Epoch 1138/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5084 - accuracy: 0.8653 - val_loss: 0.6253 - val_accuracy: 0.7874\n",
      "Epoch 1139/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5201 - accuracy: 0.8436 - val_loss: 0.6558 - val_accuracy: 0.7244\n",
      "Epoch 1140/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.5195 - accuracy: 0.8356 - val_loss: 0.6682 - val_accuracy: 0.6929\n",
      "Epoch 1141/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5036 - accuracy: 0.8812 - val_loss: 0.6162 - val_accuracy: 0.7795\n",
      "Epoch 1142/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5192 - accuracy: 0.8356 - val_loss: 0.6200 - val_accuracy: 0.7953\n",
      "Epoch 1143/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5100 - accuracy: 0.8693 - val_loss: 0.5986 - val_accuracy: 0.7953\n",
      "Epoch 1144/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.4991 - accuracy: 0.8733 - val_loss: 0.6316 - val_accuracy: 0.7717\n",
      "Epoch 1145/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5109 - accuracy: 0.8693 - val_loss: 0.5999 - val_accuracy: 0.7480\n",
      "Epoch 1146/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5057 - accuracy: 0.8673 - val_loss: 0.6205 - val_accuracy: 0.8268\n",
      "Epoch 1147/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5011 - accuracy: 0.8673 - val_loss: 0.5985 - val_accuracy: 0.7795\n",
      "Epoch 1148/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5047 - accuracy: 0.8554 - val_loss: 0.6117 - val_accuracy: 0.8268\n",
      "Epoch 1149/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4963 - accuracy: 0.8812 - val_loss: 0.6019 - val_accuracy: 0.8031\n",
      "Epoch 1150/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4974 - accuracy: 0.8673 - val_loss: 0.5933 - val_accuracy: 0.7717\n",
      "Epoch 1151/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.5124 - accuracy: 0.8634 - val_loss: 0.5977 - val_accuracy: 0.7638\n",
      "Epoch 1152/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.4977 - accuracy: 0.8653 - val_loss: 0.6119 - val_accuracy: 0.7717\n",
      "Epoch 1153/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.4957 - accuracy: 0.8634 - val_loss: 0.6266 - val_accuracy: 0.7874\n",
      "Epoch 1154/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4941 - accuracy: 0.8871 - val_loss: 0.5907 - val_accuracy: 0.8110\n",
      "Epoch 1155/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.4971 - accuracy: 0.8594 - val_loss: 0.7021 - val_accuracy: 0.7165\n",
      "Epoch 1156/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.5027 - accuracy: 0.8515 - val_loss: 0.5783 - val_accuracy: 0.7323\n",
      "Epoch 1157/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 221us/step - loss: 0.5012 - accuracy: 0.8495 - val_loss: 0.5806 - val_accuracy: 0.7953\n",
      "Epoch 1158/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5064 - accuracy: 0.8634 - val_loss: 0.6276 - val_accuracy: 0.7244\n",
      "Epoch 1159/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5037 - accuracy: 0.8475 - val_loss: 0.5814 - val_accuracy: 0.7480\n",
      "Epoch 1160/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.5084 - accuracy: 0.8416 - val_loss: 0.6071 - val_accuracy: 0.8425\n",
      "Epoch 1161/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4964 - accuracy: 0.8475 - val_loss: 0.6614 - val_accuracy: 0.7244\n",
      "Epoch 1162/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.5023 - accuracy: 0.8693 - val_loss: 0.6081 - val_accuracy: 0.7638\n",
      "Epoch 1163/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.4932 - accuracy: 0.8772 - val_loss: 0.6074 - val_accuracy: 0.7402\n",
      "Epoch 1164/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4957 - accuracy: 0.8733 - val_loss: 0.5844 - val_accuracy: 0.7795\n",
      "Epoch 1165/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4982 - accuracy: 0.8614 - val_loss: 0.6176 - val_accuracy: 0.7953\n",
      "Epoch 1166/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4784 - accuracy: 0.8733 - val_loss: 0.5882 - val_accuracy: 0.7953\n",
      "Epoch 1167/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4990 - accuracy: 0.8515 - val_loss: 0.6005 - val_accuracy: 0.7717\n",
      "Epoch 1168/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4882 - accuracy: 0.8634 - val_loss: 0.5962 - val_accuracy: 0.7559\n",
      "Epoch 1169/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4988 - accuracy: 0.8554 - val_loss: 0.5794 - val_accuracy: 0.7717\n",
      "Epoch 1170/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.5024 - accuracy: 0.8475 - val_loss: 0.6086 - val_accuracy: 0.7953\n",
      "Epoch 1171/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4960 - accuracy: 0.8713 - val_loss: 0.5648 - val_accuracy: 0.7480\n",
      "Epoch 1172/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.5027 - accuracy: 0.8376 - val_loss: 0.5900 - val_accuracy: 0.7953\n",
      "Epoch 1173/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.4947 - accuracy: 0.8535 - val_loss: 0.5887 - val_accuracy: 0.7717\n",
      "Epoch 1174/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4899 - accuracy: 0.8673 - val_loss: 0.6670 - val_accuracy: 0.7323\n",
      "Epoch 1175/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.4891 - accuracy: 0.8693 - val_loss: 0.5833 - val_accuracy: 0.7874\n",
      "Epoch 1176/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4797 - accuracy: 0.8832 - val_loss: 0.5891 - val_accuracy: 0.8110\n",
      "Epoch 1177/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.4855 - accuracy: 0.8713 - val_loss: 0.6566 - val_accuracy: 0.7165\n",
      "Epoch 1178/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4931 - accuracy: 0.8634 - val_loss: 0.6611 - val_accuracy: 0.7244\n",
      "Epoch 1179/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4994 - accuracy: 0.8337 - val_loss: 0.6489 - val_accuracy: 0.7244\n",
      "Epoch 1180/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4856 - accuracy: 0.8495 - val_loss: 0.5618 - val_accuracy: 0.7638\n",
      "Epoch 1181/2000\n",
      "505/505 [==============================] - 0s 226us/step - loss: 0.4761 - accuracy: 0.8832 - val_loss: 0.6047 - val_accuracy: 0.7559\n",
      "Epoch 1182/2000\n",
      "505/505 [==============================] - 0s 237us/step - loss: 0.4876 - accuracy: 0.8614 - val_loss: 0.6224 - val_accuracy: 0.7559\n",
      "Epoch 1183/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.4786 - accuracy: 0.8713 - val_loss: 0.6020 - val_accuracy: 0.7402\n",
      "Epoch 1184/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4858 - accuracy: 0.8812 - val_loss: 0.6580 - val_accuracy: 0.7008\n",
      "Epoch 1185/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4864 - accuracy: 0.8634 - val_loss: 0.5830 - val_accuracy: 0.8110\n",
      "Epoch 1186/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4746 - accuracy: 0.8693 - val_loss: 0.6146 - val_accuracy: 0.7717\n",
      "Epoch 1187/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4949 - accuracy: 0.8416 - val_loss: 0.5834 - val_accuracy: 0.7559\n",
      "Epoch 1188/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4829 - accuracy: 0.8614 - val_loss: 0.6918 - val_accuracy: 0.7008\n",
      "Epoch 1189/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.4732 - accuracy: 0.8832 - val_loss: 0.6273 - val_accuracy: 0.7323\n",
      "Epoch 1190/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.4886 - accuracy: 0.8614 - val_loss: 0.5620 - val_accuracy: 0.7717\n",
      "Epoch 1191/2000\n",
      "505/505 [==============================] - 0s 224us/step - loss: 0.4698 - accuracy: 0.8812 - val_loss: 0.6270 - val_accuracy: 0.7165\n",
      "Epoch 1192/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.4769 - accuracy: 0.8574 - val_loss: 0.5751 - val_accuracy: 0.8268\n",
      "Epoch 1193/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.4832 - accuracy: 0.8574 - val_loss: 0.5630 - val_accuracy: 0.7795\n",
      "Epoch 1194/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4830 - accuracy: 0.8792 - val_loss: 0.5894 - val_accuracy: 0.7874\n",
      "Epoch 1195/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4737 - accuracy: 0.8733 - val_loss: 0.5669 - val_accuracy: 0.8031\n",
      "Epoch 1196/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4820 - accuracy: 0.8396 - val_loss: 0.6211 - val_accuracy: 0.7717\n",
      "Epoch 1197/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4616 - accuracy: 0.8871 - val_loss: 0.5533 - val_accuracy: 0.7480\n",
      "Epoch 1198/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4857 - accuracy: 0.8495 - val_loss: 0.5888 - val_accuracy: 0.8110\n",
      "Epoch 1199/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4729 - accuracy: 0.8792 - val_loss: 0.5694 - val_accuracy: 0.7795\n",
      "Epoch 1200/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.4662 - accuracy: 0.8911 - val_loss: 0.6080 - val_accuracy: 0.7638\n",
      "Epoch 1201/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4858 - accuracy: 0.8416 - val_loss: 0.5946 - val_accuracy: 0.7953\n",
      "Epoch 1202/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4687 - accuracy: 0.8673 - val_loss: 0.6109 - val_accuracy: 0.7874\n",
      "Epoch 1203/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4788 - accuracy: 0.8535 - val_loss: 0.5982 - val_accuracy: 0.7717\n",
      "Epoch 1204/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4670 - accuracy: 0.8614 - val_loss: 0.6221 - val_accuracy: 0.7402\n",
      "Epoch 1205/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4543 - accuracy: 0.8931 - val_loss: 0.5961 - val_accuracy: 0.8031\n",
      "Epoch 1206/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.4816 - accuracy: 0.8574 - val_loss: 0.5837 - val_accuracy: 0.7953\n",
      "Epoch 1207/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4792 - accuracy: 0.8634 - val_loss: 0.5979 - val_accuracy: 0.8110\n",
      "Epoch 1208/2000\n",
      "505/505 [==============================] - 0s 252us/step - loss: 0.4783 - accuracy: 0.8337 - val_loss: 0.6445 - val_accuracy: 0.7087\n",
      "Epoch 1209/2000\n",
      "505/505 [==============================] - 0s 250us/step - loss: 0.4709 - accuracy: 0.8594 - val_loss: 0.6663 - val_accuracy: 0.7244\n",
      "Epoch 1210/2000\n",
      "505/505 [==============================] - 0s 276us/step - loss: 0.4761 - accuracy: 0.8653 - val_loss: 0.6562 - val_accuracy: 0.7165\n",
      "Epoch 1211/2000\n",
      "505/505 [==============================] - 0s 267us/step - loss: 0.4735 - accuracy: 0.8653 - val_loss: 0.6170 - val_accuracy: 0.7559\n",
      "Epoch 1212/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 225us/step - loss: 0.4610 - accuracy: 0.8752 - val_loss: 0.5705 - val_accuracy: 0.8346\n",
      "Epoch 1213/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4837 - accuracy: 0.8515 - val_loss: 0.6117 - val_accuracy: 0.7638\n",
      "Epoch 1214/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4725 - accuracy: 0.8653 - val_loss: 0.6564 - val_accuracy: 0.7087\n",
      "Epoch 1215/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4685 - accuracy: 0.8673 - val_loss: 0.5386 - val_accuracy: 0.7638\n",
      "Epoch 1216/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4751 - accuracy: 0.8515 - val_loss: 0.6146 - val_accuracy: 0.7638\n",
      "Epoch 1217/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.4521 - accuracy: 0.8851 - val_loss: 0.6007 - val_accuracy: 0.7795\n",
      "Epoch 1218/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.4629 - accuracy: 0.8653 - val_loss: 0.6331 - val_accuracy: 0.7559\n",
      "Epoch 1219/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4584 - accuracy: 0.8812 - val_loss: 0.5807 - val_accuracy: 0.8346\n",
      "Epoch 1220/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.4615 - accuracy: 0.8792 - val_loss: 0.5716 - val_accuracy: 0.8189\n",
      "Epoch 1221/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4580 - accuracy: 0.8752 - val_loss: 0.5342 - val_accuracy: 0.7795\n",
      "Epoch 1222/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.4579 - accuracy: 0.8772 - val_loss: 0.6590 - val_accuracy: 0.7087\n",
      "Epoch 1223/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4694 - accuracy: 0.8634 - val_loss: 0.5957 - val_accuracy: 0.7874\n",
      "Epoch 1224/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4638 - accuracy: 0.8792 - val_loss: 0.5448 - val_accuracy: 0.7638\n",
      "Epoch 1225/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4596 - accuracy: 0.8693 - val_loss: 0.5491 - val_accuracy: 0.7717\n",
      "Epoch 1226/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4587 - accuracy: 0.8812 - val_loss: 0.6022 - val_accuracy: 0.7559\n",
      "Epoch 1227/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.4540 - accuracy: 0.8752 - val_loss: 0.5903 - val_accuracy: 0.7953\n",
      "Epoch 1228/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4639 - accuracy: 0.8673 - val_loss: 0.5572 - val_accuracy: 0.8189\n",
      "Epoch 1229/2000\n",
      "505/505 [==============================] - 0s 233us/step - loss: 0.4490 - accuracy: 0.8950 - val_loss: 0.5356 - val_accuracy: 0.7717\n",
      "Epoch 1230/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4618 - accuracy: 0.8713 - val_loss: 0.5400 - val_accuracy: 0.7795\n",
      "Epoch 1231/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4485 - accuracy: 0.8733 - val_loss: 0.6104 - val_accuracy: 0.7323\n",
      "Epoch 1232/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.4560 - accuracy: 0.8693 - val_loss: 0.5640 - val_accuracy: 0.8189\n",
      "Epoch 1233/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4535 - accuracy: 0.8792 - val_loss: 0.5916 - val_accuracy: 0.7874\n",
      "Epoch 1234/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4537 - accuracy: 0.8673 - val_loss: 0.5423 - val_accuracy: 0.7953\n",
      "Epoch 1235/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.4532 - accuracy: 0.8634 - val_loss: 0.5679 - val_accuracy: 0.8110\n",
      "Epoch 1236/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4561 - accuracy: 0.8515 - val_loss: 0.6485 - val_accuracy: 0.7087\n",
      "Epoch 1237/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.4464 - accuracy: 0.8713 - val_loss: 0.5345 - val_accuracy: 0.7402\n",
      "Epoch 1238/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4557 - accuracy: 0.8792 - val_loss: 0.5544 - val_accuracy: 0.8031\n",
      "Epoch 1239/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.4620 - accuracy: 0.8673 - val_loss: 0.5399 - val_accuracy: 0.7638\n",
      "Epoch 1240/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4330 - accuracy: 0.8911 - val_loss: 0.5655 - val_accuracy: 0.8268\n",
      "Epoch 1241/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.4536 - accuracy: 0.8495 - val_loss: 0.6326 - val_accuracy: 0.7402\n",
      "Epoch 1242/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4394 - accuracy: 0.8713 - val_loss: 0.5491 - val_accuracy: 0.8189\n",
      "Epoch 1243/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.4445 - accuracy: 0.8832 - val_loss: 0.5897 - val_accuracy: 0.7795\n",
      "Epoch 1244/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.4514 - accuracy: 0.8713 - val_loss: 0.5556 - val_accuracy: 0.8110\n",
      "Epoch 1245/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4391 - accuracy: 0.8772 - val_loss: 0.5670 - val_accuracy: 0.8189\n",
      "Epoch 1246/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4454 - accuracy: 0.8752 - val_loss: 0.5436 - val_accuracy: 0.7953\n",
      "Epoch 1247/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4522 - accuracy: 0.8713 - val_loss: 0.5782 - val_accuracy: 0.8031\n",
      "Epoch 1248/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4516 - accuracy: 0.8614 - val_loss: 0.6108 - val_accuracy: 0.7323\n",
      "Epoch 1249/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4443 - accuracy: 0.8713 - val_loss: 0.5330 - val_accuracy: 0.8504\n",
      "Epoch 1250/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4462 - accuracy: 0.8653 - val_loss: 0.5289 - val_accuracy: 0.7638\n",
      "Epoch 1251/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4519 - accuracy: 0.8653 - val_loss: 0.5370 - val_accuracy: 0.8189\n",
      "Epoch 1252/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4420 - accuracy: 0.8832 - val_loss: 0.5410 - val_accuracy: 0.8268\n",
      "Epoch 1253/2000\n",
      "505/505 [==============================] - 0s 233us/step - loss: 0.4337 - accuracy: 0.8911 - val_loss: 0.6587 - val_accuracy: 0.7323\n",
      "Epoch 1254/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4445 - accuracy: 0.8634 - val_loss: 0.5327 - val_accuracy: 0.7953\n",
      "Epoch 1255/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.4323 - accuracy: 0.8733 - val_loss: 0.5366 - val_accuracy: 0.8583\n",
      "Epoch 1256/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.4230 - accuracy: 0.8871 - val_loss: 0.5167 - val_accuracy: 0.7559\n",
      "Epoch 1257/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4348 - accuracy: 0.8772 - val_loss: 0.5488 - val_accuracy: 0.8504\n",
      "Epoch 1258/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4438 - accuracy: 0.8574 - val_loss: 0.5722 - val_accuracy: 0.8110\n",
      "Epoch 1259/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4280 - accuracy: 0.8911 - val_loss: 0.6413 - val_accuracy: 0.7323\n",
      "Epoch 1260/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.4239 - accuracy: 0.8891 - val_loss: 0.5779 - val_accuracy: 0.8504\n",
      "Epoch 1261/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.4380 - accuracy: 0.8772 - val_loss: 0.5243 - val_accuracy: 0.7874\n",
      "Epoch 1262/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4466 - accuracy: 0.8574 - val_loss: 0.5185 - val_accuracy: 0.7638\n",
      "Epoch 1263/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.4333 - accuracy: 0.8673 - val_loss: 0.5954 - val_accuracy: 0.8031\n",
      "Epoch 1264/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.4233 - accuracy: 0.8673 - val_loss: 0.5448 - val_accuracy: 0.8110\n",
      "Epoch 1265/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4271 - accuracy: 0.8752 - val_loss: 0.5654 - val_accuracy: 0.7953\n",
      "Epoch 1266/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4370 - accuracy: 0.8693 - val_loss: 0.5825 - val_accuracy: 0.7323\n",
      "Epoch 1267/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 215us/step - loss: 0.4137 - accuracy: 0.8911 - val_loss: 0.5176 - val_accuracy: 0.7874\n",
      "Epoch 1268/2000\n",
      "505/505 [==============================] - 0s 263us/step - loss: 0.4425 - accuracy: 0.8653 - val_loss: 0.6331 - val_accuracy: 0.7165\n",
      "Epoch 1269/2000\n",
      "505/505 [==============================] - 0s 239us/step - loss: 0.4200 - accuracy: 0.8871 - val_loss: 0.5635 - val_accuracy: 0.8110\n",
      "Epoch 1270/2000\n",
      "505/505 [==============================] - 0s 239us/step - loss: 0.4357 - accuracy: 0.8713 - val_loss: 0.5371 - val_accuracy: 0.8268\n",
      "Epoch 1271/2000\n",
      "505/505 [==============================] - 0s 239us/step - loss: 0.4330 - accuracy: 0.8634 - val_loss: 0.5325 - val_accuracy: 0.7874\n",
      "Epoch 1272/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.4380 - accuracy: 0.8574 - val_loss: 0.5640 - val_accuracy: 0.7559\n",
      "Epoch 1273/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4254 - accuracy: 0.8792 - val_loss: 0.5161 - val_accuracy: 0.8031\n",
      "Epoch 1274/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4305 - accuracy: 0.8832 - val_loss: 0.6025 - val_accuracy: 0.7795\n",
      "Epoch 1275/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.4404 - accuracy: 0.8713 - val_loss: 0.5305 - val_accuracy: 0.8110\n",
      "Epoch 1276/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4265 - accuracy: 0.8832 - val_loss: 0.6082 - val_accuracy: 0.7480\n",
      "Epoch 1277/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4416 - accuracy: 0.8554 - val_loss: 0.5217 - val_accuracy: 0.7953\n",
      "Epoch 1278/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4167 - accuracy: 0.8891 - val_loss: 0.5754 - val_accuracy: 0.7795\n",
      "Epoch 1279/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.4160 - accuracy: 0.8752 - val_loss: 0.6036 - val_accuracy: 0.7638\n",
      "Epoch 1280/2000\n",
      "505/505 [==============================] - 0s 231us/step - loss: 0.4238 - accuracy: 0.8812 - val_loss: 0.5264 - val_accuracy: 0.7874\n",
      "Epoch 1281/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.4336 - accuracy: 0.8752 - val_loss: 0.5115 - val_accuracy: 0.8189\n",
      "Epoch 1282/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4418 - accuracy: 0.8475 - val_loss: 0.5736 - val_accuracy: 0.8110\n",
      "Epoch 1283/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4191 - accuracy: 0.8812 - val_loss: 0.6020 - val_accuracy: 0.7165\n",
      "Epoch 1284/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.4231 - accuracy: 0.8653 - val_loss: 0.5827 - val_accuracy: 0.7638\n",
      "Epoch 1285/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4241 - accuracy: 0.8752 - val_loss: 0.5345 - val_accuracy: 0.8110\n",
      "Epoch 1286/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4147 - accuracy: 0.8812 - val_loss: 0.5090 - val_accuracy: 0.7795\n",
      "Epoch 1287/2000\n",
      "505/505 [==============================] - 0s 229us/step - loss: 0.4204 - accuracy: 0.8733 - val_loss: 0.5525 - val_accuracy: 0.8425\n",
      "Epoch 1288/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4306 - accuracy: 0.8673 - val_loss: 0.6690 - val_accuracy: 0.7244\n",
      "Epoch 1289/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4244 - accuracy: 0.8653 - val_loss: 0.5072 - val_accuracy: 0.7953\n",
      "Epoch 1290/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4198 - accuracy: 0.8851 - val_loss: 0.5341 - val_accuracy: 0.7953\n",
      "Epoch 1291/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4065 - accuracy: 0.8871 - val_loss: 0.5094 - val_accuracy: 0.8189\n",
      "Epoch 1292/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4213 - accuracy: 0.8772 - val_loss: 0.5618 - val_accuracy: 0.7953\n",
      "Epoch 1293/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4103 - accuracy: 0.8832 - val_loss: 0.5367 - val_accuracy: 0.8583\n",
      "Epoch 1294/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4153 - accuracy: 0.8832 - val_loss: 0.5675 - val_accuracy: 0.7638\n",
      "Epoch 1295/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4038 - accuracy: 0.9109 - val_loss: 0.5215 - val_accuracy: 0.8346\n",
      "Epoch 1296/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.4088 - accuracy: 0.8891 - val_loss: 0.5816 - val_accuracy: 0.7559\n",
      "Epoch 1297/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4233 - accuracy: 0.8713 - val_loss: 0.5900 - val_accuracy: 0.7559\n",
      "Epoch 1298/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4070 - accuracy: 0.8891 - val_loss: 0.5752 - val_accuracy: 0.7402\n",
      "Epoch 1299/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.4162 - accuracy: 0.8653 - val_loss: 0.5550 - val_accuracy: 0.7953\n",
      "Epoch 1300/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4407 - accuracy: 0.8653 - val_loss: 0.5144 - val_accuracy: 0.8110\n",
      "Epoch 1301/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.4112 - accuracy: 0.8634 - val_loss: 0.5809 - val_accuracy: 0.7559\n",
      "Epoch 1302/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4297 - accuracy: 0.8653 - val_loss: 0.5279 - val_accuracy: 0.8268\n",
      "Epoch 1303/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3984 - accuracy: 0.8911 - val_loss: 0.5222 - val_accuracy: 0.8425\n",
      "Epoch 1304/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4089 - accuracy: 0.8911 - val_loss: 0.5153 - val_accuracy: 0.8031\n",
      "Epoch 1305/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4030 - accuracy: 0.8911 - val_loss: 0.5617 - val_accuracy: 0.7953\n",
      "Epoch 1306/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4261 - accuracy: 0.8337 - val_loss: 0.6044 - val_accuracy: 0.7323\n",
      "Epoch 1307/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4083 - accuracy: 0.8812 - val_loss: 0.4810 - val_accuracy: 0.8110\n",
      "Epoch 1308/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4054 - accuracy: 0.8693 - val_loss: 0.4919 - val_accuracy: 0.7953\n",
      "Epoch 1309/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.4146 - accuracy: 0.8851 - val_loss: 0.5318 - val_accuracy: 0.7874\n",
      "Epoch 1310/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3904 - accuracy: 0.8950 - val_loss: 0.4838 - val_accuracy: 0.7874\n",
      "Epoch 1311/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.4181 - accuracy: 0.8455 - val_loss: 0.5895 - val_accuracy: 0.7087\n",
      "Epoch 1312/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4043 - accuracy: 0.8653 - val_loss: 0.5206 - val_accuracy: 0.8031\n",
      "Epoch 1313/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.3984 - accuracy: 0.8713 - val_loss: 0.5118 - val_accuracy: 0.8268\n",
      "Epoch 1314/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4027 - accuracy: 0.8832 - val_loss: 0.5676 - val_accuracy: 0.7795\n",
      "Epoch 1315/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3914 - accuracy: 0.8871 - val_loss: 0.5988 - val_accuracy: 0.7402\n",
      "Epoch 1316/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3932 - accuracy: 0.8812 - val_loss: 0.5046 - val_accuracy: 0.7953\n",
      "Epoch 1317/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.4271 - accuracy: 0.8554 - val_loss: 0.6765 - val_accuracy: 0.7323\n",
      "Epoch 1318/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.4100 - accuracy: 0.8752 - val_loss: 0.6365 - val_accuracy: 0.7087\n",
      "Epoch 1319/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.4129 - accuracy: 0.8673 - val_loss: 0.7635 - val_accuracy: 0.7323\n",
      "Epoch 1320/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.4068 - accuracy: 0.8931 - val_loss: 0.4946 - val_accuracy: 0.8031\n",
      "Epoch 1321/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3971 - accuracy: 0.8931 - val_loss: 0.5357 - val_accuracy: 0.7874\n",
      "Epoch 1322/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 213us/step - loss: 0.4172 - accuracy: 0.8436 - val_loss: 0.5426 - val_accuracy: 0.7480\n",
      "Epoch 1323/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.4013 - accuracy: 0.8653 - val_loss: 0.4827 - val_accuracy: 0.7638\n",
      "Epoch 1324/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.4031 - accuracy: 0.8574 - val_loss: 0.5031 - val_accuracy: 0.8583\n",
      "Epoch 1325/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.4137 - accuracy: 0.8812 - val_loss: 0.4858 - val_accuracy: 0.7795\n",
      "Epoch 1326/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.3879 - accuracy: 0.8950 - val_loss: 0.5014 - val_accuracy: 0.8268\n",
      "Epoch 1327/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3854 - accuracy: 0.8891 - val_loss: 0.5048 - val_accuracy: 0.8583\n",
      "Epoch 1328/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3926 - accuracy: 0.8693 - val_loss: 0.4871 - val_accuracy: 0.7480\n",
      "Epoch 1329/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3887 - accuracy: 0.8832 - val_loss: 0.4852 - val_accuracy: 0.7953\n",
      "Epoch 1330/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3938 - accuracy: 0.8594 - val_loss: 0.5100 - val_accuracy: 0.8110\n",
      "Epoch 1331/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.3771 - accuracy: 0.8950 - val_loss: 0.4897 - val_accuracy: 0.8189\n",
      "Epoch 1332/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3851 - accuracy: 0.8871 - val_loss: 0.5823 - val_accuracy: 0.7638\n",
      "Epoch 1333/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3873 - accuracy: 0.8931 - val_loss: 0.5320 - val_accuracy: 0.7874\n",
      "Epoch 1334/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3844 - accuracy: 0.8891 - val_loss: 0.4638 - val_accuracy: 0.7717\n",
      "Epoch 1335/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3783 - accuracy: 0.9010 - val_loss: 0.4794 - val_accuracy: 0.8504\n",
      "Epoch 1336/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3795 - accuracy: 0.8911 - val_loss: 0.5298 - val_accuracy: 0.7874\n",
      "Epoch 1337/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3810 - accuracy: 0.8931 - val_loss: 0.5108 - val_accuracy: 0.8031\n",
      "Epoch 1338/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3992 - accuracy: 0.8614 - val_loss: 0.4791 - val_accuracy: 0.7717\n",
      "Epoch 1339/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3859 - accuracy: 0.8752 - val_loss: 0.5937 - val_accuracy: 0.7402\n",
      "Epoch 1340/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3901 - accuracy: 0.8812 - val_loss: 0.4850 - val_accuracy: 0.7638\n",
      "Epoch 1341/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3842 - accuracy: 0.8891 - val_loss: 0.4730 - val_accuracy: 0.7717\n",
      "Epoch 1342/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3813 - accuracy: 0.8752 - val_loss: 0.5131 - val_accuracy: 0.8268\n",
      "Epoch 1343/2000\n",
      "505/505 [==============================] - 0s 227us/step - loss: 0.3716 - accuracy: 0.8871 - val_loss: 0.4933 - val_accuracy: 0.8504\n",
      "Epoch 1344/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.3850 - accuracy: 0.8792 - val_loss: 0.5627 - val_accuracy: 0.7402\n",
      "Epoch 1345/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3718 - accuracy: 0.8970 - val_loss: 0.4849 - val_accuracy: 0.8346\n",
      "Epoch 1346/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3994 - accuracy: 0.8634 - val_loss: 0.5441 - val_accuracy: 0.7480\n",
      "Epoch 1347/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3709 - accuracy: 0.8851 - val_loss: 0.5052 - val_accuracy: 0.8346\n",
      "Epoch 1348/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3750 - accuracy: 0.8832 - val_loss: 0.4684 - val_accuracy: 0.8110\n",
      "Epoch 1349/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3855 - accuracy: 0.8713 - val_loss: 0.5256 - val_accuracy: 0.8189\n",
      "Epoch 1350/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3691 - accuracy: 0.8891 - val_loss: 0.4932 - val_accuracy: 0.8189\n",
      "Epoch 1351/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3851 - accuracy: 0.8733 - val_loss: 0.4943 - val_accuracy: 0.8110\n",
      "Epoch 1352/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3803 - accuracy: 0.8792 - val_loss: 0.4684 - val_accuracy: 0.8031\n",
      "Epoch 1353/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.3858 - accuracy: 0.8832 - val_loss: 0.4711 - val_accuracy: 0.7717\n",
      "Epoch 1354/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.3826 - accuracy: 0.8733 - val_loss: 0.6000 - val_accuracy: 0.7244\n",
      "Epoch 1355/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.3683 - accuracy: 0.8931 - val_loss: 0.5288 - val_accuracy: 0.8110\n",
      "Epoch 1356/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3708 - accuracy: 0.8911 - val_loss: 0.5332 - val_accuracy: 0.7402\n",
      "Epoch 1357/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3719 - accuracy: 0.8851 - val_loss: 0.5050 - val_accuracy: 0.8031\n",
      "Epoch 1358/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3684 - accuracy: 0.8871 - val_loss: 0.4654 - val_accuracy: 0.7638\n",
      "Epoch 1359/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3570 - accuracy: 0.8931 - val_loss: 0.5375 - val_accuracy: 0.7402\n",
      "Epoch 1360/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3769 - accuracy: 0.8772 - val_loss: 0.4757 - val_accuracy: 0.8346\n",
      "Epoch 1361/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3850 - accuracy: 0.8733 - val_loss: 0.4801 - val_accuracy: 0.8504\n",
      "Epoch 1362/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3662 - accuracy: 0.8970 - val_loss: 0.4717 - val_accuracy: 0.8189\n",
      "Epoch 1363/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3703 - accuracy: 0.8931 - val_loss: 0.4798 - val_accuracy: 0.8740\n",
      "Epoch 1364/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3776 - accuracy: 0.8693 - val_loss: 0.5527 - val_accuracy: 0.7402\n",
      "Epoch 1365/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3699 - accuracy: 0.8733 - val_loss: 0.5946 - val_accuracy: 0.7638\n",
      "Epoch 1366/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3622 - accuracy: 0.8990 - val_loss: 0.4919 - val_accuracy: 0.7480\n",
      "Epoch 1367/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3734 - accuracy: 0.8733 - val_loss: 0.4820 - val_accuracy: 0.8110\n",
      "Epoch 1368/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.3778 - accuracy: 0.8931 - val_loss: 0.4663 - val_accuracy: 0.7717\n",
      "Epoch 1369/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.3808 - accuracy: 0.8733 - val_loss: 0.4928 - val_accuracy: 0.7559\n",
      "Epoch 1370/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.3652 - accuracy: 0.8950 - val_loss: 0.5247 - val_accuracy: 0.7717\n",
      "Epoch 1371/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3781 - accuracy: 0.8733 - val_loss: 0.4795 - val_accuracy: 0.8661\n",
      "Epoch 1372/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3668 - accuracy: 0.8950 - val_loss: 0.4962 - val_accuracy: 0.8268\n",
      "Epoch 1373/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3628 - accuracy: 0.8832 - val_loss: 0.5695 - val_accuracy: 0.7402\n",
      "Epoch 1374/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3653 - accuracy: 0.8911 - val_loss: 0.4765 - val_accuracy: 0.7953\n",
      "Epoch 1375/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3546 - accuracy: 0.9030 - val_loss: 0.4819 - val_accuracy: 0.7953\n",
      "Epoch 1376/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3610 - accuracy: 0.8871 - val_loss: 0.4759 - val_accuracy: 0.7874\n",
      "Epoch 1377/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 217us/step - loss: 0.3515 - accuracy: 0.8931 - val_loss: 0.5488 - val_accuracy: 0.7795\n",
      "Epoch 1378/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3569 - accuracy: 0.8871 - val_loss: 0.4487 - val_accuracy: 0.8583\n",
      "Epoch 1379/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3856 - accuracy: 0.8574 - val_loss: 0.5974 - val_accuracy: 0.7087\n",
      "Epoch 1380/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3663 - accuracy: 0.8792 - val_loss: 0.5462 - val_accuracy: 0.7638\n",
      "Epoch 1381/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.3740 - accuracy: 0.8653 - val_loss: 0.5609 - val_accuracy: 0.7559\n",
      "Epoch 1382/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3646 - accuracy: 0.8693 - val_loss: 0.4584 - val_accuracy: 0.7717\n",
      "Epoch 1383/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3778 - accuracy: 0.8614 - val_loss: 0.5040 - val_accuracy: 0.7559\n",
      "Epoch 1384/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3704 - accuracy: 0.8772 - val_loss: 0.5934 - val_accuracy: 0.7402\n",
      "Epoch 1385/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3938 - accuracy: 0.8396 - val_loss: 0.5833 - val_accuracy: 0.7087\n",
      "Epoch 1386/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3642 - accuracy: 0.8614 - val_loss: 0.5183 - val_accuracy: 0.7874\n",
      "Epoch 1387/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.3646 - accuracy: 0.8911 - val_loss: 0.4655 - val_accuracy: 0.8031\n",
      "Epoch 1388/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3579 - accuracy: 0.8851 - val_loss: 0.5387 - val_accuracy: 0.7638\n",
      "Epoch 1389/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3598 - accuracy: 0.8950 - val_loss: 0.5317 - val_accuracy: 0.7638\n",
      "Epoch 1390/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3506 - accuracy: 0.8891 - val_loss: 0.5375 - val_accuracy: 0.7717\n",
      "Epoch 1391/2000\n",
      "505/505 [==============================] - 0s 231us/step - loss: 0.3539 - accuracy: 0.8871 - val_loss: 0.4692 - val_accuracy: 0.8189\n",
      "Epoch 1392/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3423 - accuracy: 0.9129 - val_loss: 0.4815 - val_accuracy: 0.7559\n",
      "Epoch 1393/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3526 - accuracy: 0.8931 - val_loss: 0.5851 - val_accuracy: 0.7244\n",
      "Epoch 1394/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3744 - accuracy: 0.8713 - val_loss: 0.4550 - val_accuracy: 0.8189\n",
      "Epoch 1395/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3500 - accuracy: 0.8990 - val_loss: 0.5324 - val_accuracy: 0.7874\n",
      "Epoch 1396/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3633 - accuracy: 0.8752 - val_loss: 0.5466 - val_accuracy: 0.7402\n",
      "Epoch 1397/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3480 - accuracy: 0.8911 - val_loss: 0.4390 - val_accuracy: 0.8031\n",
      "Epoch 1398/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3407 - accuracy: 0.9050 - val_loss: 0.4398 - val_accuracy: 0.7638\n",
      "Epoch 1399/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.3669 - accuracy: 0.8614 - val_loss: 0.4879 - val_accuracy: 0.7874\n",
      "Epoch 1400/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3569 - accuracy: 0.8812 - val_loss: 0.5852 - val_accuracy: 0.7165\n",
      "Epoch 1401/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3631 - accuracy: 0.8713 - val_loss: 0.6320 - val_accuracy: 0.7087\n",
      "Epoch 1402/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3649 - accuracy: 0.8950 - val_loss: 0.4866 - val_accuracy: 0.8031\n",
      "Epoch 1403/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3547 - accuracy: 0.8891 - val_loss: 0.5103 - val_accuracy: 0.8189\n",
      "Epoch 1404/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3454 - accuracy: 0.8970 - val_loss: 0.4403 - val_accuracy: 0.8110\n",
      "Epoch 1405/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3311 - accuracy: 0.8970 - val_loss: 0.4608 - val_accuracy: 0.8740\n",
      "Epoch 1406/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3620 - accuracy: 0.8772 - val_loss: 0.5359 - val_accuracy: 0.7638\n",
      "Epoch 1407/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3415 - accuracy: 0.8950 - val_loss: 0.4972 - val_accuracy: 0.8031\n",
      "Epoch 1408/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.3484 - accuracy: 0.9030 - val_loss: 0.5926 - val_accuracy: 0.7323\n",
      "Epoch 1409/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3356 - accuracy: 0.8931 - val_loss: 0.4530 - val_accuracy: 0.7795\n",
      "Epoch 1410/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.3519 - accuracy: 0.8792 - val_loss: 0.4847 - val_accuracy: 0.8110\n",
      "Epoch 1411/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3420 - accuracy: 0.8950 - val_loss: 0.4418 - val_accuracy: 0.8268\n",
      "Epoch 1412/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3459 - accuracy: 0.8772 - val_loss: 0.4935 - val_accuracy: 0.8268\n",
      "Epoch 1413/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3417 - accuracy: 0.8931 - val_loss: 0.5016 - val_accuracy: 0.7795\n",
      "Epoch 1414/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3337 - accuracy: 0.8812 - val_loss: 0.5609 - val_accuracy: 0.7402\n",
      "Epoch 1415/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3644 - accuracy: 0.8851 - val_loss: 0.4686 - val_accuracy: 0.8819\n",
      "Epoch 1416/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3369 - accuracy: 0.8891 - val_loss: 0.4993 - val_accuracy: 0.8031\n",
      "Epoch 1417/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3383 - accuracy: 0.9010 - val_loss: 0.4958 - val_accuracy: 0.7953\n",
      "Epoch 1418/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3600 - accuracy: 0.8594 - val_loss: 0.4489 - val_accuracy: 0.8504\n",
      "Epoch 1419/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3363 - accuracy: 0.8851 - val_loss: 0.4661 - val_accuracy: 0.8268\n",
      "Epoch 1420/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3374 - accuracy: 0.9030 - val_loss: 0.4623 - val_accuracy: 0.8346\n",
      "Epoch 1421/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3452 - accuracy: 0.8970 - val_loss: 0.4885 - val_accuracy: 0.8346\n",
      "Epoch 1422/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3457 - accuracy: 0.8812 - val_loss: 0.4776 - val_accuracy: 0.8504\n",
      "Epoch 1423/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3364 - accuracy: 0.8950 - val_loss: 0.5132 - val_accuracy: 0.7874\n",
      "Epoch 1424/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3267 - accuracy: 0.9010 - val_loss: 0.7086 - val_accuracy: 0.7087\n",
      "Epoch 1425/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3491 - accuracy: 0.8812 - val_loss: 0.4300 - val_accuracy: 0.8031\n",
      "Epoch 1426/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3421 - accuracy: 0.8733 - val_loss: 0.4820 - val_accuracy: 0.7559\n",
      "Epoch 1427/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3415 - accuracy: 0.8812 - val_loss: 0.5640 - val_accuracy: 0.7402\n",
      "Epoch 1428/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3328 - accuracy: 0.8950 - val_loss: 0.5707 - val_accuracy: 0.7165\n",
      "Epoch 1429/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3303 - accuracy: 0.8851 - val_loss: 0.4409 - val_accuracy: 0.8425\n",
      "Epoch 1430/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3393 - accuracy: 0.8950 - val_loss: 0.4313 - val_accuracy: 0.8268\n",
      "Epoch 1431/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3292 - accuracy: 0.8950 - val_loss: 0.4678 - val_accuracy: 0.8268\n",
      "Epoch 1432/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 215us/step - loss: 0.3239 - accuracy: 0.9010 - val_loss: 0.4512 - val_accuracy: 0.8189\n",
      "Epoch 1433/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3332 - accuracy: 0.9010 - val_loss: 0.5033 - val_accuracy: 0.7480\n",
      "Epoch 1434/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3278 - accuracy: 0.8911 - val_loss: 0.4273 - val_accuracy: 0.8031\n",
      "Epoch 1435/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3286 - accuracy: 0.9089 - val_loss: 0.4417 - val_accuracy: 0.8425\n",
      "Epoch 1436/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.3309 - accuracy: 0.8891 - val_loss: 0.5443 - val_accuracy: 0.7402\n",
      "Epoch 1437/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.3292 - accuracy: 0.8891 - val_loss: 0.4652 - val_accuracy: 0.8189\n",
      "Epoch 1438/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3190 - accuracy: 0.9069 - val_loss: 0.4782 - val_accuracy: 0.8189\n",
      "Epoch 1439/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3383 - accuracy: 0.8851 - val_loss: 0.5443 - val_accuracy: 0.7402\n",
      "Epoch 1440/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3261 - accuracy: 0.9050 - val_loss: 0.4624 - val_accuracy: 0.8504\n",
      "Epoch 1441/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3475 - accuracy: 0.8733 - val_loss: 0.4166 - val_accuracy: 0.7480\n",
      "Epoch 1442/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3201 - accuracy: 0.9089 - val_loss: 0.4743 - val_accuracy: 0.8031\n",
      "Epoch 1443/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3203 - accuracy: 0.8970 - val_loss: 0.4446 - val_accuracy: 0.8346\n",
      "Epoch 1444/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3060 - accuracy: 0.9069 - val_loss: 0.4350 - val_accuracy: 0.8740\n",
      "Epoch 1445/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3134 - accuracy: 0.9129 - val_loss: 0.4395 - val_accuracy: 0.7638\n",
      "Epoch 1446/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3451 - accuracy: 0.8693 - val_loss: 0.5003 - val_accuracy: 0.7638\n",
      "Epoch 1447/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3362 - accuracy: 0.8792 - val_loss: 0.4352 - val_accuracy: 0.8898\n",
      "Epoch 1448/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3178 - accuracy: 0.9010 - val_loss: 0.4562 - val_accuracy: 0.7559\n",
      "Epoch 1449/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3200 - accuracy: 0.8931 - val_loss: 0.4407 - val_accuracy: 0.8268\n",
      "Epoch 1450/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3326 - accuracy: 0.8812 - val_loss: 0.5033 - val_accuracy: 0.7244\n",
      "Epoch 1451/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3162 - accuracy: 0.9089 - val_loss: 0.4124 - val_accuracy: 0.8189\n",
      "Epoch 1452/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3411 - accuracy: 0.8832 - val_loss: 0.4277 - val_accuracy: 0.8110\n",
      "Epoch 1453/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3381 - accuracy: 0.8673 - val_loss: 0.5838 - val_accuracy: 0.7323\n",
      "Epoch 1454/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3175 - accuracy: 0.9010 - val_loss: 0.4932 - val_accuracy: 0.7402\n",
      "Epoch 1455/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3130 - accuracy: 0.9030 - val_loss: 0.4205 - val_accuracy: 0.8425\n",
      "Epoch 1456/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3116 - accuracy: 0.8970 - val_loss: 0.4524 - val_accuracy: 0.7874\n",
      "Epoch 1457/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3265 - accuracy: 0.8911 - val_loss: 0.4243 - val_accuracy: 0.8031\n",
      "Epoch 1458/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3399 - accuracy: 0.8812 - val_loss: 0.4284 - val_accuracy: 0.8346\n",
      "Epoch 1459/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3334 - accuracy: 0.8713 - val_loss: 0.4575 - val_accuracy: 0.8031\n",
      "Epoch 1460/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3458 - accuracy: 0.8614 - val_loss: 0.4516 - val_accuracy: 0.8346\n",
      "Epoch 1461/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3292 - accuracy: 0.8693 - val_loss: 0.4561 - val_accuracy: 0.8110\n",
      "Epoch 1462/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3092 - accuracy: 0.9030 - val_loss: 0.4522 - val_accuracy: 0.8031\n",
      "Epoch 1463/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3121 - accuracy: 0.8990 - val_loss: 0.4490 - val_accuracy: 0.8268\n",
      "Epoch 1464/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.3030 - accuracy: 0.9129 - val_loss: 0.4345 - val_accuracy: 0.7638\n",
      "Epoch 1465/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3187 - accuracy: 0.8911 - val_loss: 0.5063 - val_accuracy: 0.7717\n",
      "Epoch 1466/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.3249 - accuracy: 0.8891 - val_loss: 0.4527 - val_accuracy: 0.7559\n",
      "Epoch 1467/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3364 - accuracy: 0.8792 - val_loss: 0.6132 - val_accuracy: 0.7402\n",
      "Epoch 1468/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3276 - accuracy: 0.8752 - val_loss: 0.5093 - val_accuracy: 0.7638\n",
      "Epoch 1469/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3131 - accuracy: 0.9129 - val_loss: 0.4440 - val_accuracy: 0.8425\n",
      "Epoch 1470/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3232 - accuracy: 0.8832 - val_loss: 0.4053 - val_accuracy: 0.7953\n",
      "Epoch 1471/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3047 - accuracy: 0.9168 - val_loss: 0.4060 - val_accuracy: 0.8346\n",
      "Epoch 1472/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3040 - accuracy: 0.9109 - val_loss: 0.4761 - val_accuracy: 0.7874\n",
      "Epoch 1473/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2970 - accuracy: 0.9030 - val_loss: 0.5263 - val_accuracy: 0.7480\n",
      "Epoch 1474/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3277 - accuracy: 0.8832 - val_loss: 0.4594 - val_accuracy: 0.7638\n",
      "Epoch 1475/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3117 - accuracy: 0.8891 - val_loss: 0.4585 - val_accuracy: 0.7795\n",
      "Epoch 1476/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3475 - accuracy: 0.8594 - val_loss: 0.5985 - val_accuracy: 0.7165\n",
      "Epoch 1477/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.3132 - accuracy: 0.8990 - val_loss: 0.4842 - val_accuracy: 0.8189\n",
      "Epoch 1478/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.3208 - accuracy: 0.8931 - val_loss: 0.4411 - val_accuracy: 0.8504\n",
      "Epoch 1479/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3180 - accuracy: 0.9069 - val_loss: 0.4856 - val_accuracy: 0.7874\n",
      "Epoch 1480/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3186 - accuracy: 0.8911 - val_loss: 0.5139 - val_accuracy: 0.7480\n",
      "Epoch 1481/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3146 - accuracy: 0.8950 - val_loss: 0.4079 - val_accuracy: 0.7795\n",
      "Epoch 1482/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3228 - accuracy: 0.8733 - val_loss: 0.4214 - val_accuracy: 0.8504\n",
      "Epoch 1483/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3215 - accuracy: 0.8891 - val_loss: 0.4169 - val_accuracy: 0.8425\n",
      "Epoch 1484/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3139 - accuracy: 0.8970 - val_loss: 0.4927 - val_accuracy: 0.7953\n",
      "Epoch 1485/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3234 - accuracy: 0.8713 - val_loss: 0.4359 - val_accuracy: 0.7795\n",
      "Epoch 1486/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3297 - accuracy: 0.8792 - val_loss: 0.4264 - val_accuracy: 0.8583\n",
      "Epoch 1487/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 215us/step - loss: 0.2965 - accuracy: 0.9109 - val_loss: 0.4600 - val_accuracy: 0.8268\n",
      "Epoch 1488/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.3647 - accuracy: 0.8574 - val_loss: 0.5253 - val_accuracy: 0.7402\n",
      "Epoch 1489/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3154 - accuracy: 0.8891 - val_loss: 0.4399 - val_accuracy: 0.7638\n",
      "Epoch 1490/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.3110 - accuracy: 0.8911 - val_loss: 0.4268 - val_accuracy: 0.8110\n",
      "Epoch 1491/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.3152 - accuracy: 0.8950 - val_loss: 0.4142 - val_accuracy: 0.8661\n",
      "Epoch 1492/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3358 - accuracy: 0.8653 - val_loss: 0.4877 - val_accuracy: 0.7638\n",
      "Epoch 1493/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2977 - accuracy: 0.9109 - val_loss: 0.4416 - val_accuracy: 0.8110\n",
      "Epoch 1494/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3126 - accuracy: 0.8990 - val_loss: 0.4130 - val_accuracy: 0.8110\n",
      "Epoch 1495/2000\n",
      "505/505 [==============================] - 0s 208us/step - loss: 0.3178 - accuracy: 0.8693 - val_loss: 0.4407 - val_accuracy: 0.8583\n",
      "Epoch 1496/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.3264 - accuracy: 0.8752 - val_loss: 0.3970 - val_accuracy: 0.7717\n",
      "Epoch 1497/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.3256 - accuracy: 0.8812 - val_loss: 0.5447 - val_accuracy: 0.7559\n",
      "Epoch 1498/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3066 - accuracy: 0.8772 - val_loss: 0.4878 - val_accuracy: 0.7480\n",
      "Epoch 1499/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2945 - accuracy: 0.9050 - val_loss: 0.4184 - val_accuracy: 0.8346\n",
      "Epoch 1500/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3082 - accuracy: 0.8931 - val_loss: 0.4503 - val_accuracy: 0.8189\n",
      "Epoch 1501/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2926 - accuracy: 0.9030 - val_loss: 0.4246 - val_accuracy: 0.8583\n",
      "Epoch 1502/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2976 - accuracy: 0.9010 - val_loss: 0.4039 - val_accuracy: 0.8583\n",
      "Epoch 1503/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3065 - accuracy: 0.8832 - val_loss: 0.5604 - val_accuracy: 0.7402\n",
      "Epoch 1504/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3013 - accuracy: 0.8911 - val_loss: 0.5011 - val_accuracy: 0.7717\n",
      "Epoch 1505/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3050 - accuracy: 0.8772 - val_loss: 0.5524 - val_accuracy: 0.7480\n",
      "Epoch 1506/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3041 - accuracy: 0.8891 - val_loss: 0.4267 - val_accuracy: 0.8268\n",
      "Epoch 1507/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.3368 - accuracy: 0.8673 - val_loss: 0.4012 - val_accuracy: 0.8504\n",
      "Epoch 1508/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2960 - accuracy: 0.9030 - val_loss: 0.4176 - val_accuracy: 0.8031\n",
      "Epoch 1509/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.3043 - accuracy: 0.8871 - val_loss: 0.5418 - val_accuracy: 0.7480\n",
      "Epoch 1510/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2954 - accuracy: 0.9129 - val_loss: 0.4271 - val_accuracy: 0.8661\n",
      "Epoch 1511/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.3097 - accuracy: 0.8891 - val_loss: 0.4005 - val_accuracy: 0.8268\n",
      "Epoch 1512/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3019 - accuracy: 0.8950 - val_loss: 0.5441 - val_accuracy: 0.7402\n",
      "Epoch 1513/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2952 - accuracy: 0.8812 - val_loss: 0.4912 - val_accuracy: 0.7874\n",
      "Epoch 1514/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2891 - accuracy: 0.9069 - val_loss: 0.4062 - val_accuracy: 0.8268\n",
      "Epoch 1515/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3196 - accuracy: 0.8792 - val_loss: 0.4262 - val_accuracy: 0.8819\n",
      "Epoch 1516/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2977 - accuracy: 0.8891 - val_loss: 0.4351 - val_accuracy: 0.8110\n",
      "Epoch 1517/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2944 - accuracy: 0.8950 - val_loss: 0.4103 - val_accuracy: 0.8740\n",
      "Epoch 1518/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3073 - accuracy: 0.8950 - val_loss: 0.4920 - val_accuracy: 0.7638\n",
      "Epoch 1519/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2885 - accuracy: 0.9089 - val_loss: 0.3914 - val_accuracy: 0.8583\n",
      "Epoch 1520/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3152 - accuracy: 0.8871 - val_loss: 0.5001 - val_accuracy: 0.7638\n",
      "Epoch 1521/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2845 - accuracy: 0.9010 - val_loss: 0.4129 - val_accuracy: 0.7795\n",
      "Epoch 1522/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3164 - accuracy: 0.8634 - val_loss: 0.4733 - val_accuracy: 0.8110\n",
      "Epoch 1523/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2983 - accuracy: 0.9030 - val_loss: 0.4138 - val_accuracy: 0.7874\n",
      "Epoch 1524/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3115 - accuracy: 0.8792 - val_loss: 0.4079 - val_accuracy: 0.8583\n",
      "Epoch 1525/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2934 - accuracy: 0.8950 - val_loss: 0.3917 - val_accuracy: 0.8425\n",
      "Epoch 1526/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3008 - accuracy: 0.9050 - val_loss: 0.4414 - val_accuracy: 0.7953\n",
      "Epoch 1527/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2916 - accuracy: 0.9010 - val_loss: 0.4685 - val_accuracy: 0.7874\n",
      "Epoch 1528/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2826 - accuracy: 0.9010 - val_loss: 0.4824 - val_accuracy: 0.7795\n",
      "Epoch 1529/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.3011 - accuracy: 0.8931 - val_loss: 0.4938 - val_accuracy: 0.8189\n",
      "Epoch 1530/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3169 - accuracy: 0.8653 - val_loss: 0.4702 - val_accuracy: 0.7953\n",
      "Epoch 1531/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2932 - accuracy: 0.8970 - val_loss: 0.4549 - val_accuracy: 0.7717\n",
      "Epoch 1532/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2836 - accuracy: 0.9069 - val_loss: 0.5610 - val_accuracy: 0.7402\n",
      "Epoch 1533/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3131 - accuracy: 0.8832 - val_loss: 0.3940 - val_accuracy: 0.7638\n",
      "Epoch 1534/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2899 - accuracy: 0.8990 - val_loss: 0.4864 - val_accuracy: 0.7244\n",
      "Epoch 1535/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3193 - accuracy: 0.8713 - val_loss: 0.4492 - val_accuracy: 0.8189\n",
      "Epoch 1536/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2988 - accuracy: 0.8950 - val_loss: 0.4764 - val_accuracy: 0.7480\n",
      "Epoch 1537/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3076 - accuracy: 0.8772 - val_loss: 0.4665 - val_accuracy: 0.7874\n",
      "Epoch 1538/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.3035 - accuracy: 0.8911 - val_loss: 0.4702 - val_accuracy: 0.7717\n",
      "Epoch 1539/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2708 - accuracy: 0.9149 - val_loss: 0.3713 - val_accuracy: 0.8504\n",
      "Epoch 1540/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2909 - accuracy: 0.8970 - val_loss: 0.4191 - val_accuracy: 0.7638\n",
      "Epoch 1541/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.3116 - accuracy: 0.8792 - val_loss: 0.5197 - val_accuracy: 0.7480\n",
      "Epoch 1542/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 218us/step - loss: 0.3129 - accuracy: 0.8653 - val_loss: 0.4816 - val_accuracy: 0.7559\n",
      "Epoch 1543/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2881 - accuracy: 0.8950 - val_loss: 0.5040 - val_accuracy: 0.7402\n",
      "Epoch 1544/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2768 - accuracy: 0.9089 - val_loss: 0.5471 - val_accuracy: 0.7244\n",
      "Epoch 1545/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2835 - accuracy: 0.9149 - val_loss: 0.4278 - val_accuracy: 0.7795\n",
      "Epoch 1546/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2958 - accuracy: 0.8950 - val_loss: 0.4077 - val_accuracy: 0.8504\n",
      "Epoch 1547/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2894 - accuracy: 0.8950 - val_loss: 0.4072 - val_accuracy: 0.8346\n",
      "Epoch 1548/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2788 - accuracy: 0.9149 - val_loss: 0.3939 - val_accuracy: 0.8898\n",
      "Epoch 1549/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.3088 - accuracy: 0.8792 - val_loss: 0.4100 - val_accuracy: 0.8898\n",
      "Epoch 1550/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2771 - accuracy: 0.9149 - val_loss: 0.3953 - val_accuracy: 0.7717\n",
      "Epoch 1551/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2815 - accuracy: 0.9030 - val_loss: 0.3819 - val_accuracy: 0.8268\n",
      "Epoch 1552/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2838 - accuracy: 0.9050 - val_loss: 0.4062 - val_accuracy: 0.8583\n",
      "Epoch 1553/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2857 - accuracy: 0.9010 - val_loss: 0.4914 - val_accuracy: 0.7874\n",
      "Epoch 1554/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2720 - accuracy: 0.9149 - val_loss: 0.4034 - val_accuracy: 0.8346\n",
      "Epoch 1555/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3200 - accuracy: 0.8733 - val_loss: 0.3913 - val_accuracy: 0.7717\n",
      "Epoch 1556/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.3029 - accuracy: 0.8752 - val_loss: 0.4275 - val_accuracy: 0.8425\n",
      "Epoch 1557/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2709 - accuracy: 0.9149 - val_loss: 0.5448 - val_accuracy: 0.7323\n",
      "Epoch 1558/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2890 - accuracy: 0.8970 - val_loss: 0.3950 - val_accuracy: 0.8661\n",
      "Epoch 1559/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2748 - accuracy: 0.9267 - val_loss: 0.4151 - val_accuracy: 0.7953\n",
      "Epoch 1560/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.3109 - accuracy: 0.8713 - val_loss: 0.4311 - val_accuracy: 0.8425\n",
      "Epoch 1561/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2689 - accuracy: 0.9168 - val_loss: 0.4027 - val_accuracy: 0.7717\n",
      "Epoch 1562/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2741 - accuracy: 0.9149 - val_loss: 0.3807 - val_accuracy: 0.8425\n",
      "Epoch 1563/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2977 - accuracy: 0.8792 - val_loss: 0.4328 - val_accuracy: 0.8110\n",
      "Epoch 1564/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2748 - accuracy: 0.9050 - val_loss: 0.4556 - val_accuracy: 0.8189\n",
      "Epoch 1565/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2808 - accuracy: 0.8950 - val_loss: 0.4137 - val_accuracy: 0.8976\n",
      "Epoch 1566/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2755 - accuracy: 0.8990 - val_loss: 0.4003 - val_accuracy: 0.7795\n",
      "Epoch 1567/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2664 - accuracy: 0.9109 - val_loss: 0.4052 - val_accuracy: 0.7874\n",
      "Epoch 1568/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2761 - accuracy: 0.8970 - val_loss: 0.4230 - val_accuracy: 0.8110\n",
      "Epoch 1569/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2932 - accuracy: 0.8792 - val_loss: 0.3919 - val_accuracy: 0.7874\n",
      "Epoch 1570/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2610 - accuracy: 0.9267 - val_loss: 0.3714 - val_accuracy: 0.8110\n",
      "Epoch 1571/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2935 - accuracy: 0.9050 - val_loss: 0.3885 - val_accuracy: 0.8189\n",
      "Epoch 1572/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2869 - accuracy: 0.8970 - val_loss: 0.5262 - val_accuracy: 0.7087\n",
      "Epoch 1573/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2881 - accuracy: 0.8812 - val_loss: 0.5516 - val_accuracy: 0.7402\n",
      "Epoch 1574/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2924 - accuracy: 0.8990 - val_loss: 0.4907 - val_accuracy: 0.7402\n",
      "Epoch 1575/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2666 - accuracy: 0.9129 - val_loss: 0.3684 - val_accuracy: 0.8268\n",
      "Epoch 1576/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2803 - accuracy: 0.8990 - val_loss: 0.4321 - val_accuracy: 0.8425\n",
      "Epoch 1577/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2731 - accuracy: 0.9089 - val_loss: 0.4984 - val_accuracy: 0.7717\n",
      "Epoch 1578/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2812 - accuracy: 0.8911 - val_loss: 0.3736 - val_accuracy: 0.8110\n",
      "Epoch 1579/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2781 - accuracy: 0.9089 - val_loss: 0.4348 - val_accuracy: 0.7717\n",
      "Epoch 1580/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2926 - accuracy: 0.8931 - val_loss: 0.3703 - val_accuracy: 0.8189\n",
      "Epoch 1581/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2959 - accuracy: 0.8832 - val_loss: 0.3860 - val_accuracy: 0.7795\n",
      "Epoch 1582/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2796 - accuracy: 0.8871 - val_loss: 0.3951 - val_accuracy: 0.8583\n",
      "Epoch 1583/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2797 - accuracy: 0.8970 - val_loss: 0.4837 - val_accuracy: 0.7717\n",
      "Epoch 1584/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2820 - accuracy: 0.8752 - val_loss: 0.3961 - val_accuracy: 0.7795\n",
      "Epoch 1585/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2707 - accuracy: 0.9089 - val_loss: 0.3987 - val_accuracy: 0.8346\n",
      "Epoch 1586/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2968 - accuracy: 0.8693 - val_loss: 0.3814 - val_accuracy: 0.8189\n",
      "Epoch 1587/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2714 - accuracy: 0.9129 - val_loss: 0.3835 - val_accuracy: 0.8583\n",
      "Epoch 1588/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2650 - accuracy: 0.8990 - val_loss: 0.3698 - val_accuracy: 0.8110\n",
      "Epoch 1589/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2843 - accuracy: 0.9010 - val_loss: 0.3938 - val_accuracy: 0.8110\n",
      "Epoch 1590/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2689 - accuracy: 0.9069 - val_loss: 0.4464 - val_accuracy: 0.7717\n",
      "Epoch 1591/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2565 - accuracy: 0.9109 - val_loss: 0.3797 - val_accuracy: 0.8189\n",
      "Epoch 1592/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2631 - accuracy: 0.8950 - val_loss: 0.3837 - val_accuracy: 0.8504\n",
      "Epoch 1593/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2616 - accuracy: 0.9188 - val_loss: 0.3667 - val_accuracy: 0.7874\n",
      "Epoch 1594/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2783 - accuracy: 0.9050 - val_loss: 0.4163 - val_accuracy: 0.8268\n",
      "Epoch 1595/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2959 - accuracy: 0.8792 - val_loss: 0.4353 - val_accuracy: 0.8189\n",
      "Epoch 1596/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2625 - accuracy: 0.9109 - val_loss: 0.3786 - val_accuracy: 0.7795\n",
      "Epoch 1597/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 215us/step - loss: 0.2729 - accuracy: 0.8990 - val_loss: 0.4610 - val_accuracy: 0.7638\n",
      "Epoch 1598/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2765 - accuracy: 0.8950 - val_loss: 0.4829 - val_accuracy: 0.7402\n",
      "Epoch 1599/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2586 - accuracy: 0.9030 - val_loss: 0.3649 - val_accuracy: 0.8661\n",
      "Epoch 1600/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2747 - accuracy: 0.9050 - val_loss: 0.3649 - val_accuracy: 0.8819\n",
      "Epoch 1601/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2594 - accuracy: 0.9168 - val_loss: 0.3855 - val_accuracy: 0.8661\n",
      "Epoch 1602/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2664 - accuracy: 0.9030 - val_loss: 0.4030 - val_accuracy: 0.8661\n",
      "Epoch 1603/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2594 - accuracy: 0.9208 - val_loss: 0.5509 - val_accuracy: 0.7480\n",
      "Epoch 1604/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2688 - accuracy: 0.8970 - val_loss: 0.4175 - val_accuracy: 0.8110\n",
      "Epoch 1605/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2604 - accuracy: 0.8950 - val_loss: 0.4126 - val_accuracy: 0.8189\n",
      "Epoch 1606/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2542 - accuracy: 0.9188 - val_loss: 0.4211 - val_accuracy: 0.7953\n",
      "Epoch 1607/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2611 - accuracy: 0.9050 - val_loss: 0.4213 - val_accuracy: 0.7638\n",
      "Epoch 1608/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2654 - accuracy: 0.9069 - val_loss: 0.3782 - val_accuracy: 0.8583\n",
      "Epoch 1609/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2768 - accuracy: 0.8832 - val_loss: 0.4440 - val_accuracy: 0.8268\n",
      "Epoch 1610/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2853 - accuracy: 0.8792 - val_loss: 0.3589 - val_accuracy: 0.8189\n",
      "Epoch 1611/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2766 - accuracy: 0.8891 - val_loss: 0.5008 - val_accuracy: 0.7402\n",
      "Epoch 1612/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.2873 - accuracy: 0.8713 - val_loss: 0.3640 - val_accuracy: 0.8504\n",
      "Epoch 1613/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2550 - accuracy: 0.9109 - val_loss: 0.3865 - val_accuracy: 0.8425\n",
      "Epoch 1614/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2727 - accuracy: 0.9050 - val_loss: 0.4006 - val_accuracy: 0.8031\n",
      "Epoch 1615/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2618 - accuracy: 0.9050 - val_loss: 0.4117 - val_accuracy: 0.8504\n",
      "Epoch 1616/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.2628 - accuracy: 0.8990 - val_loss: 0.3557 - val_accuracy: 0.8583\n",
      "Epoch 1617/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2622 - accuracy: 0.9010 - val_loss: 0.3616 - val_accuracy: 0.8110\n",
      "Epoch 1618/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2776 - accuracy: 0.8891 - val_loss: 0.4452 - val_accuracy: 0.8110\n",
      "Epoch 1619/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2778 - accuracy: 0.8891 - val_loss: 0.3916 - val_accuracy: 0.8819\n",
      "Epoch 1620/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2597 - accuracy: 0.9109 - val_loss: 0.4228 - val_accuracy: 0.8110\n",
      "Epoch 1621/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2688 - accuracy: 0.8950 - val_loss: 0.3944 - val_accuracy: 0.8189\n",
      "Epoch 1622/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2556 - accuracy: 0.9267 - val_loss: 0.3898 - val_accuracy: 0.8504\n",
      "Epoch 1623/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2558 - accuracy: 0.9089 - val_loss: 0.3583 - val_accuracy: 0.8504\n",
      "Epoch 1624/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2567 - accuracy: 0.8970 - val_loss: 0.3838 - val_accuracy: 0.8976\n",
      "Epoch 1625/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2490 - accuracy: 0.9129 - val_loss: 0.4728 - val_accuracy: 0.7402\n",
      "Epoch 1626/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2689 - accuracy: 0.8812 - val_loss: 0.3810 - val_accuracy: 0.8661\n",
      "Epoch 1627/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2616 - accuracy: 0.9030 - val_loss: 0.4993 - val_accuracy: 0.7480\n",
      "Epoch 1628/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2670 - accuracy: 0.9168 - val_loss: 0.4071 - val_accuracy: 0.8504\n",
      "Epoch 1629/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2644 - accuracy: 0.9010 - val_loss: 0.3769 - val_accuracy: 0.8031\n",
      "Epoch 1630/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2674 - accuracy: 0.8832 - val_loss: 0.3983 - val_accuracy: 0.8110\n",
      "Epoch 1631/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2789 - accuracy: 0.8832 - val_loss: 0.3775 - val_accuracy: 0.8583\n",
      "Epoch 1632/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2553 - accuracy: 0.9010 - val_loss: 0.4313 - val_accuracy: 0.7953\n",
      "Epoch 1633/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2797 - accuracy: 0.8970 - val_loss: 0.4007 - val_accuracy: 0.8583\n",
      "Epoch 1634/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2728 - accuracy: 0.8950 - val_loss: 0.4355 - val_accuracy: 0.7953\n",
      "Epoch 1635/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2596 - accuracy: 0.9010 - val_loss: 0.3648 - val_accuracy: 0.8583\n",
      "Epoch 1636/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2624 - accuracy: 0.9050 - val_loss: 0.4897 - val_accuracy: 0.7559\n",
      "Epoch 1637/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2949 - accuracy: 0.8673 - val_loss: 0.3517 - val_accuracy: 0.8740\n",
      "Epoch 1638/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2669 - accuracy: 0.8970 - val_loss: 0.3730 - val_accuracy: 0.8661\n",
      "Epoch 1639/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2621 - accuracy: 0.8990 - val_loss: 0.4050 - val_accuracy: 0.8504\n",
      "Epoch 1640/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2505 - accuracy: 0.9208 - val_loss: 0.5255 - val_accuracy: 0.7402\n",
      "Epoch 1641/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2720 - accuracy: 0.8911 - val_loss: 0.4032 - val_accuracy: 0.7953\n",
      "Epoch 1642/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2479 - accuracy: 0.9149 - val_loss: 0.3573 - val_accuracy: 0.7874\n",
      "Epoch 1643/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2660 - accuracy: 0.8990 - val_loss: 0.4478 - val_accuracy: 0.7717\n",
      "Epoch 1644/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2569 - accuracy: 0.9030 - val_loss: 0.3937 - val_accuracy: 0.8661\n",
      "Epoch 1645/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2568 - accuracy: 0.8931 - val_loss: 0.3579 - val_accuracy: 0.8661\n",
      "Epoch 1646/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.2667 - accuracy: 0.8792 - val_loss: 0.3660 - val_accuracy: 0.8819\n",
      "Epoch 1647/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2507 - accuracy: 0.9267 - val_loss: 0.3556 - val_accuracy: 0.8661\n",
      "Epoch 1648/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2576 - accuracy: 0.9129 - val_loss: 0.3717 - val_accuracy: 0.8425\n",
      "Epoch 1649/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2701 - accuracy: 0.8891 - val_loss: 0.4320 - val_accuracy: 0.8189\n",
      "Epoch 1650/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2570 - accuracy: 0.8950 - val_loss: 0.4620 - val_accuracy: 0.7480\n",
      "Epoch 1651/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2573 - accuracy: 0.9069 - val_loss: 0.3755 - val_accuracy: 0.8583\n",
      "Epoch 1652/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 219us/step - loss: 0.2682 - accuracy: 0.9069 - val_loss: 0.3866 - val_accuracy: 0.8661\n",
      "Epoch 1653/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2631 - accuracy: 0.8931 - val_loss: 0.4009 - val_accuracy: 0.8425\n",
      "Epoch 1654/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2669 - accuracy: 0.8931 - val_loss: 0.3786 - val_accuracy: 0.8425\n",
      "Epoch 1655/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2588 - accuracy: 0.9010 - val_loss: 0.4002 - val_accuracy: 0.8031\n",
      "Epoch 1656/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2573 - accuracy: 0.9129 - val_loss: 0.3909 - val_accuracy: 0.8740\n",
      "Epoch 1657/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2470 - accuracy: 0.9109 - val_loss: 0.4076 - val_accuracy: 0.8425\n",
      "Epoch 1658/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2603 - accuracy: 0.9050 - val_loss: 0.5844 - val_accuracy: 0.7165\n",
      "Epoch 1659/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2842 - accuracy: 0.8713 - val_loss: 0.3701 - val_accuracy: 0.8110\n",
      "Epoch 1660/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2541 - accuracy: 0.8950 - val_loss: 0.3905 - val_accuracy: 0.8661\n",
      "Epoch 1661/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2540 - accuracy: 0.9168 - val_loss: 0.3552 - val_accuracy: 0.8346\n",
      "Epoch 1662/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2519 - accuracy: 0.9089 - val_loss: 0.4378 - val_accuracy: 0.7717\n",
      "Epoch 1663/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2856 - accuracy: 0.8812 - val_loss: 0.3551 - val_accuracy: 0.7953\n",
      "Epoch 1664/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2699 - accuracy: 0.8832 - val_loss: 0.4172 - val_accuracy: 0.7638\n",
      "Epoch 1665/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2516 - accuracy: 0.9089 - val_loss: 0.3724 - val_accuracy: 0.8110\n",
      "Epoch 1666/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.2422 - accuracy: 0.9149 - val_loss: 0.3416 - val_accuracy: 0.8740\n",
      "Epoch 1667/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2508 - accuracy: 0.9010 - val_loss: 0.3995 - val_accuracy: 0.8189\n",
      "Epoch 1668/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2497 - accuracy: 0.9010 - val_loss: 0.3568 - val_accuracy: 0.8268\n",
      "Epoch 1669/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2495 - accuracy: 0.9109 - val_loss: 0.4514 - val_accuracy: 0.7480\n",
      "Epoch 1670/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.2491 - accuracy: 0.8990 - val_loss: 0.4027 - val_accuracy: 0.8425\n",
      "Epoch 1671/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2533 - accuracy: 0.9069 - val_loss: 0.3891 - val_accuracy: 0.8346\n",
      "Epoch 1672/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.2369 - accuracy: 0.9228 - val_loss: 0.4290 - val_accuracy: 0.7717\n",
      "Epoch 1673/2000\n",
      "505/505 [==============================] - 0s 222us/step - loss: 0.2438 - accuracy: 0.9030 - val_loss: 0.3896 - val_accuracy: 0.8189\n",
      "Epoch 1674/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2564 - accuracy: 0.8990 - val_loss: 0.3932 - val_accuracy: 0.8110\n",
      "Epoch 1675/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.2553 - accuracy: 0.8990 - val_loss: 0.4292 - val_accuracy: 0.7717\n",
      "Epoch 1676/2000\n",
      "505/505 [==============================] - 0s 231us/step - loss: 0.2381 - accuracy: 0.9010 - val_loss: 0.4207 - val_accuracy: 0.7874\n",
      "Epoch 1677/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2671 - accuracy: 0.8891 - val_loss: 0.3891 - val_accuracy: 0.8661\n",
      "Epoch 1678/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2575 - accuracy: 0.8990 - val_loss: 0.3686 - val_accuracy: 0.8346\n",
      "Epoch 1679/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2449 - accuracy: 0.9050 - val_loss: 0.3649 - val_accuracy: 0.8346\n",
      "Epoch 1680/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2346 - accuracy: 0.9089 - val_loss: 0.3742 - val_accuracy: 0.8583\n",
      "Epoch 1681/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2292 - accuracy: 0.9287 - val_loss: 0.4610 - val_accuracy: 0.7638\n",
      "Epoch 1682/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2424 - accuracy: 0.9188 - val_loss: 0.4032 - val_accuracy: 0.8583\n",
      "Epoch 1683/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.2576 - accuracy: 0.8891 - val_loss: 0.3491 - val_accuracy: 0.7795\n",
      "Epoch 1684/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2528 - accuracy: 0.8891 - val_loss: 0.3352 - val_accuracy: 0.9055\n",
      "Epoch 1685/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2339 - accuracy: 0.9168 - val_loss: 0.4231 - val_accuracy: 0.7638\n",
      "Epoch 1686/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2630 - accuracy: 0.8970 - val_loss: 0.3667 - val_accuracy: 0.8031\n",
      "Epoch 1687/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2419 - accuracy: 0.9050 - val_loss: 0.3798 - val_accuracy: 0.8740\n",
      "Epoch 1688/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2604 - accuracy: 0.8891 - val_loss: 0.3460 - val_accuracy: 0.7874\n",
      "Epoch 1689/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2435 - accuracy: 0.9050 - val_loss: 0.3837 - val_accuracy: 0.8661\n",
      "Epoch 1690/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2509 - accuracy: 0.9129 - val_loss: 0.5137 - val_accuracy: 0.7244\n",
      "Epoch 1691/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2559 - accuracy: 0.8891 - val_loss: 0.3943 - val_accuracy: 0.8110\n",
      "Epoch 1692/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2453 - accuracy: 0.9069 - val_loss: 0.4195 - val_accuracy: 0.8268\n",
      "Epoch 1693/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2504 - accuracy: 0.9069 - val_loss: 0.3480 - val_accuracy: 0.8661\n",
      "Epoch 1694/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2560 - accuracy: 0.8970 - val_loss: 0.3460 - val_accuracy: 0.8268\n",
      "Epoch 1695/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2394 - accuracy: 0.9149 - val_loss: 0.3485 - val_accuracy: 0.7874\n",
      "Epoch 1696/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2473 - accuracy: 0.9089 - val_loss: 0.4323 - val_accuracy: 0.7480\n",
      "Epoch 1697/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2463 - accuracy: 0.9109 - val_loss: 0.3985 - val_accuracy: 0.8346\n",
      "Epoch 1698/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2552 - accuracy: 0.8911 - val_loss: 0.3517 - val_accuracy: 0.8110\n",
      "Epoch 1699/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2621 - accuracy: 0.8871 - val_loss: 0.3445 - val_accuracy: 0.8740\n",
      "Epoch 1700/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2498 - accuracy: 0.9030 - val_loss: 0.5004 - val_accuracy: 0.7638\n",
      "Epoch 1701/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2623 - accuracy: 0.8990 - val_loss: 0.3589 - val_accuracy: 0.8504\n",
      "Epoch 1702/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2617 - accuracy: 0.8871 - val_loss: 0.4435 - val_accuracy: 0.7717\n",
      "Epoch 1703/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2499 - accuracy: 0.8911 - val_loss: 0.3491 - val_accuracy: 0.8661\n",
      "Epoch 1704/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2379 - accuracy: 0.9248 - val_loss: 0.4662 - val_accuracy: 0.7717\n",
      "Epoch 1705/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2396 - accuracy: 0.9168 - val_loss: 0.4019 - val_accuracy: 0.7953\n",
      "Epoch 1706/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2426 - accuracy: 0.9069 - val_loss: 0.3598 - val_accuracy: 0.8504\n",
      "Epoch 1707/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 217us/step - loss: 0.2381 - accuracy: 0.9109 - val_loss: 0.3693 - val_accuracy: 0.8189\n",
      "Epoch 1708/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2322 - accuracy: 0.9267 - val_loss: 0.3415 - val_accuracy: 0.8583\n",
      "Epoch 1709/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2631 - accuracy: 0.8812 - val_loss: 0.4449 - val_accuracy: 0.7559\n",
      "Epoch 1710/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2446 - accuracy: 0.8911 - val_loss: 0.3721 - val_accuracy: 0.8976\n",
      "Epoch 1711/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2459 - accuracy: 0.8970 - val_loss: 0.4092 - val_accuracy: 0.7953\n",
      "Epoch 1712/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2310 - accuracy: 0.9208 - val_loss: 0.4069 - val_accuracy: 0.8189\n",
      "Epoch 1713/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2396 - accuracy: 0.9129 - val_loss: 0.4912 - val_accuracy: 0.7953\n",
      "Epoch 1714/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2601 - accuracy: 0.8851 - val_loss: 0.4361 - val_accuracy: 0.7717\n",
      "Epoch 1715/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2306 - accuracy: 0.9149 - val_loss: 0.3738 - val_accuracy: 0.7953\n",
      "Epoch 1716/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2516 - accuracy: 0.9010 - val_loss: 0.3386 - val_accuracy: 0.8268\n",
      "Epoch 1717/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2671 - accuracy: 0.8752 - val_loss: 0.4084 - val_accuracy: 0.8189\n",
      "Epoch 1718/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2370 - accuracy: 0.9069 - val_loss: 0.3516 - val_accuracy: 0.8189\n",
      "Epoch 1719/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2456 - accuracy: 0.9069 - val_loss: 0.3239 - val_accuracy: 0.9134\n",
      "Epoch 1720/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2412 - accuracy: 0.9050 - val_loss: 0.4203 - val_accuracy: 0.7795\n",
      "Epoch 1721/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2387 - accuracy: 0.8931 - val_loss: 0.4582 - val_accuracy: 0.7717\n",
      "Epoch 1722/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2335 - accuracy: 0.9089 - val_loss: 0.3915 - val_accuracy: 0.7717\n",
      "Epoch 1723/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.2402 - accuracy: 0.9188 - val_loss: 0.3533 - val_accuracy: 0.8976\n",
      "Epoch 1724/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2269 - accuracy: 0.9287 - val_loss: 0.4005 - val_accuracy: 0.8110\n",
      "Epoch 1725/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2523 - accuracy: 0.9089 - val_loss: 0.4675 - val_accuracy: 0.7402\n",
      "Epoch 1726/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2426 - accuracy: 0.9069 - val_loss: 0.4030 - val_accuracy: 0.7717\n",
      "Epoch 1727/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2392 - accuracy: 0.9030 - val_loss: 0.3545 - val_accuracy: 0.8268\n",
      "Epoch 1728/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2357 - accuracy: 0.9149 - val_loss: 0.3298 - val_accuracy: 0.7874\n",
      "Epoch 1729/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2638 - accuracy: 0.8911 - val_loss: 0.4110 - val_accuracy: 0.7953\n",
      "Epoch 1730/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2354 - accuracy: 0.9050 - val_loss: 0.4596 - val_accuracy: 0.7480\n",
      "Epoch 1731/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2542 - accuracy: 0.8772 - val_loss: 0.4417 - val_accuracy: 0.7480\n",
      "Epoch 1732/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2446 - accuracy: 0.9050 - val_loss: 0.3444 - val_accuracy: 0.8583\n",
      "Epoch 1733/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2613 - accuracy: 0.8812 - val_loss: 0.4045 - val_accuracy: 0.7717\n",
      "Epoch 1734/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2362 - accuracy: 0.9050 - val_loss: 0.4452 - val_accuracy: 0.7638\n",
      "Epoch 1735/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2734 - accuracy: 0.8733 - val_loss: 0.5347 - val_accuracy: 0.7402\n",
      "Epoch 1736/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.2445 - accuracy: 0.9069 - val_loss: 0.3943 - val_accuracy: 0.8661\n",
      "Epoch 1737/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2337 - accuracy: 0.9149 - val_loss: 0.3815 - val_accuracy: 0.8504\n",
      "Epoch 1738/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2489 - accuracy: 0.9050 - val_loss: 0.4348 - val_accuracy: 0.7480\n",
      "Epoch 1739/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2508 - accuracy: 0.8931 - val_loss: 0.5551 - val_accuracy: 0.7402\n",
      "Epoch 1740/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2309 - accuracy: 0.9287 - val_loss: 0.3945 - val_accuracy: 0.7953\n",
      "Epoch 1741/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2280 - accuracy: 0.9149 - val_loss: 0.3385 - val_accuracy: 0.7795\n",
      "Epoch 1742/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2536 - accuracy: 0.8931 - val_loss: 0.4493 - val_accuracy: 0.7402\n",
      "Epoch 1743/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2400 - accuracy: 0.9010 - val_loss: 0.3842 - val_accuracy: 0.8189\n",
      "Epoch 1744/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2419 - accuracy: 0.9069 - val_loss: 0.3636 - val_accuracy: 0.8976\n",
      "Epoch 1745/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2256 - accuracy: 0.9050 - val_loss: 0.3352 - val_accuracy: 0.8661\n",
      "Epoch 1746/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2195 - accuracy: 0.9267 - val_loss: 0.4024 - val_accuracy: 0.8110\n",
      "Epoch 1747/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2290 - accuracy: 0.9188 - val_loss: 0.3724 - val_accuracy: 0.8976\n",
      "Epoch 1748/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2323 - accuracy: 0.9109 - val_loss: 0.3911 - val_accuracy: 0.8346\n",
      "Epoch 1749/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2483 - accuracy: 0.8931 - val_loss: 0.3339 - val_accuracy: 0.7795\n",
      "Epoch 1750/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2366 - accuracy: 0.9050 - val_loss: 0.3826 - val_accuracy: 0.8031\n",
      "Epoch 1751/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2472 - accuracy: 0.9010 - val_loss: 0.3415 - val_accuracy: 0.8268\n",
      "Epoch 1752/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2357 - accuracy: 0.9050 - val_loss: 0.3264 - val_accuracy: 0.8268\n",
      "Epoch 1753/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2352 - accuracy: 0.9050 - val_loss: 0.4446 - val_accuracy: 0.7717\n",
      "Epoch 1754/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2308 - accuracy: 0.9030 - val_loss: 0.3753 - val_accuracy: 0.8110\n",
      "Epoch 1755/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2428 - accuracy: 0.8871 - val_loss: 0.4148 - val_accuracy: 0.7717\n",
      "Epoch 1756/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2426 - accuracy: 0.8990 - val_loss: 0.3760 - val_accuracy: 0.8189\n",
      "Epoch 1757/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2454 - accuracy: 0.9050 - val_loss: 0.3565 - val_accuracy: 0.8976\n",
      "Epoch 1758/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2179 - accuracy: 0.9366 - val_loss: 0.3250 - val_accuracy: 0.8504\n",
      "Epoch 1759/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2364 - accuracy: 0.9069 - val_loss: 0.4573 - val_accuracy: 0.7480\n",
      "Epoch 1760/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2570 - accuracy: 0.8673 - val_loss: 0.3900 - val_accuracy: 0.8504\n",
      "Epoch 1761/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2387 - accuracy: 0.9069 - val_loss: 0.3471 - val_accuracy: 0.8031\n",
      "Epoch 1762/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 213us/step - loss: 0.2421 - accuracy: 0.8832 - val_loss: 0.3705 - val_accuracy: 0.8819\n",
      "Epoch 1763/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2339 - accuracy: 0.8990 - val_loss: 0.3560 - val_accuracy: 0.8976\n",
      "Epoch 1764/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2237 - accuracy: 0.9188 - val_loss: 0.3887 - val_accuracy: 0.8583\n",
      "Epoch 1765/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2368 - accuracy: 0.9030 - val_loss: 0.3488 - val_accuracy: 0.8268\n",
      "Epoch 1766/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.2227 - accuracy: 0.9010 - val_loss: 0.3688 - val_accuracy: 0.8189\n",
      "Epoch 1767/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2196 - accuracy: 0.9228 - val_loss: 0.3377 - val_accuracy: 0.7874\n",
      "Epoch 1768/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2214 - accuracy: 0.9327 - val_loss: 0.3346 - val_accuracy: 0.7874\n",
      "Epoch 1769/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2204 - accuracy: 0.9109 - val_loss: 0.3552 - val_accuracy: 0.8740\n",
      "Epoch 1770/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2248 - accuracy: 0.9168 - val_loss: 0.3911 - val_accuracy: 0.8189\n",
      "Epoch 1771/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2207 - accuracy: 0.9228 - val_loss: 0.3627 - val_accuracy: 0.8425\n",
      "Epoch 1772/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2637 - accuracy: 0.8693 - val_loss: 0.3525 - val_accuracy: 0.8189\n",
      "Epoch 1773/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2363 - accuracy: 0.8970 - val_loss: 0.3306 - val_accuracy: 0.8268\n",
      "Epoch 1774/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2360 - accuracy: 0.8891 - val_loss: 0.4072 - val_accuracy: 0.7717\n",
      "Epoch 1775/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2304 - accuracy: 0.9050 - val_loss: 0.5617 - val_accuracy: 0.7480\n",
      "Epoch 1776/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2447 - accuracy: 0.8990 - val_loss: 0.3845 - val_accuracy: 0.7874\n",
      "Epoch 1777/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2313 - accuracy: 0.9109 - val_loss: 0.4454 - val_accuracy: 0.7480\n",
      "Epoch 1778/2000\n",
      "505/505 [==============================] - 0s 218us/step - loss: 0.2199 - accuracy: 0.9149 - val_loss: 0.3480 - val_accuracy: 0.8031\n",
      "Epoch 1779/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2279 - accuracy: 0.9089 - val_loss: 0.3190 - val_accuracy: 0.8661\n",
      "Epoch 1780/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2832 - accuracy: 0.8634 - val_loss: 0.3539 - val_accuracy: 0.8346\n",
      "Epoch 1781/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2172 - accuracy: 0.9307 - val_loss: 0.3265 - val_accuracy: 0.7874\n",
      "Epoch 1782/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2667 - accuracy: 0.8653 - val_loss: 0.3822 - val_accuracy: 0.8189\n",
      "Epoch 1783/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2352 - accuracy: 0.9069 - val_loss: 0.3214 - val_accuracy: 0.8268\n",
      "Epoch 1784/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2405 - accuracy: 0.9089 - val_loss: 0.3440 - val_accuracy: 0.8346\n",
      "Epoch 1785/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2255 - accuracy: 0.9168 - val_loss: 0.4170 - val_accuracy: 0.7717\n",
      "Epoch 1786/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2284 - accuracy: 0.9030 - val_loss: 0.3616 - val_accuracy: 0.8110\n",
      "Epoch 1787/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2470 - accuracy: 0.8891 - val_loss: 0.4004 - val_accuracy: 0.7874\n",
      "Epoch 1788/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2740 - accuracy: 0.8653 - val_loss: 0.3699 - val_accuracy: 0.8189\n",
      "Epoch 1789/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2312 - accuracy: 0.8990 - val_loss: 0.3404 - val_accuracy: 0.7795\n",
      "Epoch 1790/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2516 - accuracy: 0.8832 - val_loss: 0.3656 - val_accuracy: 0.8268\n",
      "Epoch 1791/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2393 - accuracy: 0.8871 - val_loss: 0.3975 - val_accuracy: 0.8031\n",
      "Epoch 1792/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2173 - accuracy: 0.9030 - val_loss: 0.3712 - val_accuracy: 0.8189\n",
      "Epoch 1793/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2197 - accuracy: 0.9188 - val_loss: 0.3944 - val_accuracy: 0.7953\n",
      "Epoch 1794/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2566 - accuracy: 0.8832 - val_loss: 0.3751 - val_accuracy: 0.8661\n",
      "Epoch 1795/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2155 - accuracy: 0.9267 - val_loss: 0.3652 - val_accuracy: 0.8268\n",
      "Epoch 1796/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2214 - accuracy: 0.9109 - val_loss: 0.3514 - val_accuracy: 0.8976\n",
      "Epoch 1797/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2213 - accuracy: 0.9109 - val_loss: 0.3676 - val_accuracy: 0.8425\n",
      "Epoch 1798/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2244 - accuracy: 0.9050 - val_loss: 0.3785 - val_accuracy: 0.7795\n",
      "Epoch 1799/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2429 - accuracy: 0.8950 - val_loss: 0.3373 - val_accuracy: 0.7874\n",
      "Epoch 1800/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2493 - accuracy: 0.8891 - val_loss: 0.3234 - val_accuracy: 0.7874\n",
      "Epoch 1801/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2414 - accuracy: 0.8812 - val_loss: 0.4115 - val_accuracy: 0.7559\n",
      "Epoch 1802/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2298 - accuracy: 0.9030 - val_loss: 0.3288 - val_accuracy: 0.8583\n",
      "Epoch 1803/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2440 - accuracy: 0.8772 - val_loss: 0.3414 - val_accuracy: 0.8268\n",
      "Epoch 1804/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2613 - accuracy: 0.8851 - val_loss: 0.3782 - val_accuracy: 0.8189\n",
      "Epoch 1805/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2282 - accuracy: 0.9010 - val_loss: 0.3768 - val_accuracy: 0.8583\n",
      "Epoch 1806/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2402 - accuracy: 0.9010 - val_loss: 0.3532 - val_accuracy: 0.8110\n",
      "Epoch 1807/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2335 - accuracy: 0.8970 - val_loss: 0.4240 - val_accuracy: 0.7638\n",
      "Epoch 1808/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2423 - accuracy: 0.8871 - val_loss: 0.3452 - val_accuracy: 0.8425\n",
      "Epoch 1809/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2311 - accuracy: 0.9069 - val_loss: 0.3502 - val_accuracy: 0.8504\n",
      "Epoch 1810/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2530 - accuracy: 0.8931 - val_loss: 0.3596 - val_accuracy: 0.8504\n",
      "Epoch 1811/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.2188 - accuracy: 0.9188 - val_loss: 0.3283 - val_accuracy: 0.8268\n",
      "Epoch 1812/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2377 - accuracy: 0.8871 - val_loss: 0.4262 - val_accuracy: 0.7717\n",
      "Epoch 1813/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2204 - accuracy: 0.9030 - val_loss: 0.3462 - val_accuracy: 0.8425\n",
      "Epoch 1814/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2298 - accuracy: 0.8970 - val_loss: 0.4139 - val_accuracy: 0.7795\n",
      "Epoch 1815/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2273 - accuracy: 0.9010 - val_loss: 0.3424 - val_accuracy: 0.7717\n",
      "Epoch 1816/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2306 - accuracy: 0.8871 - val_loss: 0.4533 - val_accuracy: 0.7402\n",
      "Epoch 1817/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 213us/step - loss: 0.2322 - accuracy: 0.9010 - val_loss: 0.3246 - val_accuracy: 0.8268\n",
      "Epoch 1818/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2223 - accuracy: 0.9069 - val_loss: 0.3785 - val_accuracy: 0.7953\n",
      "Epoch 1819/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2216 - accuracy: 0.9188 - val_loss: 0.4928 - val_accuracy: 0.7559\n",
      "Epoch 1820/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2182 - accuracy: 0.9168 - val_loss: 0.3208 - val_accuracy: 0.8661\n",
      "Epoch 1821/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2204 - accuracy: 0.9208 - val_loss: 0.4214 - val_accuracy: 0.7480\n",
      "Epoch 1822/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2242 - accuracy: 0.9149 - val_loss: 0.3573 - val_accuracy: 0.8425\n",
      "Epoch 1823/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2370 - accuracy: 0.9069 - val_loss: 0.3837 - val_accuracy: 0.7795\n",
      "Epoch 1824/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2526 - accuracy: 0.8891 - val_loss: 0.3435 - val_accuracy: 0.8425\n",
      "Epoch 1825/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2221 - accuracy: 0.9267 - val_loss: 0.3694 - val_accuracy: 0.8189\n",
      "Epoch 1826/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2578 - accuracy: 0.8614 - val_loss: 0.3231 - val_accuracy: 0.8346\n",
      "Epoch 1827/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2218 - accuracy: 0.9069 - val_loss: 0.3473 - val_accuracy: 0.8189\n",
      "Epoch 1828/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2201 - accuracy: 0.9089 - val_loss: 0.3560 - val_accuracy: 0.8661\n",
      "Epoch 1829/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2161 - accuracy: 0.9327 - val_loss: 0.3634 - val_accuracy: 0.8031\n",
      "Epoch 1830/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2403 - accuracy: 0.8970 - val_loss: 0.3748 - val_accuracy: 0.8268\n",
      "Epoch 1831/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2319 - accuracy: 0.9010 - val_loss: 0.3393 - val_accuracy: 0.8268\n",
      "Epoch 1832/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2204 - accuracy: 0.9109 - val_loss: 0.3321 - val_accuracy: 0.8189\n",
      "Epoch 1833/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2158 - accuracy: 0.9267 - val_loss: 0.4134 - val_accuracy: 0.7480\n",
      "Epoch 1834/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2275 - accuracy: 0.9010 - val_loss: 0.3367 - val_accuracy: 0.8661\n",
      "Epoch 1835/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2155 - accuracy: 0.9208 - val_loss: 0.3420 - val_accuracy: 0.8031\n",
      "Epoch 1836/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2252 - accuracy: 0.9089 - val_loss: 0.4095 - val_accuracy: 0.7638\n",
      "Epoch 1837/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2320 - accuracy: 0.9129 - val_loss: 0.4421 - val_accuracy: 0.7480\n",
      "Epoch 1838/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2087 - accuracy: 0.9208 - val_loss: 0.3938 - val_accuracy: 0.7717\n",
      "Epoch 1839/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.2223 - accuracy: 0.9030 - val_loss: 0.4299 - val_accuracy: 0.7717\n",
      "Epoch 1840/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2568 - accuracy: 0.8733 - val_loss: 0.3563 - val_accuracy: 0.8346\n",
      "Epoch 1841/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2217 - accuracy: 0.9010 - val_loss: 0.3307 - val_accuracy: 0.8898\n",
      "Epoch 1842/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2108 - accuracy: 0.9228 - val_loss: 0.3288 - val_accuracy: 0.8661\n",
      "Epoch 1843/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2216 - accuracy: 0.9109 - val_loss: 0.3303 - val_accuracy: 0.7874\n",
      "Epoch 1844/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2122 - accuracy: 0.9129 - val_loss: 0.3793 - val_accuracy: 0.7874\n",
      "Epoch 1845/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2060 - accuracy: 0.9248 - val_loss: 0.4187 - val_accuracy: 0.7717\n",
      "Epoch 1846/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2103 - accuracy: 0.9149 - val_loss: 0.4336 - val_accuracy: 0.7559\n",
      "Epoch 1847/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2222 - accuracy: 0.9050 - val_loss: 0.3493 - val_accuracy: 0.8661\n",
      "Epoch 1848/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2124 - accuracy: 0.9149 - val_loss: 0.3472 - val_accuracy: 0.8583\n",
      "Epoch 1849/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2280 - accuracy: 0.8990 - val_loss: 0.3379 - val_accuracy: 0.7795\n",
      "Epoch 1850/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2104 - accuracy: 0.9109 - val_loss: 0.3388 - val_accuracy: 0.8189\n",
      "Epoch 1851/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.2157 - accuracy: 0.9010 - val_loss: 0.3133 - val_accuracy: 0.7874\n",
      "Epoch 1852/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2283 - accuracy: 0.9010 - val_loss: 0.3407 - val_accuracy: 0.8346\n",
      "Epoch 1853/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2277 - accuracy: 0.9089 - val_loss: 0.5987 - val_accuracy: 0.7480\n",
      "Epoch 1854/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2118 - accuracy: 0.9089 - val_loss: 0.3635 - val_accuracy: 0.7717\n",
      "Epoch 1855/2000\n",
      "505/505 [==============================] - 0s 221us/step - loss: 0.2187 - accuracy: 0.9030 - val_loss: 0.3331 - val_accuracy: 0.8504\n",
      "Epoch 1856/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2178 - accuracy: 0.9208 - val_loss: 0.3464 - val_accuracy: 0.8583\n",
      "Epoch 1857/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2189 - accuracy: 0.9030 - val_loss: 0.3310 - val_accuracy: 0.8898\n",
      "Epoch 1858/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2179 - accuracy: 0.9129 - val_loss: 0.4259 - val_accuracy: 0.7559\n",
      "Epoch 1859/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2262 - accuracy: 0.8970 - val_loss: 0.3369 - val_accuracy: 0.7874\n",
      "Epoch 1860/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2069 - accuracy: 0.9228 - val_loss: 0.3378 - val_accuracy: 0.8425\n",
      "Epoch 1861/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2509 - accuracy: 0.8832 - val_loss: 0.3419 - val_accuracy: 0.8504\n",
      "Epoch 1862/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2119 - accuracy: 0.9168 - val_loss: 0.4423 - val_accuracy: 0.7480\n",
      "Epoch 1863/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2250 - accuracy: 0.9208 - val_loss: 0.4233 - val_accuracy: 0.7717\n",
      "Epoch 1864/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2107 - accuracy: 0.9149 - val_loss: 0.4790 - val_accuracy: 0.7480\n",
      "Epoch 1865/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2416 - accuracy: 0.8832 - val_loss: 0.3338 - val_accuracy: 0.8583\n",
      "Epoch 1866/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2225 - accuracy: 0.9109 - val_loss: 0.4370 - val_accuracy: 0.7795\n",
      "Epoch 1867/2000\n",
      "505/505 [==============================] - 0s 216us/step - loss: 0.2115 - accuracy: 0.9248 - val_loss: 0.3466 - val_accuracy: 0.8898\n",
      "Epoch 1868/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2161 - accuracy: 0.9069 - val_loss: 0.4340 - val_accuracy: 0.7638\n",
      "Epoch 1869/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2355 - accuracy: 0.8950 - val_loss: 0.3459 - val_accuracy: 0.8189\n",
      "Epoch 1870/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2180 - accuracy: 0.8950 - val_loss: 0.3295 - val_accuracy: 0.9055\n",
      "Epoch 1871/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2098 - accuracy: 0.9149 - val_loss: 0.3492 - val_accuracy: 0.8583\n",
      "Epoch 1872/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 213us/step - loss: 0.2233 - accuracy: 0.9089 - val_loss: 0.3359 - val_accuracy: 0.8976\n",
      "Epoch 1873/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2109 - accuracy: 0.9168 - val_loss: 0.3442 - val_accuracy: 0.8268\n",
      "Epoch 1874/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2235 - accuracy: 0.9030 - val_loss: 0.3427 - val_accuracy: 0.8976\n",
      "Epoch 1875/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2243 - accuracy: 0.9010 - val_loss: 0.4119 - val_accuracy: 0.7480\n",
      "Epoch 1876/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2241 - accuracy: 0.9089 - val_loss: 0.3282 - val_accuracy: 0.8425\n",
      "Epoch 1877/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2197 - accuracy: 0.9069 - val_loss: 0.3434 - val_accuracy: 0.8189\n",
      "Epoch 1878/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2118 - accuracy: 0.9168 - val_loss: 0.3480 - val_accuracy: 0.8189\n",
      "Epoch 1879/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2250 - accuracy: 0.8990 - val_loss: 0.3714 - val_accuracy: 0.8346\n",
      "Epoch 1880/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2060 - accuracy: 0.9149 - val_loss: 0.3389 - val_accuracy: 0.7795\n",
      "Epoch 1881/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.2141 - accuracy: 0.9010 - val_loss: 0.3772 - val_accuracy: 0.8268\n",
      "Epoch 1882/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2122 - accuracy: 0.9208 - val_loss: 0.3258 - val_accuracy: 0.8740\n",
      "Epoch 1883/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2035 - accuracy: 0.9188 - val_loss: 0.3180 - val_accuracy: 0.8976\n",
      "Epoch 1884/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2199 - accuracy: 0.8970 - val_loss: 0.3119 - val_accuracy: 0.8504\n",
      "Epoch 1885/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2147 - accuracy: 0.9129 - val_loss: 0.3666 - val_accuracy: 0.8268\n",
      "Epoch 1886/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2194 - accuracy: 0.8950 - val_loss: 0.3224 - val_accuracy: 0.8110\n",
      "Epoch 1887/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2222 - accuracy: 0.8990 - val_loss: 0.3209 - val_accuracy: 0.8425\n",
      "Epoch 1888/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2098 - accuracy: 0.9188 - val_loss: 0.3463 - val_accuracy: 0.8819\n",
      "Epoch 1889/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2206 - accuracy: 0.8871 - val_loss: 0.3668 - val_accuracy: 0.8504\n",
      "Epoch 1890/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2329 - accuracy: 0.8990 - val_loss: 0.3651 - val_accuracy: 0.8268\n",
      "Epoch 1891/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2133 - accuracy: 0.9069 - val_loss: 0.3342 - val_accuracy: 0.8031\n",
      "Epoch 1892/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2232 - accuracy: 0.8970 - val_loss: 0.3158 - val_accuracy: 0.8661\n",
      "Epoch 1893/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2316 - accuracy: 0.8812 - val_loss: 0.2966 - val_accuracy: 0.9291\n",
      "Epoch 1894/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2133 - accuracy: 0.9050 - val_loss: 0.3716 - val_accuracy: 0.8268\n",
      "Epoch 1895/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2370 - accuracy: 0.8851 - val_loss: 0.3572 - val_accuracy: 0.8268\n",
      "Epoch 1896/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2201 - accuracy: 0.8950 - val_loss: 0.3154 - val_accuracy: 0.7874\n",
      "Epoch 1897/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2178 - accuracy: 0.9149 - val_loss: 0.2948 - val_accuracy: 0.8976\n",
      "Epoch 1898/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2227 - accuracy: 0.9010 - val_loss: 0.3787 - val_accuracy: 0.7795\n",
      "Epoch 1899/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2159 - accuracy: 0.9030 - val_loss: 0.3496 - val_accuracy: 0.8425\n",
      "Epoch 1900/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2102 - accuracy: 0.9208 - val_loss: 0.3921 - val_accuracy: 0.7559\n",
      "Epoch 1901/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2034 - accuracy: 0.9188 - val_loss: 0.3362 - val_accuracy: 0.8740\n",
      "Epoch 1902/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2580 - accuracy: 0.8812 - val_loss: 0.3226 - val_accuracy: 0.7795\n",
      "Epoch 1903/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2126 - accuracy: 0.9069 - val_loss: 0.3922 - val_accuracy: 0.7953\n",
      "Epoch 1904/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2154 - accuracy: 0.9010 - val_loss: 0.4012 - val_accuracy: 0.8189\n",
      "Epoch 1905/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2018 - accuracy: 0.9168 - val_loss: 0.3116 - val_accuracy: 0.8425\n",
      "Epoch 1906/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2223 - accuracy: 0.9149 - val_loss: 0.4025 - val_accuracy: 0.7795\n",
      "Epoch 1907/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.2343 - accuracy: 0.8871 - val_loss: 0.3230 - val_accuracy: 0.7953\n",
      "Epoch 1908/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2277 - accuracy: 0.9050 - val_loss: 0.3609 - val_accuracy: 0.7874\n",
      "Epoch 1909/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2177 - accuracy: 0.9129 - val_loss: 0.4288 - val_accuracy: 0.7559\n",
      "Epoch 1910/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2060 - accuracy: 0.9188 - val_loss: 0.3765 - val_accuracy: 0.8031\n",
      "Epoch 1911/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2233 - accuracy: 0.9069 - val_loss: 0.3841 - val_accuracy: 0.7717\n",
      "Epoch 1912/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2572 - accuracy: 0.8653 - val_loss: 0.4162 - val_accuracy: 0.7638\n",
      "Epoch 1913/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2130 - accuracy: 0.9149 - val_loss: 0.3125 - val_accuracy: 0.8425\n",
      "Epoch 1914/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.2010 - accuracy: 0.9228 - val_loss: 0.3492 - val_accuracy: 0.8583\n",
      "Epoch 1915/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2443 - accuracy: 0.8891 - val_loss: 0.3158 - val_accuracy: 0.8189\n",
      "Epoch 1916/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2179 - accuracy: 0.9168 - val_loss: 0.3921 - val_accuracy: 0.8189\n",
      "Epoch 1917/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2156 - accuracy: 0.9089 - val_loss: 0.3213 - val_accuracy: 0.7795\n",
      "Epoch 1918/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.2082 - accuracy: 0.9109 - val_loss: 0.4023 - val_accuracy: 0.7953\n",
      "Epoch 1919/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2029 - accuracy: 0.9228 - val_loss: 0.3628 - val_accuracy: 0.8583\n",
      "Epoch 1920/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.2181 - accuracy: 0.9050 - val_loss: 0.3286 - val_accuracy: 0.8504\n",
      "Epoch 1921/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2340 - accuracy: 0.9050 - val_loss: 0.4439 - val_accuracy: 0.7480\n",
      "Epoch 1922/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2180 - accuracy: 0.8990 - val_loss: 0.3441 - val_accuracy: 0.7874\n",
      "Epoch 1923/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2164 - accuracy: 0.9069 - val_loss: 0.3151 - val_accuracy: 0.8346\n",
      "Epoch 1924/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2297 - accuracy: 0.8911 - val_loss: 0.4872 - val_accuracy: 0.7480\n",
      "Epoch 1925/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2054 - accuracy: 0.9228 - val_loss: 0.3791 - val_accuracy: 0.8268\n",
      "Epoch 1926/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2032 - accuracy: 0.9168 - val_loss: 0.3954 - val_accuracy: 0.7795\n",
      "Epoch 1927/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 213us/step - loss: 0.2091 - accuracy: 0.9267 - val_loss: 0.3928 - val_accuracy: 0.7795\n",
      "Epoch 1928/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2170 - accuracy: 0.9069 - val_loss: 0.3576 - val_accuracy: 0.8031\n",
      "Epoch 1929/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2092 - accuracy: 0.9109 - val_loss: 0.3522 - val_accuracy: 0.8740\n",
      "Epoch 1930/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2156 - accuracy: 0.9149 - val_loss: 0.3572 - val_accuracy: 0.7795\n",
      "Epoch 1931/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2046 - accuracy: 0.9149 - val_loss: 0.3824 - val_accuracy: 0.7717\n",
      "Epoch 1932/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.2323 - accuracy: 0.8911 - val_loss: 0.3799 - val_accuracy: 0.7717\n",
      "Epoch 1933/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2005 - accuracy: 0.9168 - val_loss: 0.4392 - val_accuracy: 0.7480\n",
      "Epoch 1934/2000\n",
      "505/505 [==============================] - 0s 212us/step - loss: 0.2346 - accuracy: 0.8990 - val_loss: 0.3089 - val_accuracy: 0.8425\n",
      "Epoch 1935/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2086 - accuracy: 0.9168 - val_loss: 0.3510 - val_accuracy: 0.8661\n",
      "Epoch 1936/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.2162 - accuracy: 0.9030 - val_loss: 0.2991 - val_accuracy: 0.8661\n",
      "Epoch 1937/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2088 - accuracy: 0.9149 - val_loss: 0.3522 - val_accuracy: 0.8189\n",
      "Epoch 1938/2000\n",
      "505/505 [==============================] - 0s 207us/step - loss: 0.2116 - accuracy: 0.9050 - val_loss: 0.3440 - val_accuracy: 0.7874\n",
      "Epoch 1939/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2012 - accuracy: 0.9129 - val_loss: 0.2967 - val_accuracy: 0.8661\n",
      "Epoch 1940/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2090 - accuracy: 0.9089 - val_loss: 0.4047 - val_accuracy: 0.7795\n",
      "Epoch 1941/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2080 - accuracy: 0.9129 - val_loss: 0.3299 - val_accuracy: 0.8110\n",
      "Epoch 1942/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2127 - accuracy: 0.9030 - val_loss: 0.3048 - val_accuracy: 0.8031\n",
      "Epoch 1943/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2314 - accuracy: 0.8812 - val_loss: 0.3110 - val_accuracy: 0.8268\n",
      "Epoch 1944/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2121 - accuracy: 0.9149 - val_loss: 0.3038 - val_accuracy: 0.8268\n",
      "Epoch 1945/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2300 - accuracy: 0.8931 - val_loss: 0.3189 - val_accuracy: 0.8583\n",
      "Epoch 1946/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2126 - accuracy: 0.8990 - val_loss: 0.3900 - val_accuracy: 0.7717\n",
      "Epoch 1947/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2057 - accuracy: 0.9208 - val_loss: 0.4020 - val_accuracy: 0.8031\n",
      "Epoch 1948/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2050 - accuracy: 0.9168 - val_loss: 0.3296 - val_accuracy: 0.8661\n",
      "Epoch 1949/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2116 - accuracy: 0.9069 - val_loss: 0.3469 - val_accuracy: 0.8504\n",
      "Epoch 1950/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2291 - accuracy: 0.8792 - val_loss: 0.3019 - val_accuracy: 0.7874\n",
      "Epoch 1951/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2026 - accuracy: 0.9267 - val_loss: 0.3620 - val_accuracy: 0.8583\n",
      "Epoch 1952/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2251 - accuracy: 0.8891 - val_loss: 0.3352 - val_accuracy: 0.8268\n",
      "Epoch 1953/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.1986 - accuracy: 0.9287 - val_loss: 0.3466 - val_accuracy: 0.8346\n",
      "Epoch 1954/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2041 - accuracy: 0.9208 - val_loss: 0.3435 - val_accuracy: 0.8740\n",
      "Epoch 1955/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2086 - accuracy: 0.9089 - val_loss: 0.3209 - val_accuracy: 0.8110\n",
      "Epoch 1956/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2144 - accuracy: 0.9089 - val_loss: 0.3369 - val_accuracy: 0.8346\n",
      "Epoch 1957/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2503 - accuracy: 0.8733 - val_loss: 0.3132 - val_accuracy: 0.8740\n",
      "Epoch 1958/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2106 - accuracy: 0.9030 - val_loss: 0.3325 - val_accuracy: 0.7717\n",
      "Epoch 1959/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2109 - accuracy: 0.9069 - val_loss: 0.3447 - val_accuracy: 0.8268\n",
      "Epoch 1960/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2243 - accuracy: 0.8990 - val_loss: 0.3881 - val_accuracy: 0.8346\n",
      "Epoch 1961/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2154 - accuracy: 0.9030 - val_loss: 0.3154 - val_accuracy: 0.8346\n",
      "Epoch 1962/2000\n",
      "505/505 [==============================] - 0s 220us/step - loss: 0.2047 - accuracy: 0.9168 - val_loss: 0.3022 - val_accuracy: 0.8740\n",
      "Epoch 1963/2000\n",
      "505/505 [==============================] - 0s 225us/step - loss: 0.2088 - accuracy: 0.9228 - val_loss: 0.3216 - val_accuracy: 0.7795\n",
      "Epoch 1964/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2170 - accuracy: 0.9030 - val_loss: 0.3248 - val_accuracy: 0.8504\n",
      "Epoch 1965/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2019 - accuracy: 0.9248 - val_loss: 0.3365 - val_accuracy: 0.8583\n",
      "Epoch 1966/2000\n",
      "505/505 [==============================] - 0s 219us/step - loss: 0.2317 - accuracy: 0.8970 - val_loss: 0.3288 - val_accuracy: 0.7874\n",
      "Epoch 1967/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2205 - accuracy: 0.8990 - val_loss: 0.3364 - val_accuracy: 0.8425\n",
      "Epoch 1968/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2065 - accuracy: 0.9109 - val_loss: 0.3344 - val_accuracy: 0.8898\n",
      "Epoch 1969/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2119 - accuracy: 0.9089 - val_loss: 0.3411 - val_accuracy: 0.8268\n",
      "Epoch 1970/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2111 - accuracy: 0.9109 - val_loss: 0.3646 - val_accuracy: 0.7795\n",
      "Epoch 1971/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2105 - accuracy: 0.9050 - val_loss: 0.3189 - val_accuracy: 0.8031\n",
      "Epoch 1972/2000\n",
      "505/505 [==============================] - 0s 223us/step - loss: 0.2174 - accuracy: 0.9089 - val_loss: 0.3936 - val_accuracy: 0.7480\n",
      "Epoch 1973/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.2064 - accuracy: 0.9030 - val_loss: 0.2972 - val_accuracy: 0.8740\n",
      "Epoch 1974/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2019 - accuracy: 0.9208 - val_loss: 0.3310 - val_accuracy: 0.8583\n",
      "Epoch 1975/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2001 - accuracy: 0.9168 - val_loss: 0.3557 - val_accuracy: 0.8268\n",
      "Epoch 1976/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2176 - accuracy: 0.8970 - val_loss: 0.3338 - val_accuracy: 0.8661\n",
      "Epoch 1977/2000\n",
      "505/505 [==============================] - 0s 214us/step - loss: 0.1973 - accuracy: 0.9208 - val_loss: 0.3208 - val_accuracy: 0.8661\n",
      "Epoch 1978/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2033 - accuracy: 0.9050 - val_loss: 0.3015 - val_accuracy: 0.8110\n",
      "Epoch 1979/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2087 - accuracy: 0.9129 - val_loss: 0.3766 - val_accuracy: 0.7638\n",
      "Epoch 1980/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2021 - accuracy: 0.9347 - val_loss: 0.4406 - val_accuracy: 0.7795\n",
      "Epoch 1981/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2076 - accuracy: 0.8871 - val_loss: 0.3722 - val_accuracy: 0.7874\n",
      "Epoch 1982/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 213us/step - loss: 0.2067 - accuracy: 0.9208 - val_loss: 0.4017 - val_accuracy: 0.7795\n",
      "Epoch 1983/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2261 - accuracy: 0.8792 - val_loss: 0.3702 - val_accuracy: 0.8268\n",
      "Epoch 1984/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.1976 - accuracy: 0.9307 - val_loss: 0.3029 - val_accuracy: 0.7953\n",
      "Epoch 1985/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2507 - accuracy: 0.8713 - val_loss: 0.3152 - val_accuracy: 0.8189\n",
      "Epoch 1986/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2157 - accuracy: 0.9089 - val_loss: 0.3005 - val_accuracy: 0.8268\n",
      "Epoch 1987/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2153 - accuracy: 0.8990 - val_loss: 0.3762 - val_accuracy: 0.8189\n",
      "Epoch 1988/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2042 - accuracy: 0.9188 - val_loss: 0.4048 - val_accuracy: 0.7559\n",
      "Epoch 1989/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.2092 - accuracy: 0.9267 - val_loss: 0.3729 - val_accuracy: 0.8189\n",
      "Epoch 1990/2000\n",
      "505/505 [==============================] - 0s 210us/step - loss: 0.2212 - accuracy: 0.9010 - val_loss: 0.3760 - val_accuracy: 0.7795\n",
      "Epoch 1991/2000\n",
      "505/505 [==============================] - 0s 217us/step - loss: 0.2054 - accuracy: 0.9149 - val_loss: 0.2942 - val_accuracy: 0.8346\n",
      "Epoch 1992/2000\n",
      "505/505 [==============================] - 0s 211us/step - loss: 0.1990 - accuracy: 0.9188 - val_loss: 0.3391 - val_accuracy: 0.9055\n",
      "Epoch 1993/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2103 - accuracy: 0.9168 - val_loss: 0.3057 - val_accuracy: 0.7874\n",
      "Epoch 1994/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2005 - accuracy: 0.9188 - val_loss: 0.4413 - val_accuracy: 0.7559\n",
      "Epoch 1995/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2019 - accuracy: 0.9050 - val_loss: 0.3288 - val_accuracy: 0.9055\n",
      "Epoch 1996/2000\n",
      "505/505 [==============================] - 0s 215us/step - loss: 0.2026 - accuracy: 0.9208 - val_loss: 0.3093 - val_accuracy: 0.8740\n",
      "Epoch 1997/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.2081 - accuracy: 0.9089 - val_loss: 0.3371 - val_accuracy: 0.8346\n",
      "Epoch 1998/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.1929 - accuracy: 0.9248 - val_loss: 0.3383 - val_accuracy: 0.8661\n",
      "Epoch 1999/2000\n",
      "505/505 [==============================] - 0s 209us/step - loss: 0.2184 - accuracy: 0.8812 - val_loss: 0.3116 - val_accuracy: 0.8346\n",
      "Epoch 2000/2000\n",
      "505/505 [==============================] - 0s 213us/step - loss: 0.1990 - accuracy: 0.9188 - val_loss: 0.3042 - val_accuracy: 0.8740\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, Activation, MaxPooling1D\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_dim=2000))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "sgd = SGD()\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "his = model.fit(x=x_train, y=y_train, batch_size=64, epochs=2000,shuffle=True, verbose=1, \n",
    "               validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(his.history['accuracy'])\n",
    "plt.plot(his.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# 绘制训练 & 验证的损失值\n",
    "plt.plot(his.history['loss'])\n",
    "plt.plot(his.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisherscore(data, labels, num):\n",
    "\n",
    "    high = len(data)  # 向量个数\n",
    "    weight = len(data[0])  # 向量长度\n",
    "    P_num = np.sum(labels == 0)  # 正样本\n",
    "    N_num = np.sum(labels == 1)  # 负样本\n",
    "\n",
    "    # 计算Fisher score\n",
    "\n",
    "    fisherscore = []\n",
    "    for i in range(weight):\n",
    "        p = []\n",
    "        n = []\n",
    "        p_var = []\n",
    "        n_var = []\n",
    "        for j in range(high):\n",
    "            if labels[j] == 0:\n",
    "                p.append(data[j, i])\n",
    "            if labels[j] == 1:\n",
    "                n.append(data[j, i])\n",
    "\n",
    "        p_average = np.sum(p) / len(p)\n",
    "        n_average = np.sum(n) / len(n)\n",
    "        average = (np.sum(p) + np.sum(n)) / (len(p) + len(n))\n",
    "\n",
    "        for j in range(high):\n",
    "            if labels[j] == 0:\n",
    "                p_var.append((data[j, i] - p_average) ** 2)\n",
    "            if labels[j] == 1:\n",
    "                n_var.append((data[j, i] - n_average) ** 2)\n",
    "\n",
    "        score = ((p_average - average) ** 2 + (n_average - average) ** 2) / (\n",
    "                    np.sum(p_var) / len(p) + np.sum(n_var) / len(n))\n",
    "\n",
    "        fisherscore.append(score)\n",
    "\n",
    "    index = np.argsort(-np.array(fisherscore))  # 返回索引\n",
    "    new_data = []\n",
    "    for i in range(num):\n",
    "        new_data.append(data[:, index[i]])\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_data = new_data.transpose(1, 0)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fisherscore(data, label, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22780081748>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(data[:, 0], data[:, 1], c=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4881889763779528"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(x_train, y_train)\n",
    "classifier.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 505 samples, validate on 127 samples\n",
      "Epoch 1/500\n",
      "505/505 [==============================] - 0s 936us/step - loss: 0.6931 - accuracy: 0.4990 - val_loss: 0.6921 - val_accuracy: 0.5276\n",
      "Epoch 2/500\n",
      "505/505 [==============================] - 0s 142us/step - loss: 0.6930 - accuracy: 0.5228 - val_loss: 0.6918 - val_accuracy: 0.5906\n",
      "Epoch 3/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6928 - accuracy: 0.5307 - val_loss: 0.6916 - val_accuracy: 0.6142\n",
      "Epoch 4/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6926 - accuracy: 0.5208 - val_loss: 0.6913 - val_accuracy: 0.5984\n",
      "Epoch 5/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6925 - accuracy: 0.5267 - val_loss: 0.6912 - val_accuracy: 0.6063\n",
      "Epoch 6/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6924 - accuracy: 0.5248 - val_loss: 0.6910 - val_accuracy: 0.6220\n",
      "Epoch 7/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6922 - accuracy: 0.5347 - val_loss: 0.6908 - val_accuracy: 0.6378\n",
      "Epoch 8/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6921 - accuracy: 0.5248 - val_loss: 0.6906 - val_accuracy: 0.6378\n",
      "Epoch 9/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6920 - accuracy: 0.5386 - val_loss: 0.6904 - val_accuracy: 0.6299\n",
      "Epoch 10/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6920 - accuracy: 0.5426 - val_loss: 0.6902 - val_accuracy: 0.6142\n",
      "Epoch 11/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6919 - accuracy: 0.5406 - val_loss: 0.6901 - val_accuracy: 0.5906\n",
      "Epoch 12/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6918 - accuracy: 0.5366 - val_loss: 0.6899 - val_accuracy: 0.5906\n",
      "Epoch 13/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6917 - accuracy: 0.5465 - val_loss: 0.6898 - val_accuracy: 0.5827\n",
      "Epoch 14/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6916 - accuracy: 0.5446 - val_loss: 0.6897 - val_accuracy: 0.5906\n",
      "Epoch 15/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6915 - accuracy: 0.5426 - val_loss: 0.6895 - val_accuracy: 0.5827\n",
      "Epoch 16/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6914 - accuracy: 0.5446 - val_loss: 0.6894 - val_accuracy: 0.5748\n",
      "Epoch 17/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6914 - accuracy: 0.5465 - val_loss: 0.6892 - val_accuracy: 0.5748\n",
      "Epoch 18/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6913 - accuracy: 0.5406 - val_loss: 0.6890 - val_accuracy: 0.5748\n",
      "Epoch 19/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6912 - accuracy: 0.5406 - val_loss: 0.6889 - val_accuracy: 0.5827\n",
      "Epoch 20/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6911 - accuracy: 0.5406 - val_loss: 0.6888 - val_accuracy: 0.5748\n",
      "Epoch 21/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6911 - accuracy: 0.5446 - val_loss: 0.6886 - val_accuracy: 0.5748\n",
      "Epoch 22/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6910 - accuracy: 0.5406 - val_loss: 0.6885 - val_accuracy: 0.5748\n",
      "Epoch 23/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6909 - accuracy: 0.5426 - val_loss: 0.6884 - val_accuracy: 0.5748\n",
      "Epoch 24/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6908 - accuracy: 0.5426 - val_loss: 0.6882 - val_accuracy: 0.5748\n",
      "Epoch 25/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6908 - accuracy: 0.5426 - val_loss: 0.6882 - val_accuracy: 0.5827\n",
      "Epoch 26/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6907 - accuracy: 0.5446 - val_loss: 0.6880 - val_accuracy: 0.5827\n",
      "Epoch 27/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6906 - accuracy: 0.5465 - val_loss: 0.6879 - val_accuracy: 0.5827\n",
      "Epoch 28/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6906 - accuracy: 0.5446 - val_loss: 0.6878 - val_accuracy: 0.5827\n",
      "Epoch 29/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6906 - accuracy: 0.5426 - val_loss: 0.6877 - val_accuracy: 0.5827\n",
      "Epoch 30/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6905 - accuracy: 0.5446 - val_loss: 0.6875 - val_accuracy: 0.5827\n",
      "Epoch 31/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6904 - accuracy: 0.5406 - val_loss: 0.6874 - val_accuracy: 0.5827\n",
      "Epoch 32/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6903 - accuracy: 0.5426 - val_loss: 0.6873 - val_accuracy: 0.5748\n",
      "Epoch 33/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6903 - accuracy: 0.5426 - val_loss: 0.6872 - val_accuracy: 0.5827\n",
      "Epoch 34/500\n",
      "505/505 [==============================] - ETA: 0s - loss: 0.6898 - accuracy: 0.58 - 0s 118us/step - loss: 0.6902 - accuracy: 0.5426 - val_loss: 0.6871 - val_accuracy: 0.5827\n",
      "Epoch 35/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6902 - accuracy: 0.5465 - val_loss: 0.6870 - val_accuracy: 0.5827\n",
      "Epoch 36/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6901 - accuracy: 0.5426 - val_loss: 0.6869 - val_accuracy: 0.5748\n",
      "Epoch 37/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6901 - accuracy: 0.5426 - val_loss: 0.6868 - val_accuracy: 0.5748\n",
      "Epoch 38/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6900 - accuracy: 0.5426 - val_loss: 0.6867 - val_accuracy: 0.5748\n",
      "Epoch 39/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6900 - accuracy: 0.5426 - val_loss: 0.6866 - val_accuracy: 0.5748\n",
      "Epoch 40/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6899 - accuracy: 0.5426 - val_loss: 0.6865 - val_accuracy: 0.5748\n",
      "Epoch 41/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6898 - accuracy: 0.5426 - val_loss: 0.6864 - val_accuracy: 0.5827\n",
      "Epoch 42/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6899 - accuracy: 0.5426 - val_loss: 0.6863 - val_accuracy: 0.5748\n",
      "Epoch 43/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6898 - accuracy: 0.5426 - val_loss: 0.6862 - val_accuracy: 0.5748\n",
      "Epoch 44/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6897 - accuracy: 0.5426 - val_loss: 0.6861 - val_accuracy: 0.5748\n",
      "Epoch 45/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6897 - accuracy: 0.5426 - val_loss: 0.6860 - val_accuracy: 0.5748\n",
      "Epoch 46/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6896 - accuracy: 0.5426 - val_loss: 0.6860 - val_accuracy: 0.5748\n",
      "Epoch 47/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6896 - accuracy: 0.5426 - val_loss: 0.6859 - val_accuracy: 0.5827\n",
      "Epoch 48/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6895 - accuracy: 0.5446 - val_loss: 0.6858 - val_accuracy: 0.5748\n",
      "Epoch 49/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6894 - accuracy: 0.5426 - val_loss: 0.6857 - val_accuracy: 0.5827\n",
      "Epoch 50/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6894 - accuracy: 0.5446 - val_loss: 0.6856 - val_accuracy: 0.5827\n",
      "Epoch 51/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6894 - accuracy: 0.5446 - val_loss: 0.6855 - val_accuracy: 0.5827\n",
      "Epoch 52/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6893 - accuracy: 0.5426 - val_loss: 0.6855 - val_accuracy: 0.5827\n",
      "Epoch 53/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6893 - accuracy: 0.5426 - val_loss: 0.6854 - val_accuracy: 0.5827\n",
      "Epoch 54/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6893 - accuracy: 0.5446 - val_loss: 0.6853 - val_accuracy: 0.5827\n",
      "Epoch 55/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6892 - accuracy: 0.5465 - val_loss: 0.6852 - val_accuracy: 0.5827\n",
      "Epoch 56/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6892 - accuracy: 0.5465 - val_loss: 0.6851 - val_accuracy: 0.5827\n",
      "Epoch 57/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6892 - accuracy: 0.5465 - val_loss: 0.6850 - val_accuracy: 0.5827\n",
      "Epoch 58/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6892 - accuracy: 0.5446 - val_loss: 0.6849 - val_accuracy: 0.5827\n",
      "Epoch 59/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6891 - accuracy: 0.5465 - val_loss: 0.6849 - val_accuracy: 0.5827\n",
      "Epoch 60/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6890 - accuracy: 0.5465 - val_loss: 0.6848 - val_accuracy: 0.5827\n",
      "Epoch 61/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6890 - accuracy: 0.5485 - val_loss: 0.6847 - val_accuracy: 0.5827\n",
      "Epoch 62/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6889 - accuracy: 0.5426 - val_loss: 0.6846 - val_accuracy: 0.5827\n",
      "Epoch 63/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6889 - accuracy: 0.5505 - val_loss: 0.6845 - val_accuracy: 0.5827\n",
      "Epoch 64/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6889 - accuracy: 0.5465 - val_loss: 0.6844 - val_accuracy: 0.5906\n",
      "Epoch 65/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6888 - accuracy: 0.5465 - val_loss: 0.6844 - val_accuracy: 0.5906\n",
      "Epoch 66/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6889 - accuracy: 0.5505 - val_loss: 0.6843 - val_accuracy: 0.5906\n",
      "Epoch 67/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6888 - accuracy: 0.5465 - val_loss: 0.6842 - val_accuracy: 0.5906\n",
      "Epoch 68/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6887 - accuracy: 0.5545 - val_loss: 0.6842 - val_accuracy: 0.5906\n",
      "Epoch 69/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6887 - accuracy: 0.5485 - val_loss: 0.6841 - val_accuracy: 0.5827\n",
      "Epoch 70/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6887 - accuracy: 0.5465 - val_loss: 0.6840 - val_accuracy: 0.5906\n",
      "Epoch 71/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6886 - accuracy: 0.5465 - val_loss: 0.6839 - val_accuracy: 0.5906\n",
      "Epoch 72/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6886 - accuracy: 0.5446 - val_loss: 0.6838 - val_accuracy: 0.5906\n",
      "Epoch 73/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6885 - accuracy: 0.5446 - val_loss: 0.6837 - val_accuracy: 0.5906\n",
      "Epoch 74/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6885 - accuracy: 0.5485 - val_loss: 0.6837 - val_accuracy: 0.5906\n",
      "Epoch 75/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6884 - accuracy: 0.5426 - val_loss: 0.6836 - val_accuracy: 0.5906\n",
      "Epoch 76/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6885 - accuracy: 0.5525 - val_loss: 0.6836 - val_accuracy: 0.5906\n",
      "Epoch 77/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6884 - accuracy: 0.5485 - val_loss: 0.6835 - val_accuracy: 0.5906\n",
      "Epoch 78/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6883 - accuracy: 0.5485 - val_loss: 0.6835 - val_accuracy: 0.5906\n",
      "Epoch 79/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6883 - accuracy: 0.5465 - val_loss: 0.6834 - val_accuracy: 0.5906\n",
      "Epoch 80/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6883 - accuracy: 0.5604 - val_loss: 0.6833 - val_accuracy: 0.5906\n",
      "Epoch 81/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6883 - accuracy: 0.5505 - val_loss: 0.6832 - val_accuracy: 0.5906\n",
      "Epoch 82/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6882 - accuracy: 0.5604 - val_loss: 0.6832 - val_accuracy: 0.5906\n",
      "Epoch 83/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6882 - accuracy: 0.5545 - val_loss: 0.6831 - val_accuracy: 0.5906\n",
      "Epoch 84/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6881 - accuracy: 0.5525 - val_loss: 0.6830 - val_accuracy: 0.5906\n",
      "Epoch 85/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6881 - accuracy: 0.5545 - val_loss: 0.6830 - val_accuracy: 0.5906\n",
      "Epoch 86/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6881 - accuracy: 0.5525 - val_loss: 0.6829 - val_accuracy: 0.5906\n",
      "Epoch 87/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6880 - accuracy: 0.5545 - val_loss: 0.6828 - val_accuracy: 0.5906\n",
      "Epoch 88/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6880 - accuracy: 0.5525 - val_loss: 0.6828 - val_accuracy: 0.5984\n",
      "Epoch 89/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6880 - accuracy: 0.5525 - val_loss: 0.6827 - val_accuracy: 0.5906\n",
      "Epoch 90/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6880 - accuracy: 0.5485 - val_loss: 0.6827 - val_accuracy: 0.6063\n",
      "Epoch 91/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6879 - accuracy: 0.5584 - val_loss: 0.6826 - val_accuracy: 0.6063\n",
      "Epoch 92/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6879 - accuracy: 0.5584 - val_loss: 0.6825 - val_accuracy: 0.6063\n",
      "Epoch 93/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6878 - accuracy: 0.5564 - val_loss: 0.6825 - val_accuracy: 0.5984\n",
      "Epoch 94/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6878 - accuracy: 0.5644 - val_loss: 0.6824 - val_accuracy: 0.5984\n",
      "Epoch 95/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6878 - accuracy: 0.5703 - val_loss: 0.6823 - val_accuracy: 0.5984\n",
      "Epoch 96/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6877 - accuracy: 0.5584 - val_loss: 0.6823 - val_accuracy: 0.5984\n",
      "Epoch 97/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6878 - accuracy: 0.5624 - val_loss: 0.6822 - val_accuracy: 0.5984\n",
      "Epoch 98/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6877 - accuracy: 0.5822 - val_loss: 0.6822 - val_accuracy: 0.6063\n",
      "Epoch 99/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6877 - accuracy: 0.5604 - val_loss: 0.6821 - val_accuracy: 0.5984\n",
      "Epoch 100/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6877 - accuracy: 0.5644 - val_loss: 0.6820 - val_accuracy: 0.6063\n",
      "Epoch 101/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6877 - accuracy: 0.5703 - val_loss: 0.6820 - val_accuracy: 0.6063\n",
      "Epoch 102/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6876 - accuracy: 0.5525 - val_loss: 0.6819 - val_accuracy: 0.5748\n",
      "Epoch 103/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6875 - accuracy: 0.5782 - val_loss: 0.6819 - val_accuracy: 0.5827\n",
      "Epoch 104/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6875 - accuracy: 0.5723 - val_loss: 0.6818 - val_accuracy: 0.5984\n",
      "Epoch 105/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6875 - accuracy: 0.5822 - val_loss: 0.6818 - val_accuracy: 0.5748\n",
      "Epoch 106/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6874 - accuracy: 0.5683 - val_loss: 0.6817 - val_accuracy: 0.5984\n",
      "Epoch 107/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6874 - accuracy: 0.5802 - val_loss: 0.6817 - val_accuracy: 0.5906\n",
      "Epoch 108/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6874 - accuracy: 0.5822 - val_loss: 0.6816 - val_accuracy: 0.5748\n",
      "Epoch 109/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6873 - accuracy: 0.5762 - val_loss: 0.6816 - val_accuracy: 0.5748\n",
      "Epoch 110/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6873 - accuracy: 0.5703 - val_loss: 0.6815 - val_accuracy: 0.5827\n",
      "Epoch 111/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6873 - accuracy: 0.5802 - val_loss: 0.6815 - val_accuracy: 0.5748\n",
      "Epoch 112/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505/505 [==============================] - 0s 120us/step - loss: 0.6873 - accuracy: 0.5703 - val_loss: 0.6814 - val_accuracy: 0.5827\n",
      "Epoch 113/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6873 - accuracy: 0.5822 - val_loss: 0.6814 - val_accuracy: 0.5748\n",
      "Epoch 114/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6872 - accuracy: 0.5743 - val_loss: 0.6813 - val_accuracy: 0.5984\n",
      "Epoch 115/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6872 - accuracy: 0.5842 - val_loss: 0.6813 - val_accuracy: 0.5906\n",
      "Epoch 116/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6871 - accuracy: 0.5881 - val_loss: 0.6812 - val_accuracy: 0.5827\n",
      "Epoch 117/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6872 - accuracy: 0.5980 - val_loss: 0.6812 - val_accuracy: 0.5748\n",
      "Epoch 118/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6871 - accuracy: 0.5743 - val_loss: 0.6811 - val_accuracy: 0.5748\n",
      "Epoch 119/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6870 - accuracy: 0.5762 - val_loss: 0.6811 - val_accuracy: 0.5827\n",
      "Epoch 120/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6870 - accuracy: 0.5842 - val_loss: 0.6810 - val_accuracy: 0.5906\n",
      "Epoch 121/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6870 - accuracy: 0.5822 - val_loss: 0.6810 - val_accuracy: 0.5906\n",
      "Epoch 122/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6869 - accuracy: 0.5842 - val_loss: 0.6809 - val_accuracy: 0.5748\n",
      "Epoch 123/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6870 - accuracy: 0.5901 - val_loss: 0.6809 - val_accuracy: 0.5906\n",
      "Epoch 124/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6869 - accuracy: 0.5941 - val_loss: 0.6808 - val_accuracy: 0.5827\n",
      "Epoch 125/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6869 - accuracy: 0.5842 - val_loss: 0.6808 - val_accuracy: 0.5827\n",
      "Epoch 126/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6868 - accuracy: 0.5802 - val_loss: 0.6807 - val_accuracy: 0.5748\n",
      "Epoch 127/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6869 - accuracy: 0.5941 - val_loss: 0.6807 - val_accuracy: 0.5906\n",
      "Epoch 128/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6868 - accuracy: 0.5960 - val_loss: 0.6806 - val_accuracy: 0.5748\n",
      "Epoch 129/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6868 - accuracy: 0.5802 - val_loss: 0.6806 - val_accuracy: 0.5669\n",
      "Epoch 130/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6867 - accuracy: 0.5861 - val_loss: 0.6806 - val_accuracy: 0.5748\n",
      "Epoch 131/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6867 - accuracy: 0.5842 - val_loss: 0.6805 - val_accuracy: 0.5906\n",
      "Epoch 132/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6868 - accuracy: 0.5861 - val_loss: 0.6805 - val_accuracy: 0.5748\n",
      "Epoch 133/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6867 - accuracy: 0.5881 - val_loss: 0.6805 - val_accuracy: 0.5748\n",
      "Epoch 134/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6866 - accuracy: 0.5842 - val_loss: 0.6804 - val_accuracy: 0.5748\n",
      "Epoch 135/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6866 - accuracy: 0.5901 - val_loss: 0.6803 - val_accuracy: 0.5906\n",
      "Epoch 136/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6866 - accuracy: 0.5822 - val_loss: 0.6803 - val_accuracy: 0.6063\n",
      "Epoch 137/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6866 - accuracy: 0.5842 - val_loss: 0.6803 - val_accuracy: 0.5984\n",
      "Epoch 138/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6866 - accuracy: 0.5782 - val_loss: 0.6802 - val_accuracy: 0.5906\n",
      "Epoch 139/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6865 - accuracy: 0.5822 - val_loss: 0.6802 - val_accuracy: 0.5669\n",
      "Epoch 140/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6865 - accuracy: 0.5842 - val_loss: 0.6801 - val_accuracy: 0.5748\n",
      "Epoch 141/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6865 - accuracy: 0.5861 - val_loss: 0.6801 - val_accuracy: 0.5748\n",
      "Epoch 142/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6864 - accuracy: 0.5861 - val_loss: 0.6801 - val_accuracy: 0.5827\n",
      "Epoch 143/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6864 - accuracy: 0.5861 - val_loss: 0.6800 - val_accuracy: 0.5827\n",
      "Epoch 144/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6864 - accuracy: 0.5941 - val_loss: 0.6800 - val_accuracy: 0.5827\n",
      "Epoch 145/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6864 - accuracy: 0.5822 - val_loss: 0.6799 - val_accuracy: 0.5669\n",
      "Epoch 146/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6863 - accuracy: 0.5842 - val_loss: 0.6799 - val_accuracy: 0.5669\n",
      "Epoch 147/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6863 - accuracy: 0.5842 - val_loss: 0.6799 - val_accuracy: 0.5669\n",
      "Epoch 148/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6863 - accuracy: 0.5762 - val_loss: 0.6799 - val_accuracy: 0.5669\n",
      "Epoch 149/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6863 - accuracy: 0.5802 - val_loss: 0.6798 - val_accuracy: 0.5827\n",
      "Epoch 150/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6862 - accuracy: 0.5802 - val_loss: 0.6798 - val_accuracy: 0.5827\n",
      "Epoch 151/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6862 - accuracy: 0.5901 - val_loss: 0.6797 - val_accuracy: 0.5984\n",
      "Epoch 152/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6863 - accuracy: 0.5842 - val_loss: 0.6797 - val_accuracy: 0.5984\n",
      "Epoch 153/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6861 - accuracy: 0.5901 - val_loss: 0.6797 - val_accuracy: 0.5984\n",
      "Epoch 154/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6862 - accuracy: 0.5941 - val_loss: 0.6796 - val_accuracy: 0.6063\n",
      "Epoch 155/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6861 - accuracy: 0.5921 - val_loss: 0.6796 - val_accuracy: 0.5984\n",
      "Epoch 156/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6861 - accuracy: 0.5921 - val_loss: 0.6795 - val_accuracy: 0.5984\n",
      "Epoch 157/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6860 - accuracy: 0.5881 - val_loss: 0.6795 - val_accuracy: 0.5984\n",
      "Epoch 158/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6860 - accuracy: 0.5842 - val_loss: 0.6795 - val_accuracy: 0.5984\n",
      "Epoch 159/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6860 - accuracy: 0.5941 - val_loss: 0.6794 - val_accuracy: 0.6063\n",
      "Epoch 160/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6860 - accuracy: 0.5941 - val_loss: 0.6794 - val_accuracy: 0.6063\n",
      "Epoch 161/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6860 - accuracy: 0.5881 - val_loss: 0.6794 - val_accuracy: 0.6063\n",
      "Epoch 162/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6860 - accuracy: 0.5842 - val_loss: 0.6793 - val_accuracy: 0.6063\n",
      "Epoch 163/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6859 - accuracy: 0.5782 - val_loss: 0.6792 - val_accuracy: 0.6063\n",
      "Epoch 164/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6859 - accuracy: 0.5921 - val_loss: 0.6792 - val_accuracy: 0.5984\n",
      "Epoch 165/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6859 - accuracy: 0.5861 - val_loss: 0.6792 - val_accuracy: 0.6142\n",
      "Epoch 166/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6858 - accuracy: 0.5822 - val_loss: 0.6792 - val_accuracy: 0.5984\n",
      "Epoch 167/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6858 - accuracy: 0.5941 - val_loss: 0.6791 - val_accuracy: 0.5906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6858 - accuracy: 0.5842 - val_loss: 0.6790 - val_accuracy: 0.5984\n",
      "Epoch 169/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6858 - accuracy: 0.5941 - val_loss: 0.6790 - val_accuracy: 0.5906\n",
      "Epoch 170/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6858 - accuracy: 0.5861 - val_loss: 0.6790 - val_accuracy: 0.6142\n",
      "Epoch 171/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6857 - accuracy: 0.6000 - val_loss: 0.6790 - val_accuracy: 0.5984\n",
      "Epoch 172/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6857 - accuracy: 0.5960 - val_loss: 0.6789 - val_accuracy: 0.5984\n",
      "Epoch 173/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6857 - accuracy: 0.5921 - val_loss: 0.6789 - val_accuracy: 0.5748\n",
      "Epoch 174/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6857 - accuracy: 0.5861 - val_loss: 0.6789 - val_accuracy: 0.5906\n",
      "Epoch 175/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6856 - accuracy: 0.5901 - val_loss: 0.6789 - val_accuracy: 0.5984\n",
      "Epoch 176/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6856 - accuracy: 0.5921 - val_loss: 0.6789 - val_accuracy: 0.5984\n",
      "Epoch 177/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6856 - accuracy: 0.5941 - val_loss: 0.6788 - val_accuracy: 0.5906\n",
      "Epoch 178/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6856 - accuracy: 0.5921 - val_loss: 0.6788 - val_accuracy: 0.5906\n",
      "Epoch 179/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6856 - accuracy: 0.5941 - val_loss: 0.6787 - val_accuracy: 0.5827\n",
      "Epoch 180/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6855 - accuracy: 0.5743 - val_loss: 0.6787 - val_accuracy: 0.5984\n",
      "Epoch 181/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6855 - accuracy: 0.5921 - val_loss: 0.6786 - val_accuracy: 0.5984\n",
      "Epoch 182/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6855 - accuracy: 0.5881 - val_loss: 0.6786 - val_accuracy: 0.5984\n",
      "Epoch 183/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6854 - accuracy: 0.5941 - val_loss: 0.6786 - val_accuracy: 0.5906\n",
      "Epoch 184/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6854 - accuracy: 0.5762 - val_loss: 0.6786 - val_accuracy: 0.6063\n",
      "Epoch 185/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6854 - accuracy: 0.5901 - val_loss: 0.6785 - val_accuracy: 0.6063\n",
      "Epoch 186/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6854 - accuracy: 0.5960 - val_loss: 0.6785 - val_accuracy: 0.5984\n",
      "Epoch 187/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6854 - accuracy: 0.5941 - val_loss: 0.6785 - val_accuracy: 0.5984\n",
      "Epoch 188/500\n",
      "505/505 [==============================] - 0s 130us/step - loss: 0.6853 - accuracy: 0.5842 - val_loss: 0.6784 - val_accuracy: 0.5984\n",
      "Epoch 189/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6853 - accuracy: 0.5842 - val_loss: 0.6784 - val_accuracy: 0.6063\n",
      "Epoch 190/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6853 - accuracy: 0.5901 - val_loss: 0.6784 - val_accuracy: 0.5906\n",
      "Epoch 191/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6852 - accuracy: 0.5960 - val_loss: 0.6783 - val_accuracy: 0.5827\n",
      "Epoch 192/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6852 - accuracy: 0.5960 - val_loss: 0.6783 - val_accuracy: 0.5984\n",
      "Epoch 193/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6852 - accuracy: 0.5822 - val_loss: 0.6783 - val_accuracy: 0.5827\n",
      "Epoch 194/500\n",
      "505/505 [==============================] - 0s 132us/step - loss: 0.6852 - accuracy: 0.5822 - val_loss: 0.6783 - val_accuracy: 0.5827\n",
      "Epoch 195/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6852 - accuracy: 0.5782 - val_loss: 0.6783 - val_accuracy: 0.5748\n",
      "Epoch 196/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6851 - accuracy: 0.5861 - val_loss: 0.6783 - val_accuracy: 0.5748\n",
      "Epoch 197/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6851 - accuracy: 0.5782 - val_loss: 0.6782 - val_accuracy: 0.5827\n",
      "Epoch 198/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6851 - accuracy: 0.5960 - val_loss: 0.6781 - val_accuracy: 0.5827\n",
      "Epoch 199/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6851 - accuracy: 0.5861 - val_loss: 0.6781 - val_accuracy: 0.5827\n",
      "Epoch 200/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6851 - accuracy: 0.5861 - val_loss: 0.6781 - val_accuracy: 0.5827\n",
      "Epoch 201/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6850 - accuracy: 0.5861 - val_loss: 0.6781 - val_accuracy: 0.5827\n",
      "Epoch 202/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6850 - accuracy: 0.5822 - val_loss: 0.6781 - val_accuracy: 0.5827\n",
      "Epoch 203/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6850 - accuracy: 0.5762 - val_loss: 0.6781 - val_accuracy: 0.5748\n",
      "Epoch 204/500\n",
      "505/505 [==============================] - 0s 132us/step - loss: 0.6850 - accuracy: 0.5802 - val_loss: 0.6780 - val_accuracy: 0.5827\n",
      "Epoch 205/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6850 - accuracy: 0.5901 - val_loss: 0.6780 - val_accuracy: 0.5748\n",
      "Epoch 206/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6850 - accuracy: 0.5861 - val_loss: 0.6779 - val_accuracy: 0.5748\n",
      "Epoch 207/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6849 - accuracy: 0.5861 - val_loss: 0.6779 - val_accuracy: 0.5748\n",
      "Epoch 208/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6849 - accuracy: 0.5842 - val_loss: 0.6779 - val_accuracy: 0.5827\n",
      "Epoch 209/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6849 - accuracy: 0.5861 - val_loss: 0.6779 - val_accuracy: 0.5748\n",
      "Epoch 210/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6849 - accuracy: 0.5842 - val_loss: 0.6779 - val_accuracy: 0.5984\n",
      "Epoch 211/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6848 - accuracy: 0.5980 - val_loss: 0.6778 - val_accuracy: 0.5827\n",
      "Epoch 212/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6848 - accuracy: 0.5723 - val_loss: 0.6778 - val_accuracy: 0.5748\n",
      "Epoch 213/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6848 - accuracy: 0.5901 - val_loss: 0.6778 - val_accuracy: 0.5984\n",
      "Epoch 214/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6848 - accuracy: 0.5901 - val_loss: 0.6777 - val_accuracy: 0.5827\n",
      "Epoch 215/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6847 - accuracy: 0.5802 - val_loss: 0.6777 - val_accuracy: 0.5906\n",
      "Epoch 216/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6847 - accuracy: 0.5941 - val_loss: 0.6777 - val_accuracy: 0.5748\n",
      "Epoch 217/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6847 - accuracy: 0.5703 - val_loss: 0.6777 - val_accuracy: 0.5906\n",
      "Epoch 218/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6847 - accuracy: 0.5802 - val_loss: 0.6777 - val_accuracy: 0.5906\n",
      "Epoch 219/500\n",
      "505/505 [==============================] - 0s 160us/step - loss: 0.6847 - accuracy: 0.5921 - val_loss: 0.6776 - val_accuracy: 0.5906\n",
      "Epoch 220/500\n",
      "505/505 [==============================] - 0s 144us/step - loss: 0.6846 - accuracy: 0.5822 - val_loss: 0.6776 - val_accuracy: 0.5984\n",
      "Epoch 221/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6846 - accuracy: 0.5842 - val_loss: 0.6776 - val_accuracy: 0.6142\n",
      "Epoch 222/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6846 - accuracy: 0.5901 - val_loss: 0.6776 - val_accuracy: 0.5906\n",
      "Epoch 223/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6846 - accuracy: 0.5822 - val_loss: 0.6776 - val_accuracy: 0.5906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6846 - accuracy: 0.5782 - val_loss: 0.6776 - val_accuracy: 0.5906\n",
      "Epoch 225/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6847 - accuracy: 0.5782 - val_loss: 0.6776 - val_accuracy: 0.5906\n",
      "Epoch 226/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6845 - accuracy: 0.5782 - val_loss: 0.6775 - val_accuracy: 0.6220\n",
      "Epoch 227/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6845 - accuracy: 0.5881 - val_loss: 0.6775 - val_accuracy: 0.6063\n",
      "Epoch 228/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6845 - accuracy: 0.5901 - val_loss: 0.6775 - val_accuracy: 0.6063\n",
      "Epoch 229/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6845 - accuracy: 0.5842 - val_loss: 0.6775 - val_accuracy: 0.6063\n",
      "Epoch 230/500\n",
      "505/505 [==============================] - 0s 132us/step - loss: 0.6845 - accuracy: 0.5782 - val_loss: 0.6774 - val_accuracy: 0.6063\n",
      "Epoch 231/500\n",
      "505/505 [==============================] - 0s 134us/step - loss: 0.6844 - accuracy: 0.5762 - val_loss: 0.6774 - val_accuracy: 0.6063\n",
      "Epoch 232/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6844 - accuracy: 0.5842 - val_loss: 0.6773 - val_accuracy: 0.6063\n",
      "Epoch 233/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6844 - accuracy: 0.5842 - val_loss: 0.6773 - val_accuracy: 0.5906\n",
      "Epoch 234/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6844 - accuracy: 0.5743 - val_loss: 0.6774 - val_accuracy: 0.6142\n",
      "Epoch 235/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6844 - accuracy: 0.5743 - val_loss: 0.6774 - val_accuracy: 0.6142\n",
      "Epoch 236/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6844 - accuracy: 0.5822 - val_loss: 0.6774 - val_accuracy: 0.6063\n",
      "Epoch 237/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6844 - accuracy: 0.5822 - val_loss: 0.6773 - val_accuracy: 0.6142\n",
      "Epoch 238/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6844 - accuracy: 0.5782 - val_loss: 0.6773 - val_accuracy: 0.6142\n",
      "Epoch 239/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6842 - accuracy: 0.5861 - val_loss: 0.6772 - val_accuracy: 0.6220\n",
      "Epoch 240/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6842 - accuracy: 0.5861 - val_loss: 0.6772 - val_accuracy: 0.6220\n",
      "Epoch 241/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6843 - accuracy: 0.5802 - val_loss: 0.6772 - val_accuracy: 0.6063\n",
      "Epoch 242/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6842 - accuracy: 0.5782 - val_loss: 0.6771 - val_accuracy: 0.5906\n",
      "Epoch 243/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6842 - accuracy: 0.5842 - val_loss: 0.6771 - val_accuracy: 0.6142\n",
      "Epoch 244/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6842 - accuracy: 0.5802 - val_loss: 0.6771 - val_accuracy: 0.6063\n",
      "Epoch 245/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6842 - accuracy: 0.5743 - val_loss: 0.6771 - val_accuracy: 0.5906\n",
      "Epoch 246/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6842 - accuracy: 0.5861 - val_loss: 0.6771 - val_accuracy: 0.5906\n",
      "Epoch 247/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6841 - accuracy: 0.5802 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 248/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6842 - accuracy: 0.5743 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 249/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6842 - accuracy: 0.5762 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 250/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6840 - accuracy: 0.5822 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 251/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6842 - accuracy: 0.5822 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 252/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6841 - accuracy: 0.5802 - val_loss: 0.6769 - val_accuracy: 0.5748\n",
      "Epoch 253/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6841 - accuracy: 0.5822 - val_loss: 0.6769 - val_accuracy: 0.5906\n",
      "Epoch 254/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6840 - accuracy: 0.5762 - val_loss: 0.6769 - val_accuracy: 0.5906\n",
      "Epoch 255/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6840 - accuracy: 0.5743 - val_loss: 0.6769 - val_accuracy: 0.5906\n",
      "Epoch 256/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6839 - accuracy: 0.5901 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 257/500\n",
      "505/505 [==============================] - 0s 132us/step - loss: 0.6839 - accuracy: 0.5842 - val_loss: 0.6770 - val_accuracy: 0.5748\n",
      "Epoch 258/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6839 - accuracy: 0.5901 - val_loss: 0.6770 - val_accuracy: 0.5827\n",
      "Epoch 259/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6839 - accuracy: 0.5743 - val_loss: 0.6770 - val_accuracy: 0.5748\n",
      "Epoch 260/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6839 - accuracy: 0.5842 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 261/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6839 - accuracy: 0.5941 - val_loss: 0.6770 - val_accuracy: 0.5906\n",
      "Epoch 262/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6838 - accuracy: 0.5861 - val_loss: 0.6769 - val_accuracy: 0.5748\n",
      "Epoch 263/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6839 - accuracy: 0.5842 - val_loss: 0.6769 - val_accuracy: 0.5827\n",
      "Epoch 264/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6838 - accuracy: 0.5703 - val_loss: 0.6769 - val_accuracy: 0.5827\n",
      "Epoch 265/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6838 - accuracy: 0.5802 - val_loss: 0.6769 - val_accuracy: 0.5748\n",
      "Epoch 266/500\n",
      "505/505 [==============================] - 0s 136us/step - loss: 0.6838 - accuracy: 0.5822 - val_loss: 0.6769 - val_accuracy: 0.5827\n",
      "Epoch 267/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6838 - accuracy: 0.5703 - val_loss: 0.6769 - val_accuracy: 0.5748\n",
      "Epoch 268/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6838 - accuracy: 0.5762 - val_loss: 0.6769 - val_accuracy: 0.5827\n",
      "Epoch 269/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6837 - accuracy: 0.5762 - val_loss: 0.6769 - val_accuracy: 0.5906\n",
      "Epoch 270/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6837 - accuracy: 0.5861 - val_loss: 0.6768 - val_accuracy: 0.5827\n",
      "Epoch 271/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6837 - accuracy: 0.5822 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 272/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6837 - accuracy: 0.5881 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 273/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6837 - accuracy: 0.5861 - val_loss: 0.6768 - val_accuracy: 0.5984\n",
      "Epoch 274/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6836 - accuracy: 0.5782 - val_loss: 0.6768 - val_accuracy: 0.6063\n",
      "Epoch 275/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6837 - accuracy: 0.5941 - val_loss: 0.6768 - val_accuracy: 0.5984\n",
      "Epoch 276/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6836 - accuracy: 0.5802 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 277/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6836 - accuracy: 0.5802 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 278/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6836 - accuracy: 0.5822 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 279/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6835 - accuracy: 0.5782 - val_loss: 0.6767 - val_accuracy: 0.5906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 280/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6836 - accuracy: 0.5822 - val_loss: 0.6767 - val_accuracy: 0.5906\n",
      "Epoch 281/500\n",
      "505/505 [==============================] - 0s 130us/step - loss: 0.6836 - accuracy: 0.5802 - val_loss: 0.6768 - val_accuracy: 0.5748\n",
      "Epoch 282/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6835 - accuracy: 0.5901 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 283/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6835 - accuracy: 0.5743 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 284/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6835 - accuracy: 0.5802 - val_loss: 0.6768 - val_accuracy: 0.5906\n",
      "Epoch 285/500\n",
      "505/505 [==============================] - 0s 138us/step - loss: 0.6836 - accuracy: 0.5822 - val_loss: 0.6768 - val_accuracy: 0.5748\n",
      "Epoch 286/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6835 - accuracy: 0.5683 - val_loss: 0.6767 - val_accuracy: 0.5906\n",
      "Epoch 287/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6835 - accuracy: 0.5782 - val_loss: 0.6767 - val_accuracy: 0.5906\n",
      "Epoch 288/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6835 - accuracy: 0.5683 - val_loss: 0.6767 - val_accuracy: 0.5984\n",
      "Epoch 289/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6834 - accuracy: 0.5842 - val_loss: 0.6767 - val_accuracy: 0.5827\n",
      "Epoch 290/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6834 - accuracy: 0.5822 - val_loss: 0.6767 - val_accuracy: 0.5906\n",
      "Epoch 291/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6833 - accuracy: 0.5941 - val_loss: 0.6767 - val_accuracy: 0.5906\n",
      "Epoch 292/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6833 - accuracy: 0.5881 - val_loss: 0.6767 - val_accuracy: 0.5984\n",
      "Epoch 293/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6835 - accuracy: 0.5881 - val_loss: 0.6767 - val_accuracy: 0.5906\n",
      "Epoch 294/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6833 - accuracy: 0.5861 - val_loss: 0.6767 - val_accuracy: 0.5748\n",
      "Epoch 295/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6833 - accuracy: 0.5861 - val_loss: 0.6766 - val_accuracy: 0.5827\n",
      "Epoch 296/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6832 - accuracy: 0.5822 - val_loss: 0.6766 - val_accuracy: 0.5906\n",
      "Epoch 297/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6833 - accuracy: 0.5822 - val_loss: 0.6766 - val_accuracy: 0.5906\n",
      "Epoch 298/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6832 - accuracy: 0.5822 - val_loss: 0.6766 - val_accuracy: 0.5906\n",
      "Epoch 299/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6832 - accuracy: 0.5861 - val_loss: 0.6766 - val_accuracy: 0.5748\n",
      "Epoch 300/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6832 - accuracy: 0.5762 - val_loss: 0.6765 - val_accuracy: 0.5906\n",
      "Epoch 301/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6832 - accuracy: 0.5822 - val_loss: 0.6766 - val_accuracy: 0.5827\n",
      "Epoch 302/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6831 - accuracy: 0.5723 - val_loss: 0.6765 - val_accuracy: 0.5827\n",
      "Epoch 303/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6831 - accuracy: 0.5881 - val_loss: 0.6766 - val_accuracy: 0.5906\n",
      "Epoch 304/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6831 - accuracy: 0.5822 - val_loss: 0.6765 - val_accuracy: 0.5906\n",
      "Epoch 305/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6832 - accuracy: 0.5881 - val_loss: 0.6765 - val_accuracy: 0.5906\n",
      "Epoch 306/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6831 - accuracy: 0.5881 - val_loss: 0.6765 - val_accuracy: 0.5906\n",
      "Epoch 307/500\n",
      "505/505 [==============================] - 0s 128us/step - loss: 0.6831 - accuracy: 0.5822 - val_loss: 0.6766 - val_accuracy: 0.5906\n",
      "Epoch 308/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6831 - accuracy: 0.5881 - val_loss: 0.6765 - val_accuracy: 0.5906\n",
      "Epoch 309/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6830 - accuracy: 0.5921 - val_loss: 0.6765 - val_accuracy: 0.5984\n",
      "Epoch 310/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6831 - accuracy: 0.5782 - val_loss: 0.6764 - val_accuracy: 0.5984\n",
      "Epoch 311/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6830 - accuracy: 0.5822 - val_loss: 0.6764 - val_accuracy: 0.5906\n",
      "Epoch 312/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6830 - accuracy: 0.5822 - val_loss: 0.6764 - val_accuracy: 0.5906\n",
      "Epoch 313/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6830 - accuracy: 0.5901 - val_loss: 0.6764 - val_accuracy: 0.5906\n",
      "Epoch 314/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6829 - accuracy: 0.5901 - val_loss: 0.6764 - val_accuracy: 0.5827\n",
      "Epoch 315/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6829 - accuracy: 0.5822 - val_loss: 0.6765 - val_accuracy: 0.5906\n",
      "Epoch 316/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6829 - accuracy: 0.5822 - val_loss: 0.6764 - val_accuracy: 0.5984\n",
      "Epoch 317/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6829 - accuracy: 0.5980 - val_loss: 0.6764 - val_accuracy: 0.5984\n",
      "Epoch 318/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6829 - accuracy: 0.5822 - val_loss: 0.6764 - val_accuracy: 0.5984\n",
      "Epoch 319/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6828 - accuracy: 0.5822 - val_loss: 0.6764 - val_accuracy: 0.6063\n",
      "Epoch 320/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6828 - accuracy: 0.5782 - val_loss: 0.6764 - val_accuracy: 0.5984\n",
      "Epoch 321/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6828 - accuracy: 0.5842 - val_loss: 0.6764 - val_accuracy: 0.6142\n",
      "Epoch 322/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6827 - accuracy: 0.5881 - val_loss: 0.6763 - val_accuracy: 0.6142\n",
      "Epoch 323/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6827 - accuracy: 0.5921 - val_loss: 0.6763 - val_accuracy: 0.6142\n",
      "Epoch 324/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6827 - accuracy: 0.5802 - val_loss: 0.6763 - val_accuracy: 0.5984\n",
      "Epoch 325/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6828 - accuracy: 0.5802 - val_loss: 0.6763 - val_accuracy: 0.5984\n",
      "Epoch 326/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6828 - accuracy: 0.5921 - val_loss: 0.6763 - val_accuracy: 0.5984\n",
      "Epoch 327/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6827 - accuracy: 0.5822 - val_loss: 0.6763 - val_accuracy: 0.5906\n",
      "Epoch 328/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6827 - accuracy: 0.5762 - val_loss: 0.6763 - val_accuracy: 0.5906\n",
      "Epoch 329/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6827 - accuracy: 0.5901 - val_loss: 0.6763 - val_accuracy: 0.5827\n",
      "Epoch 330/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6826 - accuracy: 0.5743 - val_loss: 0.6763 - val_accuracy: 0.5984\n",
      "Epoch 331/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6826 - accuracy: 0.5802 - val_loss: 0.6764 - val_accuracy: 0.5984\n",
      "Epoch 332/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6827 - accuracy: 0.5802 - val_loss: 0.6764 - val_accuracy: 0.5984\n",
      "Epoch 333/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6826 - accuracy: 0.5842 - val_loss: 0.6764 - val_accuracy: 0.5906\n",
      "Epoch 334/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6826 - accuracy: 0.5881 - val_loss: 0.6764 - val_accuracy: 0.5906\n",
      "Epoch 335/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6825 - accuracy: 0.5822 - val_loss: 0.6764 - val_accuracy: 0.5984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 336/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6826 - accuracy: 0.5881 - val_loss: 0.6763 - val_accuracy: 0.6142\n",
      "Epoch 337/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6825 - accuracy: 0.5842 - val_loss: 0.6763 - val_accuracy: 0.6142\n",
      "Epoch 338/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6825 - accuracy: 0.5762 - val_loss: 0.6763 - val_accuracy: 0.5906\n",
      "Epoch 339/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6825 - accuracy: 0.5822 - val_loss: 0.6763 - val_accuracy: 0.5984\n",
      "Epoch 340/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6825 - accuracy: 0.5782 - val_loss: 0.6763 - val_accuracy: 0.5984\n",
      "Epoch 341/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6825 - accuracy: 0.5861 - val_loss: 0.6763 - val_accuracy: 0.6142\n",
      "Epoch 342/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6824 - accuracy: 0.5842 - val_loss: 0.6763 - val_accuracy: 0.6063\n",
      "Epoch 343/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6824 - accuracy: 0.5822 - val_loss: 0.6763 - val_accuracy: 0.5984\n",
      "Epoch 344/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6824 - accuracy: 0.5901 - val_loss: 0.6762 - val_accuracy: 0.5984\n",
      "Epoch 345/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6824 - accuracy: 0.5901 - val_loss: 0.6762 - val_accuracy: 0.5984\n",
      "Epoch 346/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6824 - accuracy: 0.5822 - val_loss: 0.6762 - val_accuracy: 0.5984\n",
      "Epoch 347/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6823 - accuracy: 0.5723 - val_loss: 0.6761 - val_accuracy: 0.5906\n",
      "Epoch 348/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6824 - accuracy: 0.5822 - val_loss: 0.6761 - val_accuracy: 0.5984\n",
      "Epoch 349/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6824 - accuracy: 0.5743 - val_loss: 0.6761 - val_accuracy: 0.5906\n",
      "Epoch 350/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6823 - accuracy: 0.5743 - val_loss: 0.6761 - val_accuracy: 0.5984\n",
      "Epoch 351/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6822 - accuracy: 0.5762 - val_loss: 0.6761 - val_accuracy: 0.5748\n",
      "Epoch 352/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6822 - accuracy: 0.5842 - val_loss: 0.6761 - val_accuracy: 0.5669\n",
      "Epoch 353/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6822 - accuracy: 0.5743 - val_loss: 0.6761 - val_accuracy: 0.5669\n",
      "Epoch 354/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6822 - accuracy: 0.5663 - val_loss: 0.6761 - val_accuracy: 0.5906\n",
      "Epoch 355/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6822 - accuracy: 0.5802 - val_loss: 0.6761 - val_accuracy: 0.5984\n",
      "Epoch 356/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6821 - accuracy: 0.5802 - val_loss: 0.6761 - val_accuracy: 0.5984\n",
      "Epoch 357/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6823 - accuracy: 0.5842 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 358/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6821 - accuracy: 0.5822 - val_loss: 0.6760 - val_accuracy: 0.5906\n",
      "Epoch 359/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6821 - accuracy: 0.5822 - val_loss: 0.6760 - val_accuracy: 0.5748\n",
      "Epoch 360/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6822 - accuracy: 0.5762 - val_loss: 0.6760 - val_accuracy: 0.5827\n",
      "Epoch 361/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6821 - accuracy: 0.5782 - val_loss: 0.6760 - val_accuracy: 0.5906\n",
      "Epoch 362/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6821 - accuracy: 0.5842 - val_loss: 0.6760 - val_accuracy: 0.5669\n",
      "Epoch 363/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6820 - accuracy: 0.5723 - val_loss: 0.6760 - val_accuracy: 0.5827\n",
      "Epoch 364/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6820 - accuracy: 0.5822 - val_loss: 0.6760 - val_accuracy: 0.5748\n",
      "Epoch 365/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6820 - accuracy: 0.5802 - val_loss: 0.6760 - val_accuracy: 0.5906\n",
      "Epoch 366/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6821 - accuracy: 0.5762 - val_loss: 0.6760 - val_accuracy: 0.5906\n",
      "Epoch 367/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6819 - accuracy: 0.5802 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 368/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6819 - accuracy: 0.5822 - val_loss: 0.6761 - val_accuracy: 0.5906\n",
      "Epoch 369/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6820 - accuracy: 0.5822 - val_loss: 0.6761 - val_accuracy: 0.5984\n",
      "Epoch 370/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6820 - accuracy: 0.5723 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 371/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6819 - accuracy: 0.5901 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 372/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6820 - accuracy: 0.5842 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 373/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6819 - accuracy: 0.5921 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 374/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6818 - accuracy: 0.5782 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 375/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6818 - accuracy: 0.5782 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 376/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6818 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 377/500\n",
      "505/505 [==============================] - 0s 144us/step - loss: 0.6818 - accuracy: 0.5881 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 378/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6818 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 379/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6818 - accuracy: 0.5881 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 380/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6817 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 381/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6817 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 382/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6818 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 383/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6817 - accuracy: 0.6020 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 384/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6818 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 385/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6818 - accuracy: 0.5901 - val_loss: 0.6759 - val_accuracy: 0.5827\n",
      "Epoch 386/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6817 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 387/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6817 - accuracy: 0.5881 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 388/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6816 - accuracy: 0.5842 - val_loss: 0.6758 - val_accuracy: 0.5591\n",
      "Epoch 389/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6817 - accuracy: 0.5743 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 390/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6815 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 391/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6815 - accuracy: 0.5723 - val_loss: 0.6758 - val_accuracy: 0.5669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 392/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6816 - accuracy: 0.5743 - val_loss: 0.6758 - val_accuracy: 0.5748\n",
      "Epoch 393/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6815 - accuracy: 0.5861 - val_loss: 0.6758 - val_accuracy: 0.5748\n",
      "Epoch 394/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6815 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 395/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6815 - accuracy: 0.5782 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 396/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6815 - accuracy: 0.5762 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 397/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6814 - accuracy: 0.5822 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 398/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6814 - accuracy: 0.5782 - val_loss: 0.6758 - val_accuracy: 0.5748\n",
      "Epoch 399/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6815 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 400/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6813 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 401/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6814 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 402/500\n",
      "505/505 [==============================] - 0s 113us/step - loss: 0.6813 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 403/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6813 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 404/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6812 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5827\n",
      "Epoch 405/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6813 - accuracy: 0.5703 - val_loss: 0.6758 - val_accuracy: 0.5748\n",
      "Epoch 406/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6813 - accuracy: 0.5782 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 407/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6813 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 408/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6813 - accuracy: 0.5762 - val_loss: 0.6758 - val_accuracy: 0.5984\n",
      "Epoch 409/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6814 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 410/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6812 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 411/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6812 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 412/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6813 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 413/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6811 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 414/500\n",
      "505/505 [==============================] - 0s 136us/step - loss: 0.6812 - accuracy: 0.5901 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 415/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6811 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 416/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6813 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 417/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6811 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 418/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6811 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 419/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6811 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 420/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6811 - accuracy: 0.5822 - val_loss: 0.6758 - val_accuracy: 0.5906\n",
      "Epoch 421/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6810 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 422/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6811 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 423/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6810 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 424/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6809 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 425/500\n",
      "505/505 [==============================] - 0s 142us/step - loss: 0.6810 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 426/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6809 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 427/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6809 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 428/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6810 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 429/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6809 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 430/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6809 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 431/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6808 - accuracy: 0.5762 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 432/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6809 - accuracy: 0.5703 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 433/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6808 - accuracy: 0.5723 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 434/500\n",
      "505/505 [==============================] - 0s 126us/step - loss: 0.6808 - accuracy: 0.5683 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 435/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6807 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 436/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6808 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 437/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6807 - accuracy: 0.5703 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 438/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6807 - accuracy: 0.5703 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 439/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6807 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 440/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6808 - accuracy: 0.5723 - val_loss: 0.6760 - val_accuracy: 0.5906\n",
      "Epoch 441/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6806 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5827\n",
      "Epoch 442/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6807 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 443/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6807 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 444/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6806 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 445/500\n",
      "505/505 [==============================] - 0s 134us/step - loss: 0.6806 - accuracy: 0.5901 - val_loss: 0.6760 - val_accuracy: 0.5906\n",
      "Epoch 446/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6805 - accuracy: 0.5703 - val_loss: 0.6760 - val_accuracy: 0.5984\n",
      "Epoch 447/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6806 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 448/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6806 - accuracy: 0.5723 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 449/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6805 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 450/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6806 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 451/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6805 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 452/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6804 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 453/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6805 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 454/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6804 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 455/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6805 - accuracy: 0.5723 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 456/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6805 - accuracy: 0.5703 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 457/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6804 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 458/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6803 - accuracy: 0.5703 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 459/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6802 - accuracy: 0.5644 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 460/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6803 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 461/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6804 - accuracy: 0.5624 - val_loss: 0.6758 - val_accuracy: 0.5748\n",
      "Epoch 462/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6803 - accuracy: 0.5762 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 463/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6802 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 464/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6803 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 465/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6802 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 466/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6802 - accuracy: 0.5644 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 467/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6802 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 468/500\n",
      "505/505 [==============================] - 0s 113us/step - loss: 0.6801 - accuracy: 0.5703 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 469/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6802 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 470/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6802 - accuracy: 0.5782 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 471/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6801 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 472/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6801 - accuracy: 0.5822 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 473/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6801 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 474/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6801 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5984\n",
      "Epoch 475/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6800 - accuracy: 0.5921 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 476/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6800 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 477/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6800 - accuracy: 0.5723 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 478/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6800 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 479/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6799 - accuracy: 0.5723 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 480/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6798 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 481/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6799 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 482/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6799 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 483/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6798 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 484/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6799 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 485/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6798 - accuracy: 0.5743 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 486/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6799 - accuracy: 0.5842 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 487/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6799 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 488/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6797 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 489/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6798 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 490/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6797 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5827\n",
      "Epoch 491/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6797 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 492/500\n",
      "505/505 [==============================] - 0s 122us/step - loss: 0.6797 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 493/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6797 - accuracy: 0.5743 - val_loss: 0.6758 - val_accuracy: 0.5669\n",
      "Epoch 494/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6797 - accuracy: 0.5802 - val_loss: 0.6758 - val_accuracy: 0.5748\n",
      "Epoch 495/500\n",
      "505/505 [==============================] - 0s 120us/step - loss: 0.6796 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 496/500\n",
      "505/505 [==============================] - 0s 118us/step - loss: 0.6797 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5906\n",
      "Epoch 497/500\n",
      "505/505 [==============================] - 0s 124us/step - loss: 0.6795 - accuracy: 0.5861 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 498/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6796 - accuracy: 0.5802 - val_loss: 0.6759 - val_accuracy: 0.5748\n",
      "Epoch 499/500\n",
      "505/505 [==============================] - 0s 115us/step - loss: 0.6796 - accuracy: 0.5762 - val_loss: 0.6759 - val_accuracy: 0.5669\n",
      "Epoch 500/500\n",
      "505/505 [==============================] - 0s 117us/step - loss: 0.6797 - accuracy: 0.5723 - val_loss: 0.6758 - val_accuracy: 0.5669\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=1500))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "his = model.fit(x=x_train, y=y_train,batch_size=48, epochs=2000,shuffle=True, verbose=1, \n",
    "               validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555556"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.SVC(gamma='auto')\n",
    "classifier.fit(train_data, train_label)\n",
    "classifier.score(test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_train.shape: (515, 2)\n",
      "new_test.shape: (117, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5299145299145299"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用pca降维\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca_train = pca.fit_transform(train_data)\n",
    "pca_test = pca.fit_transform(test_data)\n",
    "print(\"new_train.shape:\", pca_train.shape)\n",
    "print(\"new_test.shape:\", pca_test.shape)\n",
    "\n",
    "from sklearn import svm\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(pca_train, train_label)\n",
    "classifier.score(pca_test,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisherscore(data, labels, num):\n",
    "\n",
    "    high = len(data)  # 向量个数\n",
    "    weight = len(data[0])  # 向量长度\n",
    "    P_num = np.sum(labels == 0)  # 正样本\n",
    "    N_num = np.sum(labels == 1)  # 负样本\n",
    "\n",
    "    # 计算Fisher score\n",
    "\n",
    "    fisherscore = []\n",
    "    for i in range(weight):\n",
    "        p = []\n",
    "        n = []\n",
    "        p_var = []\n",
    "        n_var = []\n",
    "        for j in range(high):\n",
    "            if labels[j] == 0:\n",
    "                p.append(data[j, i])\n",
    "            if labels[j] == 1:\n",
    "                n.append(data[j, i])\n",
    "\n",
    "        p_average = np.sum(p) / len(p)\n",
    "        n_average = np.sum(n) / len(n)\n",
    "        average = (np.sum(p) + np.sum(n)) / (len(p) + len(n))\n",
    "\n",
    "        for j in range(high):\n",
    "            if labels[j] == 0:\n",
    "                p_var.append((data[j, i] - p_average) ** 2)\n",
    "            if labels[j] == 1:\n",
    "                n_var.append((data[j, i] - n_average) ** 2)\n",
    "\n",
    "        score = ((p_average - average) ** 2 + (n_average - average) ** 2) / (\n",
    "                    np.sum(p_var) / len(p) + np.sum(n_var) / len(n))\n",
    "\n",
    "        fisherscore.append(score)\n",
    "\n",
    "    index = np.argsort(-np.array(fisherscore))  # 返回索引\n",
    "    new_data = []\n",
    "    for i in range(num):\n",
    "        new_data.append(data[:, index[i]])\n",
    "\n",
    "    new_data = np.array(new_data)\n",
    "    new_data = new_data.transpose(1, 0)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_train = fisherscore(train_data, train_label, 22)\n",
    "fisher_test = fisherscore(test_data,test_label, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5384615384615384"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(fisher_train, train_label)\n",
    "classifier.score(fisher_test,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
